{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install higher tensorboard scipy networkx scikit-learn\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import ipdb\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import ipdb\n",
    "import copy\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import ipdb\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "from collections import defaultdict\n",
    "# torch.backends.cudnn.enabled = True\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN layer based on : https://arxiv.org/abs/1609.02907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphConvolution(Module):\n",
    "#     def __init__(self, in_features, out_features, order, bias=True):\n",
    "#         super(GraphConvolution, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "#         self.order = order\n",
    "#         self.weight = torch.nn.ParameterList([])\n",
    "#         for i in range(self.order):\n",
    "#             self.weight.append(Parameter(torch.FloatTensor(in_features, out_features)))\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for i in range(self.order):\n",
    "#             stdv = 1. / math.sqrt(self.weight[i].size(1))\n",
    "#             self.weight[i].data.uniform_(-stdv, stdv)\n",
    "#         if self.bias is not None:\n",
    "#             self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#     def forward(self, input, adj_mtx):\n",
    "#         output = []\n",
    "#         if self.order == 1 and type(adj_mtx) != list:\n",
    "#             adj = [adj_mtx]\n",
    "#         for i in range(self.order):\n",
    "#             support = torch.mm(input, self.weight[i])\n",
    "#             # output.append(support)\n",
    "#             output.append(torch.mm(adj_mtx[i], support))\n",
    "#         output = sum(output)\n",
    "#         if self.bias is not None:\n",
    "#             return output + self.bias\n",
    "#         else:\n",
    "#             return output\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + ' (' \\\n",
    "#                + str(self.in_features) + ' -> ' \\\n",
    "#                + str(self.out_features) + ')'\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        #for 3_D batch, need a loop!!!\n",
    "\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "class GCN_Encoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_Encoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GCN_Encoder2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_Encoder2, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GCN_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(GCN_Classifier, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def norm_sparse(adj):#normalize a torch dense tensor for GCN, and change it into sparse.\n",
    "    adj = adj + torch.eye(adj.shape[0]).to(adj)\n",
    "    rowsum = torch.sum(adj,1)\n",
    "    r_inv = 1/rowsum\n",
    "    r_inv[torch.isinf(r_inv)] = 0.\n",
    "    new_adj = torch.mul(r_inv.reshape(-1,1), adj)\n",
    "\n",
    "    indices = torch.nonzero(new_adj).t()\n",
    "    values = new_adj[indices[0], indices[1]] # modify this based on dimensionality\n",
    "    return torch.sparse.FloatTensor(indices, values, new_adj.size())\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def find_shown_index(adj, center_ind, steps = 2):\n",
    "    seen_nodes = {}\n",
    "    shown_index = []\n",
    "    if isinstance(center_ind, int):\n",
    "        center_ind = [center_ind]\n",
    "    for center in center_ind:\n",
    "        shown_index.append(center)\n",
    "        if center not in seen_nodes:\n",
    "            seen_nodes[center] = 1\n",
    "    start_point = center_ind\n",
    "    for step in range(steps):\n",
    "        new_start_point = []\n",
    "        candid_point = set(adj[start_point,:].reshape(-1, adj.shape[1]).nonzero()[:,1])\n",
    "        for i, c_p in enumerate(candid_point):\n",
    "            if c_p.item() in seen_nodes:\n",
    "                pass\n",
    "            else:\n",
    "                seen_nodes[c_p.item()] = 1\n",
    "                shown_index.append(c_p.item())\n",
    "                new_start_point.append(c_p)\n",
    "        start_point = new_start_point\n",
    "    return shown_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4\n",
    "epochs = 1000\n",
    "learning_rate_D = 0.01\n",
    "learning_rate_W = 0.01\n",
    "dropout_D = 0.5\n",
    "dropout_W = 0.5\n",
    "gamma = 1\n",
    "no_cuda = False\n",
    "train_ratio=0.6\n",
    "test_ratio=0.2\n",
    "n_classes = 2\n",
    "seed_num = 17\n",
    "torch.manual_seed(seed_num)\n",
    "dataset = \"diabetes\"\n",
    "order = 4\n",
    "n_features = 0\n",
    "w_val_size = 10\n",
    "imbalance_ratio = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the Pima Indian Diabetes (Diabetes) graph through the method used in : https://arxiv.org/abs/2103.00221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv('./data/diabetes.csv')\n",
    "# print(diabetes_df.head(10))\n",
    "dataset_length = diabetes_df.values.shape[0]\n",
    "diabetes_n_features = diabetes_df.values.shape[1] - 2\n",
    "diabetes_train_X_df = diabetes_df.drop(columns=('Glucose')).values[:int(dataset_length * train_ratio), :-1]\n",
    "diabetes_train_Y_df = diabetes_df.values[:int(dataset_length * train_ratio), -1]\n",
    "diabetes_val_X_df = diabetes_df.drop(columns=('Glucose')).values[int(dataset_length * train_ratio):int(dataset_length * (1 - test_ratio)), :-1]\n",
    "diabetes_val_Y_df = diabetes_df.values[int(dataset_length * train_ratio):int(dataset_length * (1 - test_ratio)), -1]\n",
    "diabetes_test_X_df = diabetes_df.drop(columns=('Glucose')).values[int(dataset_length * (1 - test_ratio)):, :-1]\n",
    "diabetes_test_Y_df = diabetes_df.values[int(dataset_length * (1 - test_ratio)):, -1]\n",
    "diabetes_train_idx = list(range(int(dataset_length * train_ratio)))\n",
    "diabetes_val_idx = list(range(int(dataset_length * train_ratio), int(dataset_length * (1 - test_ratio))))\n",
    "diabetes_test_idx = list(range(int(dataset_length * (1 - test_ratio)), dataset_length))\n",
    "G = nx.Graph()\n",
    "gam = 4\n",
    "for patient_id, row_vals in diabetes_df.iterrows():\n",
    "    G.add_node(str(patient_id), \n",
    "    pregnancies = row_vals[0], \n",
    "    # glucose = row_vals[1], \n",
    "    bloodpressure = row_vals[2], \n",
    "    skinthickness = row_vals[3], \n",
    "    insulin = row_vals[4], \n",
    "    bmi = row_vals[5], \n",
    "    diabetespedigreefunction = row_vals[6], \n",
    "    age = row_vals[7]) \n",
    "    # , outcome = row_vals[8])\n",
    "# Two loops because of the order problem of NetworkX\n",
    "for patient_id, row_vals in diabetes_df.iterrows():\n",
    "    for other_patient_id in range(patient_id + 1, diabetes_df.shape[0]):\n",
    "        if abs(diabetes_df.iloc[[other_patient_id], 1].values[0] - diabetes_df.iloc[[patient_id], 1].values[0]) < gam:\n",
    "            G.add_edge(*(str(patient_id), str(other_patient_id)))\n",
    "diabetes_adj_mtx = nx.to_numpy_matrix(G)\n",
    "# print(diabetes_adj_mtx)\n",
    "# print(G)\n",
    "# pd.DataFrame(diabetes_adj_mtx).to_csv(\"data/diabetes_adj_mtx.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habermanâ€™s survival (Haberman) graph through the method used in : https://arxiv.org/abs/2103.00221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "haberman_df = pd.read_csv('./data/haberman.csv')\n",
    "# print(diabetes_df.head(10))\n",
    "dataset_length = haberman_df.values.shape[0]\n",
    "haberman_n_features = haberman_df.values.shape[1] - 2\n",
    "haberman_train_X_df = haberman_df.drop(columns=('Lymph Nodes')).values[:int(dataset_length * train_ratio), :-1]\n",
    "haberman_train_Y_df = haberman_df.values[:int(dataset_length * train_ratio), -1]\n",
    "haberman_val_X_df = haberman_df.drop(columns=('Lymph Nodes')).values[int(dataset_length * train_ratio):int(dataset_length * (1 - test_ratio)), :-1]\n",
    "haberman_val_Y_df = haberman_df.values[int(dataset_length * train_ratio):int(dataset_length * (1 - test_ratio)), -1]\n",
    "haberman_test_X_df = haberman_df.drop(columns=('Lymph Nodes')).values[int(dataset_length * (1 - test_ratio)):, :-1]\n",
    "haberman_test_Y_df = haberman_df.values[int(dataset_length * (1 - test_ratio)):, -1]\n",
    "haberman_train_idx = list(range(int(dataset_length * train_ratio)))\n",
    "haberman_val_idx = list(range(int(dataset_length * train_ratio), int(dataset_length * (1 - test_ratio))))\n",
    "haberman_test_idx = list(range(int(dataset_length * (1 - test_ratio)), dataset_length))\n",
    "G = nx.Graph()\n",
    "gam = 2\n",
    "for patient_id, row_vals in haberman_df.iterrows():\n",
    "    G.add_node(str(patient_id),\n",
    "        age = row_vals[0],\n",
    "        operation_year = row_vals[1],\n",
    "        lymph_nodes = row_vals[2],\n",
    "    )\n",
    "    # survival = row_vals[3])\n",
    "for patient_id, row_vals in haberman_df.iterrows():\n",
    "    for other_patient_id in range(patient_id + 1, haberman_df.shape[0]):\n",
    "        if abs(haberman_df.iloc[[other_patient_id], 2].values[0] - haberman_df.iloc[[patient_id], 2].values[0]) < gam:\n",
    "            G.add_edge(*(str(patient_id), str(other_patient_id)))\n",
    "haberman_adj_mtx = nx.to_numpy_matrix(G)\n",
    "# print(haberman_adj_mtx)\n",
    "# print(G)\n",
    "# pd.DataFrame(diabetes_adj_mtx).to_csv(\"data/haberman_adj_mtx.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(mx, dim):\n",
    "    mx = torch.sigmoid(mx)\n",
    "    return F.normalize(mx, p=1, dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [256, 265, 256, 256]\n",
    "    n_features = diabetes_n_features\n",
    "    train_X = diabetes_train_X_df\n",
    "    train_Y = diabetes_train_Y_df\n",
    "    val_X = diabetes_val_X_df\n",
    "    val_Y = diabetes_val_Y_df\n",
    "    test_X = diabetes_test_X_df\n",
    "    test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [256, 265]\n",
    "    n_features = haberman_n_features\n",
    "    train_X = haberman_train_X_df\n",
    "    train_Y = haberman_train_Y_df\n",
    "    val_X = haberman_val_X_df\n",
    "    val_Y = haberman_val_Y_df\n",
    "    test_X = haberman_test_X_df\n",
    "    test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the train set's imbalance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IGDM_Lab\\AppData\\Local\\Temp\\ipykernel_41648\\2795049342.py:13: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  grouped_con_df = pd.DataFrame(pd.DataFrame(concated_train).groupby(by=[col_num-1]))\n"
     ]
    }
   ],
   "source": [
    "assert len(pd.DataFrame(val_Y, columns=['labels']).labels.unique()) == len(pd.DataFrame(train_Y, columns=['labels']).labels.unique()) == len(pd.DataFrame(test_Y, columns=['labels']).labels.unique()), \\\n",
    "    \"There are some classes missing in one the 3 partitiones of the dataset\"\n",
    "# imbalance_ratio = 0.03\n",
    "if imbalance_ratio != None:\n",
    "    train_total_count = len(train_Y)\n",
    "    train_labels_df = pd.DataFrame(train_Y, columns=['labels'])\n",
    "    labels = pd.DataFrame(train_Y, columns=['labels']).labels.unique()\n",
    "    train_grouped_df = train_labels_df.groupby(by=['labels'])['labels'].count()\n",
    "    train_minority_count = train_grouped_df.min()\n",
    "    train_minority_index = train_grouped_df.argmin()\n",
    "    n_classes = len(train_grouped_df)\n",
    "    concated_train = np.concatenate([train_X, train_Y[:, None]], axis=1)\n",
    "    col_num = concated_train.shape[-1]\n",
    "    grouped_con_df = pd.DataFrame(pd.DataFrame(concated_train).groupby(by=[col_num - 1]))\n",
    "    current_minority_total_ratio = train_minority_count / train_total_count\n",
    "    train_minority_label = grouped_con_df.iloc[train_minority_index, 0]\n",
    "    assert current_minority_total_ratio > imbalance_ratio, \"The ratio is below the threshold\"\n",
    "    rest_classes_count = train_total_count - train_minority_count\n",
    "    reduction = int(imbalance_ratio * rest_classes_count * (imbalance_ratio - 1) + train_minority_count)\n",
    "    minority_ndarray = grouped_con_df.iloc[train_minority_index, 1]\n",
    "    drop_indices = np.random.choice(minority_ndarray.index, reduction, replace=False)\n",
    "    df_subset = minority_ndarray.drop(drop_indices)\n",
    "    train_idx[: -reduction]\n",
    "    val_idx = [x - reduction for x in val_idx]\n",
    "    test_idx = [x - reduction for x in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IGDM_Lab\\AppData\\Local\\Temp\\ipykernel_41648\\3914429187.py:10: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  grouped_con_df = pd.DataFrame(pd.DataFrame(concated_val).groupby(by=[col_num - 1]))\n"
     ]
    }
   ],
   "source": [
    "def balanced_sampling(val_size):\n",
    "    val_total_length = len(val_Y)\n",
    "    labels_df = pd.DataFrame(val_Y, columns=['labels'])\n",
    "    labels = pd.DataFrame(val_Y, columns=['labels']).labels.unique()\n",
    "    val_grouped_df = labels_df.groupby(by=['labels'])['labels'].count()\n",
    "    val_minority_count = val_grouped_df.min()\n",
    "    assert val_minority_count > val_size, \"The size of minority class is less the the sampling size\"\n",
    "    n_classes = len(val_grouped_df)\n",
    "    concated_val = np.concatenate([val_X, val_Y[:, None]], axis=1)\n",
    "    col_num = concated_val.shape[-1]\n",
    "    grouped_con_df = pd.DataFrame(pd.DataFrame(concated_val).groupby(by=[col_num - 1]))\n",
    "    balanced_val_set = pd.DataFrame(columns=[list(range(col_num))]).to_numpy()\n",
    "    balanced_val_idx = []\n",
    "    for label_index in range(n_classes):\n",
    "        temp_df = pd.DataFrame(grouped_con_df.iloc[label_index, 1].sample(n = val_size))\n",
    "        balanced_val_idx += list(temp_df.index)\n",
    "        balanced_val_set = pd.DataFrame(np.concatenate([balanced_val_set, temp_df.to_numpy()], axis=0))\n",
    "    return balanced_val_set, balanced_val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wighted_GCN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, order):\n",
    "        super(Wighted_GCN, self).__init__()\n",
    "        layers = []\n",
    "        if len(n_hid) == 0:\n",
    "            layers.append(GraphConvolution(n_feat, n_class, order=order))\n",
    "        else:\n",
    "            layers.append(GraphConvolution(n_feat, n_hid[0], order=order))\n",
    "            for i in range(len(n_hid) - 1):\n",
    "                layers.append(GraphConvolution(n_hid[i], n_hid[i + 1], order=order))\n",
    "        if n_class > 1:\n",
    "            layers.append(GraphConvolution(n_hid[-1], n_class, order=order))\n",
    "        self.gc = nn.ModuleList(layers)\n",
    "        self.dropout = dropout\n",
    "        self.nclass = n_class\n",
    "\n",
    "    def forward(self, x, adj, samples=-1, func=F.relu):\n",
    "        end_layer = len(self.gc) - 1 if self.nclass > 1 else len(self.gc)\n",
    "        for i in range(end_layer):\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.gc[i](x, adj)\n",
    "            x = func(x)\n",
    "\n",
    "        if self.nclass > 1:\n",
    "            classifier = self.gc[-1](x, adj)\n",
    "            classifier = F.log_softmax(classifier, dim=1)\n",
    "            return classifier[samples,:], x\n",
    "        else:\n",
    "            return None, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_upsample(embed, labels, idx_train, adj=None, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "    adj_new = None\n",
    "    for i in range(im_class_num):\n",
    "        chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        num = int(chosen.shape[0]*portion)\n",
    "        if portion == 0:\n",
    "            c_portion = int(avg_number/chosen.shape[0])\n",
    "            num = chosen.shape[0]\n",
    "        else:\n",
    "            c_portion = 1\n",
    "\n",
    "        for j in range(c_portion):\n",
    "            chosen = chosen[:num]\n",
    "\n",
    "            chosen_embed = embed[chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.cpu().detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            new_embed = embed[chosen,:] + (chosen_embed[idx_neighbor,:]-embed[chosen,:])*interp_place\n",
    "\n",
    "\n",
    "            new_labels = labels.new(torch.Size((chosen.shape[0],1))).reshape(-1).fill_(c_largest-i)\n",
    "            idx_new = np.arange(embed.shape[0], embed.shape[0]+chosen.shape[0])\n",
    "            idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "            embed = torch.cat((embed,new_embed), 0)\n",
    "            labels = torch.cat((labels,new_labels), 0)\n",
    "            idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "\n",
    "            if adj is not None:\n",
    "                if adj_new is None:\n",
    "                    adj_new = adj.new(torch.clamp_(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                else:\n",
    "                    temp = adj.new(torch.clamp_(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                    adj_new = torch.cat((adj_new, temp), 0)\n",
    "\n",
    "    if adj is not None:\n",
    "        add_num = adj_new.shape[0]\n",
    "        new_adj = adj.new(torch.Size((adj.shape[0]+add_num, adj.shape[0]+add_num))).fill_(0.0)\n",
    "        new_adj[:adj.shape[0], :adj.shape[0]] = adj[:,:]\n",
    "        new_adj[adj.shape[0]:, :adj.shape[0]] = adj_new[:,:]\n",
    "        new_adj[:adj.shape[0], adj.shape[0]:] = torch.transpose(adj_new, 0, 1)[:,:]\n",
    "\n",
    "        return embed, labels, idx_train, new_adj.detach()\n",
    "\n",
    "    else:\n",
    "        return embed, labels, idx_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arc_selection-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f122987ad9a59b0c863ec73977cb4833edd644652b774e5b01a9e2fe636c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
