{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.GraphConvolution import GCN_Encoder_s, GCN_Classifier_s, Decoder_s\n",
    "from utils.GraphConvolution import GraphConvolution, GCN_Encoder3\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w, sigmoid\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_W = 0.01\n",
    "    dropout = 0.5\n",
    "    dropout_W = 0.5\n",
    "    gamma = 1\n",
    "    no_cuda = False\n",
    "    train_ratio=0.6\n",
    "    test_ratio=0.2\n",
    "    n_classes = 2\n",
    "    seed = 1234\n",
    "    torch.manual_seed(seed)\n",
    "    # -----------------------\n",
    "    dataset = \"cora\"\n",
    "    # dataset = \"haberman\"\n",
    "    # dataset = \"diabetes\"\n",
    "    # -----------------------\n",
    "    order = 4\n",
    "    n_features = 0\n",
    "    w_val_size = 10\n",
    "    # imbalance_ratio = None\n",
    "    imbalance_ratio = None\n",
    "    n_hidden = 64\n",
    "    setting = None\n",
    "    im_class_num = 1\n",
    "    setting = \"upsampling\"\n",
    "    opt_new_G = False\n",
    "    up_scale = 1\n",
    "    im_ratio = 0.5\n",
    "    val_size = 10\n",
    "    # -----------------------\n",
    "    # momentum = 0 # For SGD\n",
    "    momentum = 0.9\n",
    "    # -----------------------\n",
    "    optimizer_alg = \"ADAM\"\n",
    "    # optimizer_alg = \"Momentum\"\n",
    "    # optimizer_alg = \"RMSProp\"\n",
    "    # -----------------------\n",
    "    # activation_func = \"ReLU\"\n",
    "    # activation_func = \"Sigmoid\"\n",
    "    activation_func = \"LeakyReLU\"\n",
    "    # activation_func = \"PReLU\"\n",
    "    # activation_func = \"PReLU\"\n",
    "    # -----------------------\n",
    "    initalization = \"Xavier Uniform\"\n",
    "    # initalization = \"Xavier Normal\"\n",
    "    # initalization = \"Kaiming Uniform\"\n",
    "    # initalization = \"Kaiming Normal\"\n",
    "    # initalization = \"Uniform\" # Uniform with 1 over squre root of the fan in \n",
    "    # -----------------------\n",
    "    res_connection = False\n",
    "    res_connection = True\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import data_loader_diabetes, data_loader_haberman, data_loader_cora\n",
    "\n",
    "cora_adj_mtx, cora_labels_df, cora_features_df, \\\n",
    "        cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features = data_loader_cora(args)\n",
    "        \n",
    "diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n",
    "        diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features = data_loader_diabetes(args)\n",
    "\n",
    "haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n",
    "        haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features = data_loader_haberman(args)\n",
    "\n",
    "if args.dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = diabetes_n_features\n",
    "    features = diabetes_features_df\n",
    "    labels = diabetes_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif args.dataset == \"cora\":\n",
    "    adj_mtx = cora_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = cora_n_features\n",
    "    features = cora_features_df\n",
    "    labels = cora_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = cora_train_idx\n",
    "    val_idx = cora_val_idx\n",
    "    test_idx = cora_test_idx\n",
    "elif args.dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = haberman_n_features\n",
    "    features = haberman_features_df\n",
    "    labels = haberman_labels_df\n",
    "    # train_X = haberman_train_X_df\n",
    "    # train_Y = haberman_train_Y_df\n",
    "    # val_X = haberman_val_X_df\n",
    "    # val_Y = haberman_val_Y_df\n",
    "    # test_X = haberman_test_X_df\n",
    "    # test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(pd.DataFrame(labels[train_idx])[0].unique()) == len(pd.DataFrame(labels[val_idx])[0].unique()) == len(pd.DataFrame(labels[test_idx])[0].unique()), \\\n",
    "    # \"There are some classes missing in one the 3 partitiones of the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if False else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to Tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.from_numpy(np.concatenate((train_X, val_X, test_X), axis=0)).to(device)\n",
    "# labels = torch.from_numpy(np.int64(np.concatenate((train_Y, val_Y, test_Y), axis=0))).to(device)\n",
    "train_idx = torch.from_numpy(np.array(train_idx, dtype = np.int64)).to(device)\n",
    "val_idx = torch.from_numpy(np.array(val_idx, dtype = np.int64)).to(device)\n",
    "test_idx = torch.from_numpy(np.array(test_idx, dtype = np.int64)).to(device)\n",
    "features = torch.from_numpy(np.array(features, dtype = np.float64)).to(device)\n",
    "labels = torch.from_numpy(np.array(labels, dtype = np.int64)).to(device)\n",
    "try:\n",
    "    adj_mtx = torch.from_numpy(np.array(adj_mtx, dtype = np.float64)).to(device)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "\n",
    "encoder_n = GCN_Encoder3(nfeat = n_features,\n",
    "        nhid = n_hidden,\n",
    "        nembed = n_hidden[-1],\n",
    "        dropout = args.dropout,\n",
    "        nclass = args.n_classes, init = args.initalization,\n",
    "        order = 1)\n",
    "classifier_n = GCN_Classifier(nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, init = args.initalization,\n",
    "        device = device)\n",
    "\n",
    "# encoder = GCN_Encoder_s(nfeat = n_features, nhid = n_hidden[-1], nembed = n_hidden[-1], dropout = args.dropout)\n",
    "# classifier = GCN_Classifier_s(nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# encoder_n = GCN_Encoder_w(nfeat = n_features, nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# optimizer_n = optim.Adam(encoder_n.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "        encoder_n.train()\n",
    "        classifier_n.train()\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        # optimizer_n.zero_grad()\n",
    "        embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "        output = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "        # output = encoder_n(features, adj_mtx)\n",
    "        out = output[train_idx]\n",
    "        gt = labels[train_idx].reshape(-1)\n",
    "        if args.setting == 'reweight':\n",
    "                weight = \"STH\"\n",
    "                loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "        else:\n",
    "                loss_train = F.cross_entropy(out, gt)\n",
    "        acc_train = accuracy(out, gt)\n",
    "        loss_train.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        # encoder_n.step()\n",
    "        gt_v = labels[val_idx].reshape(-1)\n",
    "        out_v = output[val_idx]\n",
    "        loss_val = F.cross_entropy(out_v, gt_v)\n",
    "        acc_val = accuracy(out_v, gt_v)\n",
    "        # print_class_acc(out_v, gt_v)\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "        print_class_acc(output[val_idx], labels[val_idx])\n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_n.eval()\n",
    "    classifier_n.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "    outputs = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09763884544372559,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bf1486b9ef4e47a9a4b48a63388a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 2.0757 acc_train: 0.1933 loss_val: 1.9957 acc_val: 0.1863 time: 0.2007s\n",
      "valid current auc-roc score: 0.476940, current macro_F score: 0.102918\n",
      "Test set results: loss= 1.7483 accuracy= 0.3727\n",
      "test current auc-roc score: 0.722787, current macro_F score: 0.182246\n",
      "Epoch: 00002 loss_train: 1.7546 acc_train: 0.3516 loss_val: 1.7659 acc_val: 0.3432 time: 0.1403s\n",
      "valid current auc-roc score: 0.637522, current macro_F score: 0.201209\n",
      "Epoch: 00003 loss_train: 1.6027 acc_train: 0.4292 loss_val: 1.6878 acc_val: 0.3672 time: 0.1479s\n",
      "valid current auc-roc score: 0.695063, current macro_F score: 0.226601\n",
      "Epoch: 00004 loss_train: 1.4571 acc_train: 0.4969 loss_val: 1.5795 acc_val: 0.4244 time: 0.1555s\n",
      "valid current auc-roc score: 0.756646, current macro_F score: 0.281629\n",
      "Epoch: 00005 loss_train: 1.3687 acc_train: 0.5246 loss_val: 1.5111 acc_val: 0.4576 time: 0.1403s\n",
      "valid current auc-roc score: 0.809532, current macro_F score: 0.327592\n",
      "Epoch: 00006 loss_train: 1.2730 acc_train: 0.5917 loss_val: 1.4449 acc_val: 0.5037 time: 0.1580s\n",
      "valid current auc-roc score: 0.841819, current macro_F score: 0.411933\n",
      "Epoch: 00007 loss_train: 1.1825 acc_train: 0.6355 loss_val: 1.3654 acc_val: 0.5738 time: 0.1485s\n",
      "valid current auc-roc score: 0.857645, current macro_F score: 0.464270\n",
      "Epoch: 00008 loss_train: 1.1088 acc_train: 0.6804 loss_val: 1.3136 acc_val: 0.5701 time: 0.1829s\n",
      "valid current auc-roc score: 0.881698, current macro_F score: 0.472892\n",
      "Epoch: 00009 loss_train: 1.0525 acc_train: 0.6933 loss_val: 1.2493 acc_val: 0.6513 time: 0.1351s\n",
      "valid current auc-roc score: 0.893932, current macro_F score: 0.563522\n",
      "Epoch: 00010 loss_train: 0.9952 acc_train: 0.7217 loss_val: 1.1779 acc_val: 0.6476 time: 0.1584s\n",
      "valid current auc-roc score: 0.901805, current macro_F score: 0.557674\n",
      "Epoch: 00011 loss_train: 0.9252 acc_train: 0.7334 loss_val: 1.1744 acc_val: 0.6568 time: 0.1560s\n",
      "valid current auc-roc score: 0.898856, current macro_F score: 0.579406\n",
      "Test set results: loss= 1.1189 accuracy= 0.6642\n",
      "test current auc-roc score: 0.924942, current macro_F score: 0.611281\n",
      "Epoch: 00012 loss_train: 0.9185 acc_train: 0.7426 loss_val: 1.1390 acc_val: 0.6513 time: 0.1596s\n",
      "valid current auc-roc score: 0.903449, current macro_F score: 0.579105\n",
      "Epoch: 00013 loss_train: 0.8516 acc_train: 0.7685 loss_val: 1.1469 acc_val: 0.6550 time: 0.1451s\n",
      "valid current auc-roc score: 0.902471, current macro_F score: 0.604420\n",
      "Epoch: 00014 loss_train: 0.7952 acc_train: 0.7845 loss_val: 1.0896 acc_val: 0.6900 time: 0.1695s\n",
      "valid current auc-roc score: 0.909626, current macro_F score: 0.653894\n",
      "Epoch: 00015 loss_train: 0.7384 acc_train: 0.7839 loss_val: 1.0379 acc_val: 0.7030 time: 0.1591s\n",
      "valid current auc-roc score: 0.918903, current macro_F score: 0.668141\n",
      "Epoch: 00016 loss_train: 0.7267 acc_train: 0.7986 loss_val: 0.9891 acc_val: 0.7122 time: 0.1506s\n",
      "valid current auc-roc score: 0.923727, current macro_F score: 0.668583\n",
      "Epoch: 00017 loss_train: 0.6687 acc_train: 0.8067 loss_val: 0.9721 acc_val: 0.7196 time: 0.1323s\n",
      "valid current auc-roc score: 0.924663, current macro_F score: 0.677263\n",
      "Epoch: 00018 loss_train: 0.6784 acc_train: 0.8042 loss_val: 0.9533 acc_val: 0.7269 time: 0.1443s\n",
      "valid current auc-roc score: 0.926582, current macro_F score: 0.704244\n",
      "Epoch: 00019 loss_train: 0.6322 acc_train: 0.8153 loss_val: 0.9716 acc_val: 0.7159 time: 0.1646s\n",
      "valid current auc-roc score: 0.925505, current macro_F score: 0.688723\n",
      "Epoch: 00020 loss_train: 0.6719 acc_train: 0.8140 loss_val: 0.9016 acc_val: 0.7214 time: 0.1429s\n",
      "valid current auc-roc score: 0.930732, current macro_F score: 0.698204\n",
      "Epoch: 00021 loss_train: 0.5840 acc_train: 0.8220 loss_val: 0.8880 acc_val: 0.7251 time: 0.1316s\n",
      "valid current auc-roc score: 0.938015, current macro_F score: 0.704563\n",
      "Test set results: loss= 0.8047 accuracy= 0.7546\n",
      "test current auc-roc score: 0.947647, current macro_F score: 0.723092\n",
      "Epoch: 00022 loss_train: 0.5884 acc_train: 0.8307 loss_val: 0.8317 acc_val: 0.7528 time: 0.1251s\n",
      "valid current auc-roc score: 0.941547, current macro_F score: 0.728717\n",
      "Epoch: 00023 loss_train: 0.5478 acc_train: 0.8436 loss_val: 0.8525 acc_val: 0.7657 time: 0.1231s\n",
      "valid current auc-roc score: 0.934938, current macro_F score: 0.734928\n",
      "Epoch: 00024 loss_train: 0.5397 acc_train: 0.8356 loss_val: 0.8253 acc_val: 0.7528 time: 0.1361s\n",
      "valid current auc-roc score: 0.943102, current macro_F score: 0.722413\n",
      "Epoch: 00025 loss_train: 0.5431 acc_train: 0.8424 loss_val: 0.7819 acc_val: 0.7472 time: 0.1436s\n",
      "valid current auc-roc score: 0.945772, current macro_F score: 0.720830\n",
      "Epoch: 00026 loss_train: 0.5023 acc_train: 0.8411 loss_val: 0.8488 acc_val: 0.7601 time: 0.1446s\n",
      "valid current auc-roc score: 0.937388, current macro_F score: 0.745464\n",
      "Epoch: 00027 loss_train: 0.4947 acc_train: 0.8559 loss_val: 0.8256 acc_val: 0.7435 time: 0.1442s\n",
      "valid current auc-roc score: 0.941354, current macro_F score: 0.728224\n",
      "Epoch: 00028 loss_train: 0.4868 acc_train: 0.8547 loss_val: 0.8374 acc_val: 0.7472 time: 0.1390s\n",
      "valid current auc-roc score: 0.935671, current macro_F score: 0.725833\n",
      "Epoch: 00029 loss_train: 0.4869 acc_train: 0.8528 loss_val: 0.7615 acc_val: 0.7768 time: 0.1517s\n",
      "valid current auc-roc score: 0.947061, current macro_F score: 0.756350\n",
      "Epoch: 00030 loss_train: 0.4693 acc_train: 0.8658 loss_val: 0.8358 acc_val: 0.7528 time: 0.1465s\n",
      "valid current auc-roc score: 0.938790, current macro_F score: 0.734492\n",
      "Epoch: 00031 loss_train: 0.4440 acc_train: 0.8651 loss_val: 0.7304 acc_val: 0.7841 time: 0.1390s\n",
      "valid current auc-roc score: 0.954248, current macro_F score: 0.767924\n",
      "Test set results: loss= 0.7091 accuracy= 0.7749\n",
      "test current auc-roc score: 0.951004, current macro_F score: 0.746830\n",
      "Epoch: 00032 loss_train: 0.4616 acc_train: 0.8670 loss_val: 0.7548 acc_val: 0.7823 time: 0.1477s\n",
      "valid current auc-roc score: 0.952431, current macro_F score: 0.766017\n",
      "Epoch: 00033 loss_train: 0.4107 acc_train: 0.8842 loss_val: 0.7862 acc_val: 0.7712 time: 0.1380s\n",
      "valid current auc-roc score: 0.942961, current macro_F score: 0.752151\n",
      "Epoch: 00034 loss_train: 0.4096 acc_train: 0.8775 loss_val: 0.7718 acc_val: 0.7620 time: 0.1593s\n",
      "valid current auc-roc score: 0.947277, current macro_F score: 0.739702\n",
      "Epoch: 00035 loss_train: 0.4204 acc_train: 0.8682 loss_val: 0.7301 acc_val: 0.7768 time: 0.1454s\n",
      "valid current auc-roc score: 0.952986, current macro_F score: 0.751468\n",
      "Epoch: 00036 loss_train: 0.4231 acc_train: 0.8775 loss_val: 0.8540 acc_val: 0.7546 time: 0.1525s\n",
      "valid current auc-roc score: 0.936818, current macro_F score: 0.732391\n",
      "Epoch: 00037 loss_train: 0.3859 acc_train: 0.8799 loss_val: 0.7416 acc_val: 0.7823 time: 0.1580s\n",
      "valid current auc-roc score: 0.950686, current macro_F score: 0.769571\n",
      "Epoch: 00038 loss_train: 0.3932 acc_train: 0.8861 loss_val: 0.7578 acc_val: 0.7804 time: 0.1725s\n",
      "valid current auc-roc score: 0.949479, current macro_F score: 0.755396\n",
      "Epoch: 00039 loss_train: 0.3851 acc_train: 0.8904 loss_val: 0.7607 acc_val: 0.7786 time: 0.1703s\n",
      "valid current auc-roc score: 0.947461, current macro_F score: 0.753230\n",
      "Epoch: 00040 loss_train: 0.3817 acc_train: 0.8904 loss_val: 0.7745 acc_val: 0.7897 time: 0.1551s\n",
      "valid current auc-roc score: 0.939996, current macro_F score: 0.763019\n",
      "Epoch: 00041 loss_train: 0.3782 acc_train: 0.8762 loss_val: 0.8002 acc_val: 0.7491 time: 0.1672s\n",
      "valid current auc-roc score: 0.943698, current macro_F score: 0.730070\n",
      "Test set results: loss= 0.7141 accuracy= 0.7841\n",
      "test current auc-roc score: 0.947946, current macro_F score: 0.761355\n",
      "Epoch: 00042 loss_train: 0.3623 acc_train: 0.8805 loss_val: 0.7758 acc_val: 0.7638 time: 0.1259s\n",
      "valid current auc-roc score: 0.944733, current macro_F score: 0.743242\n",
      "Epoch: 00043 loss_train: 0.3919 acc_train: 0.8762 loss_val: 0.7540 acc_val: 0.7454 time: 0.1516s\n",
      "valid current auc-roc score: 0.949034, current macro_F score: 0.721737\n",
      "Epoch: 00044 loss_train: 0.3541 acc_train: 0.8885 loss_val: 0.7719 acc_val: 0.7804 time: 0.1396s\n",
      "valid current auc-roc score: 0.946779, current macro_F score: 0.764100\n",
      "Epoch: 00045 loss_train: 0.3654 acc_train: 0.8867 loss_val: 0.7497 acc_val: 0.7786 time: 0.1331s\n",
      "valid current auc-roc score: 0.948854, current macro_F score: 0.756292\n",
      "Epoch: 00046 loss_train: 0.3660 acc_train: 0.8879 loss_val: 0.6970 acc_val: 0.7786 time: 0.1458s\n",
      "valid current auc-roc score: 0.954786, current macro_F score: 0.757290\n",
      "Epoch: 00047 loss_train: 0.3530 acc_train: 0.8947 loss_val: 0.7048 acc_val: 0.7934 time: 0.1136s\n",
      "valid current auc-roc score: 0.950139, current macro_F score: 0.770838\n",
      "Epoch: 00048 loss_train: 0.3094 acc_train: 0.9046 loss_val: 0.7195 acc_val: 0.7749 time: 0.1342s\n",
      "valid current auc-roc score: 0.953128, current macro_F score: 0.756709\n",
      "Epoch: 00049 loss_train: 0.3279 acc_train: 0.8966 loss_val: 0.8077 acc_val: 0.7675 time: 0.1251s\n",
      "valid current auc-roc score: 0.942572, current macro_F score: 0.743098\n",
      "Epoch: 00050 loss_train: 0.3596 acc_train: 0.8898 loss_val: 0.7503 acc_val: 0.7731 time: 0.1185s\n",
      "valid current auc-roc score: 0.944075, current macro_F score: 0.744833\n",
      "Epoch: 00051 loss_train: 0.3283 acc_train: 0.8978 loss_val: 0.7489 acc_val: 0.7731 time: 0.1190s\n",
      "valid current auc-roc score: 0.947314, current macro_F score: 0.749242\n",
      "Test set results: loss= 0.6969 accuracy= 0.7804\n",
      "test current auc-roc score: 0.950594, current macro_F score: 0.762630\n",
      "Epoch: 00052 loss_train: 0.3105 acc_train: 0.9076 loss_val: 0.7635 acc_val: 0.7675 time: 0.1312s\n",
      "valid current auc-roc score: 0.944591, current macro_F score: 0.750594\n",
      "Epoch: 00053 loss_train: 0.3143 acc_train: 0.9083 loss_val: 0.7359 acc_val: 0.7878 time: 0.1314s\n",
      "valid current auc-roc score: 0.951849, current macro_F score: 0.767824\n",
      "Epoch: 00054 loss_train: 0.3074 acc_train: 0.8990 loss_val: 0.7294 acc_val: 0.7915 time: 0.1334s\n",
      "valid current auc-roc score: 0.951978, current macro_F score: 0.767627\n",
      "Epoch: 00055 loss_train: 0.3083 acc_train: 0.9083 loss_val: 0.7039 acc_val: 0.8007 time: 0.1466s\n",
      "valid current auc-roc score: 0.954673, current macro_F score: 0.778438\n",
      "Epoch: 00056 loss_train: 0.3257 acc_train: 0.8972 loss_val: 0.8086 acc_val: 0.7749 time: 0.1423s\n",
      "valid current auc-roc score: 0.945671, current macro_F score: 0.747282\n",
      "Epoch: 00057 loss_train: 0.2884 acc_train: 0.9132 loss_val: 0.7804 acc_val: 0.7786 time: 0.1670s\n",
      "valid current auc-roc score: 0.948946, current macro_F score: 0.771168\n",
      "Epoch: 00058 loss_train: 0.2802 acc_train: 0.9132 loss_val: 0.8013 acc_val: 0.7620 time: 0.1405s\n",
      "valid current auc-roc score: 0.945995, current macro_F score: 0.732763\n",
      "Epoch: 00059 loss_train: 0.3054 acc_train: 0.9009 loss_val: 0.7451 acc_val: 0.7952 time: 0.1445s\n",
      "valid current auc-roc score: 0.949647, current macro_F score: 0.768429\n",
      "Epoch: 00060 loss_train: 0.3213 acc_train: 0.9009 loss_val: 0.8237 acc_val: 0.7472 time: 0.1292s\n",
      "valid current auc-roc score: 0.944718, current macro_F score: 0.716174\n",
      "Epoch: 00061 loss_train: 0.2802 acc_train: 0.9095 loss_val: 0.8051 acc_val: 0.7565 time: 0.1372s\n",
      "valid current auc-roc score: 0.944236, current macro_F score: 0.734185\n",
      "Test set results: loss= 0.7119 accuracy= 0.7804\n",
      "test current auc-roc score: 0.949207, current macro_F score: 0.756640\n",
      "Epoch: 00062 loss_train: 0.2868 acc_train: 0.9064 loss_val: 0.7889 acc_val: 0.7786 time: 0.1233s\n",
      "valid current auc-roc score: 0.944943, current macro_F score: 0.746787\n",
      "Epoch: 00063 loss_train: 0.2734 acc_train: 0.9150 loss_val: 0.8658 acc_val: 0.7841 time: 0.1551s\n",
      "valid current auc-roc score: 0.938051, current macro_F score: 0.761167\n",
      "Epoch: 00064 loss_train: 0.2776 acc_train: 0.9132 loss_val: 0.7904 acc_val: 0.7565 time: 0.1177s\n",
      "valid current auc-roc score: 0.948085, current macro_F score: 0.738473\n",
      "Epoch: 00065 loss_train: 0.2913 acc_train: 0.9119 loss_val: 0.7837 acc_val: 0.7897 time: 0.1107s\n",
      "valid current auc-roc score: 0.945696, current macro_F score: 0.759406\n",
      "Epoch: 00066 loss_train: 0.2808 acc_train: 0.9095 loss_val: 0.7841 acc_val: 0.7638 time: 0.1070s\n",
      "valid current auc-roc score: 0.946550, current macro_F score: 0.747197\n",
      "Epoch: 00067 loss_train: 0.2643 acc_train: 0.9163 loss_val: 0.7866 acc_val: 0.7804 time: 0.1177s\n",
      "valid current auc-roc score: 0.944883, current macro_F score: 0.756543\n",
      "Epoch: 00068 loss_train: 0.2754 acc_train: 0.9126 loss_val: 0.7725 acc_val: 0.7601 time: 0.1175s\n",
      "valid current auc-roc score: 0.948764, current macro_F score: 0.727942\n",
      "Epoch: 00069 loss_train: 0.2755 acc_train: 0.9169 loss_val: 0.7770 acc_val: 0.7675 time: 0.1218s\n",
      "valid current auc-roc score: 0.948724, current macro_F score: 0.747958\n",
      "Epoch: 00070 loss_train: 0.2658 acc_train: 0.9261 loss_val: 0.7568 acc_val: 0.7860 time: 0.1300s\n",
      "valid current auc-roc score: 0.949210, current macro_F score: 0.758146\n",
      "Epoch: 00071 loss_train: 0.2498 acc_train: 0.9329 loss_val: 0.7452 acc_val: 0.7749 time: 0.1114s\n",
      "valid current auc-roc score: 0.949250, current macro_F score: 0.758333\n",
      "Test set results: loss= 0.7367 accuracy= 0.7823\n",
      "test current auc-roc score: 0.947877, current macro_F score: 0.759447\n",
      "Epoch: 00072 loss_train: 0.2727 acc_train: 0.9273 loss_val: 0.7532 acc_val: 0.7970 time: 0.1109s\n",
      "valid current auc-roc score: 0.947504, current macro_F score: 0.765594\n",
      "Epoch: 00073 loss_train: 0.2564 acc_train: 0.9249 loss_val: 0.7041 acc_val: 0.8026 time: 0.1121s\n",
      "valid current auc-roc score: 0.953164, current macro_F score: 0.783411\n",
      "Epoch: 00074 loss_train: 0.2569 acc_train: 0.9255 loss_val: 0.7900 acc_val: 0.7786 time: 0.1150s\n",
      "valid current auc-roc score: 0.946629, current macro_F score: 0.751895\n",
      "Epoch: 00075 loss_train: 0.2617 acc_train: 0.9212 loss_val: 0.8268 acc_val: 0.7731 time: 0.1334s\n",
      "valid current auc-roc score: 0.947767, current macro_F score: 0.751106\n",
      "Epoch: 00076 loss_train: 0.2486 acc_train: 0.9261 loss_val: 0.7477 acc_val: 0.8026 time: 0.1271s\n",
      "valid current auc-roc score: 0.948766, current macro_F score: 0.775669\n",
      "Epoch: 00077 loss_train: 0.2596 acc_train: 0.9224 loss_val: 0.7648 acc_val: 0.7804 time: 0.1297s\n",
      "valid current auc-roc score: 0.950272, current macro_F score: 0.761253\n",
      "Epoch: 00078 loss_train: 0.2614 acc_train: 0.9187 loss_val: 0.7271 acc_val: 0.8007 time: 0.1500s\n",
      "valid current auc-roc score: 0.954535, current macro_F score: 0.782919\n",
      "Epoch: 00079 loss_train: 0.2711 acc_train: 0.9138 loss_val: 0.8110 acc_val: 0.7823 time: 0.1245s\n",
      "valid current auc-roc score: 0.945110, current macro_F score: 0.765385\n",
      "Epoch: 00080 loss_train: 0.2698 acc_train: 0.9119 loss_val: 0.7606 acc_val: 0.7970 time: 0.1255s\n",
      "valid current auc-roc score: 0.948000, current macro_F score: 0.776024\n",
      "Epoch: 00081 loss_train: 0.2569 acc_train: 0.9200 loss_val: 0.7836 acc_val: 0.7768 time: 0.1374s\n",
      "valid current auc-roc score: 0.947411, current macro_F score: 0.753142\n",
      "Test set results: loss= 0.7134 accuracy= 0.8007\n",
      "test current auc-roc score: 0.953617, current macro_F score: 0.772718\n",
      "Epoch: 00082 loss_train: 0.2598 acc_train: 0.9243 loss_val: 0.7834 acc_val: 0.7694 time: 0.1251s\n",
      "valid current auc-roc score: 0.948960, current macro_F score: 0.749140\n",
      "Epoch: 00083 loss_train: 0.2464 acc_train: 0.9292 loss_val: 0.8060 acc_val: 0.7804 time: 0.1413s\n",
      "valid current auc-roc score: 0.948994, current macro_F score: 0.756271\n",
      "Epoch: 00084 loss_train: 0.2315 acc_train: 0.9317 loss_val: 0.7478 acc_val: 0.8007 time: 0.1346s\n",
      "valid current auc-roc score: 0.950659, current macro_F score: 0.780110\n",
      "Epoch: 00085 loss_train: 0.2317 acc_train: 0.9323 loss_val: 0.7916 acc_val: 0.7823 time: 0.1228s\n",
      "valid current auc-roc score: 0.949511, current macro_F score: 0.771549\n",
      "Epoch: 00086 loss_train: 0.2476 acc_train: 0.9317 loss_val: 0.8390 acc_val: 0.7897 time: 0.1274s\n",
      "valid current auc-roc score: 0.942561, current macro_F score: 0.764199\n",
      "Epoch: 00087 loss_train: 0.2275 acc_train: 0.9323 loss_val: 0.8426 acc_val: 0.7915 time: 0.1274s\n",
      "valid current auc-roc score: 0.938834, current macro_F score: 0.769270\n",
      "Epoch: 00088 loss_train: 0.2231 acc_train: 0.9323 loss_val: 0.7192 acc_val: 0.7878 time: 0.1365s\n",
      "valid current auc-roc score: 0.957231, current macro_F score: 0.775725\n",
      "Epoch: 00089 loss_train: 0.2244 acc_train: 0.9273 loss_val: 0.7560 acc_val: 0.7878 time: 0.1130s\n",
      "valid current auc-roc score: 0.950926, current macro_F score: 0.767306\n",
      "Epoch: 00090 loss_train: 0.2228 acc_train: 0.9347 loss_val: 0.7916 acc_val: 0.7952 time: 0.1097s\n",
      "valid current auc-roc score: 0.950641, current macro_F score: 0.779425\n",
      "Epoch: 00091 loss_train: 0.2158 acc_train: 0.9335 loss_val: 0.8014 acc_val: 0.7712 time: 0.1218s\n",
      "valid current auc-roc score: 0.945547, current macro_F score: 0.743790\n",
      "Test set results: loss= 0.7529 accuracy= 0.7768\n",
      "test current auc-roc score: 0.951142, current macro_F score: 0.753325\n",
      "Epoch: 00092 loss_train: 0.2027 acc_train: 0.9366 loss_val: 0.7908 acc_val: 0.7749 time: 0.1292s\n",
      "valid current auc-roc score: 0.949991, current macro_F score: 0.757211\n",
      "Epoch: 00093 loss_train: 0.2160 acc_train: 0.9347 loss_val: 0.8262 acc_val: 0.7860 time: 0.1352s\n",
      "valid current auc-roc score: 0.939808, current macro_F score: 0.754217\n",
      "Epoch: 00094 loss_train: 0.2162 acc_train: 0.9347 loss_val: 0.8723 acc_val: 0.7804 time: 0.1670s\n",
      "valid current auc-roc score: 0.935781, current macro_F score: 0.760725\n",
      "Epoch: 00095 loss_train: 0.2074 acc_train: 0.9341 loss_val: 0.8842 acc_val: 0.7841 time: 0.1410s\n",
      "valid current auc-roc score: 0.943953, current macro_F score: 0.768884\n",
      "Epoch: 00096 loss_train: 0.2109 acc_train: 0.9440 loss_val: 0.8034 acc_val: 0.7768 time: 0.1528s\n",
      "valid current auc-roc score: 0.947082, current macro_F score: 0.750731\n",
      "Epoch: 00097 loss_train: 0.2240 acc_train: 0.9267 loss_val: 0.8165 acc_val: 0.7675 time: 0.1621s\n",
      "valid current auc-roc score: 0.946040, current macro_F score: 0.743013\n",
      "Epoch: 00098 loss_train: 0.2325 acc_train: 0.9273 loss_val: 0.7933 acc_val: 0.7804 time: 0.1148s\n",
      "valid current auc-roc score: 0.949354, current macro_F score: 0.765716\n",
      "Epoch: 00099 loss_train: 0.2178 acc_train: 0.9329 loss_val: 0.9063 acc_val: 0.7454 time: 0.1367s\n",
      "valid current auc-roc score: 0.937446, current macro_F score: 0.720239\n",
      "Epoch: 00100 loss_train: 0.2172 acc_train: 0.9298 loss_val: 0.8599 acc_val: 0.7712 time: 0.1495s\n",
      "valid current auc-roc score: 0.945412, current macro_F score: 0.752163\n",
      "Epoch: 00101 loss_train: 0.2284 acc_train: 0.9298 loss_val: 0.8766 acc_val: 0.7583 time: 0.1327s\n",
      "valid current auc-roc score: 0.940209, current macro_F score: 0.740784\n",
      "Test set results: loss= 0.7466 accuracy= 0.7768\n",
      "test current auc-roc score: 0.951737, current macro_F score: 0.749595\n",
      "Epoch: 00102 loss_train: 0.2262 acc_train: 0.9261 loss_val: 0.7762 acc_val: 0.7694 time: 0.1341s\n",
      "valid current auc-roc score: 0.947117, current macro_F score: 0.739906\n",
      "Epoch: 00103 loss_train: 0.2064 acc_train: 0.9323 loss_val: 0.7687 acc_val: 0.7915 time: 0.1767s\n",
      "valid current auc-roc score: 0.951412, current macro_F score: 0.770084\n",
      "Epoch: 00104 loss_train: 0.2057 acc_train: 0.9390 loss_val: 0.7253 acc_val: 0.7989 time: 0.1778s\n",
      "valid current auc-roc score: 0.950053, current macro_F score: 0.778194\n",
      "Epoch: 00105 loss_train: 0.1977 acc_train: 0.9372 loss_val: 0.7170 acc_val: 0.7768 time: 0.1616s\n",
      "valid current auc-roc score: 0.953425, current macro_F score: 0.749013\n",
      "Epoch: 00106 loss_train: 0.2093 acc_train: 0.9347 loss_val: 0.8150 acc_val: 0.7860 time: 0.1626s\n",
      "valid current auc-roc score: 0.941581, current macro_F score: 0.756365\n",
      "Epoch: 00107 loss_train: 0.2136 acc_train: 0.9360 loss_val: 0.8165 acc_val: 0.7620 time: 0.1464s\n",
      "valid current auc-roc score: 0.947288, current macro_F score: 0.742352\n",
      "Epoch: 00108 loss_train: 0.2087 acc_train: 0.9415 loss_val: 0.7838 acc_val: 0.7952 time: 0.1577s\n",
      "valid current auc-roc score: 0.949838, current macro_F score: 0.778579\n",
      "Epoch: 00109 loss_train: 0.2037 acc_train: 0.9353 loss_val: 0.8269 acc_val: 0.7694 time: 0.1790s\n",
      "valid current auc-roc score: 0.945935, current macro_F score: 0.751169\n",
      "Epoch: 00110 loss_train: 0.2075 acc_train: 0.9360 loss_val: 0.8175 acc_val: 0.7804 time: 0.1767s\n",
      "valid current auc-roc score: 0.946476, current macro_F score: 0.763338\n",
      "Epoch: 00111 loss_train: 0.1951 acc_train: 0.9372 loss_val: 0.7759 acc_val: 0.7878 time: 0.1571s\n",
      "valid current auc-roc score: 0.950353, current macro_F score: 0.758557\n",
      "Test set results: loss= 0.7497 accuracy= 0.7897\n",
      "test current auc-roc score: 0.951057, current macro_F score: 0.768803\n",
      "Epoch: 00112 loss_train: 0.2076 acc_train: 0.9366 loss_val: 0.7657 acc_val: 0.7804 time: 0.1334s\n",
      "valid current auc-roc score: 0.949280, current macro_F score: 0.742589\n",
      "Epoch: 00113 loss_train: 0.1990 acc_train: 0.9415 loss_val: 0.8810 acc_val: 0.7786 time: 0.1412s\n",
      "valid current auc-roc score: 0.943928, current macro_F score: 0.766297\n",
      "Epoch: 00114 loss_train: 0.2069 acc_train: 0.9335 loss_val: 0.9192 acc_val: 0.7657 time: 0.1508s\n",
      "valid current auc-roc score: 0.937009, current macro_F score: 0.734634\n",
      "Epoch: 00115 loss_train: 0.1703 acc_train: 0.9532 loss_val: 0.8170 acc_val: 0.7860 time: 0.1455s\n",
      "valid current auc-roc score: 0.947003, current macro_F score: 0.761319\n",
      "Epoch: 00116 loss_train: 0.2098 acc_train: 0.9360 loss_val: 0.8934 acc_val: 0.7657 time: 0.1494s\n",
      "valid current auc-roc score: 0.945292, current macro_F score: 0.758163\n",
      "Epoch: 00117 loss_train: 0.2032 acc_train: 0.9384 loss_val: 0.8657 acc_val: 0.7731 time: 0.1454s\n",
      "valid current auc-roc score: 0.943855, current macro_F score: 0.741532\n",
      "Epoch: 00118 loss_train: 0.2109 acc_train: 0.9304 loss_val: 0.8136 acc_val: 0.7860 time: 0.1484s\n",
      "valid current auc-roc score: 0.949228, current macro_F score: 0.763048\n",
      "Epoch: 00119 loss_train: 0.1989 acc_train: 0.9366 loss_val: 0.7849 acc_val: 0.7804 time: 0.1413s\n",
      "valid current auc-roc score: 0.949568, current macro_F score: 0.754795\n",
      "Epoch: 00120 loss_train: 0.2051 acc_train: 0.9298 loss_val: 0.8131 acc_val: 0.7860 time: 0.1386s\n",
      "valid current auc-roc score: 0.948973, current macro_F score: 0.767646\n",
      "Epoch: 00121 loss_train: 0.1898 acc_train: 0.9427 loss_val: 0.8029 acc_val: 0.7804 time: 0.1223s\n",
      "valid current auc-roc score: 0.949504, current macro_F score: 0.766503\n",
      "Test set results: loss= 0.7863 accuracy= 0.7786\n",
      "test current auc-roc score: 0.950004, current macro_F score: 0.755373\n",
      "Epoch: 00122 loss_train: 0.1948 acc_train: 0.9280 loss_val: 0.8830 acc_val: 0.7657 time: 0.1224s\n",
      "valid current auc-roc score: 0.935078, current macro_F score: 0.744601\n",
      "Epoch: 00123 loss_train: 0.1999 acc_train: 0.9372 loss_val: 0.8352 acc_val: 0.7897 time: 0.1471s\n",
      "valid current auc-roc score: 0.942844, current macro_F score: 0.769972\n",
      "Epoch: 00124 loss_train: 0.1905 acc_train: 0.9446 loss_val: 0.8211 acc_val: 0.7823 time: 0.1164s\n",
      "valid current auc-roc score: 0.947376, current macro_F score: 0.758936\n",
      "Epoch: 00125 loss_train: 0.1757 acc_train: 0.9378 loss_val: 0.9430 acc_val: 0.7620 time: 0.1201s\n",
      "valid current auc-roc score: 0.937463, current macro_F score: 0.729657\n",
      "Epoch: 00126 loss_train: 0.1855 acc_train: 0.9372 loss_val: 0.9043 acc_val: 0.7841 time: 0.1519s\n",
      "valid current auc-roc score: 0.932210, current macro_F score: 0.760087\n",
      "Epoch: 00127 loss_train: 0.1993 acc_train: 0.9323 loss_val: 0.8630 acc_val: 0.7786 time: 0.1533s\n",
      "valid current auc-roc score: 0.944917, current macro_F score: 0.758988\n",
      "Epoch: 00128 loss_train: 0.2071 acc_train: 0.9317 loss_val: 0.8343 acc_val: 0.7897 time: 0.1425s\n",
      "valid current auc-roc score: 0.941454, current macro_F score: 0.761771\n",
      "Epoch: 00129 loss_train: 0.1913 acc_train: 0.9477 loss_val: 0.9005 acc_val: 0.7804 time: 0.1589s\n",
      "valid current auc-roc score: 0.943686, current macro_F score: 0.764036\n",
      "Epoch: 00130 loss_train: 0.1916 acc_train: 0.9397 loss_val: 0.8510 acc_val: 0.7841 time: 0.1503s\n",
      "valid current auc-roc score: 0.946502, current macro_F score: 0.754827\n",
      "Epoch: 00131 loss_train: 0.1836 acc_train: 0.9403 loss_val: 0.9352 acc_val: 0.7823 time: 0.1185s\n",
      "valid current auc-roc score: 0.941807, current macro_F score: 0.760628\n",
      "Test set results: loss= 0.8253 accuracy= 0.7823\n",
      "test current auc-roc score: 0.946166, current macro_F score: 0.761925\n",
      "Epoch: 00132 loss_train: 0.2087 acc_train: 0.9341 loss_val: 0.8410 acc_val: 0.7786 time: 0.1193s\n",
      "valid current auc-roc score: 0.941129, current macro_F score: 0.760618\n",
      "Epoch: 00133 loss_train: 0.1882 acc_train: 0.9378 loss_val: 0.8431 acc_val: 0.7841 time: 0.1205s\n",
      "valid current auc-roc score: 0.945546, current macro_F score: 0.767393\n",
      "Epoch: 00134 loss_train: 0.1855 acc_train: 0.9341 loss_val: 0.7837 acc_val: 0.7804 time: 0.1243s\n",
      "valid current auc-roc score: 0.948992, current macro_F score: 0.758868\n",
      "Epoch: 00135 loss_train: 0.1784 acc_train: 0.9477 loss_val: 0.8446 acc_val: 0.7749 time: 0.1195s\n",
      "valid current auc-roc score: 0.946181, current macro_F score: 0.760622\n",
      "Epoch: 00136 loss_train: 0.1913 acc_train: 0.9366 loss_val: 0.8551 acc_val: 0.7712 time: 0.1075s\n",
      "valid current auc-roc score: 0.938888, current macro_F score: 0.737762\n",
      "Epoch: 00137 loss_train: 0.1814 acc_train: 0.9403 loss_val: 0.8166 acc_val: 0.7860 time: 0.1339s\n",
      "valid current auc-roc score: 0.949958, current macro_F score: 0.756861\n",
      "Epoch: 00138 loss_train: 0.2015 acc_train: 0.9304 loss_val: 0.8301 acc_val: 0.7768 time: 0.1210s\n",
      "valid current auc-roc score: 0.945776, current macro_F score: 0.749316\n",
      "Epoch: 00139 loss_train: 0.1776 acc_train: 0.9440 loss_val: 0.8835 acc_val: 0.8007 time: 0.1177s\n",
      "valid current auc-roc score: 0.937016, current macro_F score: 0.779458\n",
      "Epoch: 00140 loss_train: 0.1931 acc_train: 0.9366 loss_val: 0.9097 acc_val: 0.7934 time: 0.1151s\n",
      "valid current auc-roc score: 0.936131, current macro_F score: 0.766415\n",
      "Epoch: 00141 loss_train: 0.1676 acc_train: 0.9514 loss_val: 0.8819 acc_val: 0.7989 time: 0.1113s\n",
      "valid current auc-roc score: 0.938814, current macro_F score: 0.773192\n",
      "Test set results: loss= 0.7819 accuracy= 0.7952\n",
      "test current auc-roc score: 0.949374, current macro_F score: 0.772804\n",
      "Epoch: 00142 loss_train: 0.1670 acc_train: 0.9501 loss_val: 0.8882 acc_val: 0.7878 time: 0.1363s\n",
      "valid current auc-roc score: 0.940964, current macro_F score: 0.765914\n",
      "Epoch: 00143 loss_train: 0.1845 acc_train: 0.9372 loss_val: 0.8754 acc_val: 0.7768 time: 0.1329s\n",
      "valid current auc-roc score: 0.943829, current macro_F score: 0.747588\n",
      "Epoch: 00144 loss_train: 0.1760 acc_train: 0.9470 loss_val: 0.8469 acc_val: 0.7804 time: 0.1458s\n",
      "valid current auc-roc score: 0.945520, current macro_F score: 0.761367\n",
      "Epoch: 00145 loss_train: 0.1673 acc_train: 0.9477 loss_val: 0.8155 acc_val: 0.7823 time: 0.1477s\n",
      "valid current auc-roc score: 0.949834, current macro_F score: 0.761952\n",
      "Epoch: 00146 loss_train: 0.1719 acc_train: 0.9446 loss_val: 0.8299 acc_val: 0.7731 time: 0.1427s\n",
      "valid current auc-roc score: 0.947013, current macro_F score: 0.754094\n",
      "Epoch: 00147 loss_train: 0.1818 acc_train: 0.9464 loss_val: 0.8583 acc_val: 0.7915 time: 0.1424s\n",
      "valid current auc-roc score: 0.945035, current macro_F score: 0.771121\n",
      "Epoch: 00148 loss_train: 0.1584 acc_train: 0.9470 loss_val: 0.8960 acc_val: 0.7694 time: 0.1405s\n",
      "valid current auc-roc score: 0.942947, current macro_F score: 0.744656\n",
      "Epoch: 00149 loss_train: 0.1832 acc_train: 0.9409 loss_val: 0.8968 acc_val: 0.7620 time: 0.2168s\n",
      "valid current auc-roc score: 0.940399, current macro_F score: 0.740679\n",
      "Epoch: 00150 loss_train: 0.1674 acc_train: 0.9452 loss_val: 0.8573 acc_val: 0.7749 time: 0.1492s\n",
      "valid current auc-roc score: 0.951000, current macro_F score: 0.742186\n",
      "Epoch: 00151 loss_train: 0.1695 acc_train: 0.9415 loss_val: 0.8341 acc_val: 0.7915 time: 0.1594s\n",
      "valid current auc-roc score: 0.951247, current macro_F score: 0.770564\n",
      "Test set results: loss= 0.7876 accuracy= 0.7952\n",
      "test current auc-roc score: 0.949743, current macro_F score: 0.772275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m loss_vals\u001b[39m.\u001b[39mappend(loss_val)\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m         test(epoch)\n",
      "Cell \u001b[1;32mIn [8], line 12\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m acc_test \u001b[39m=\u001b[39m accuracy(outputs[test_idx], labels[test_idx])\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest set results:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mloss= \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(loss_test\u001b[39m.\u001b[39mitem()),\n\u001b[0;32m     11\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39maccuracy= \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(acc_test\u001b[39m.\u001b[39mitem()))\n\u001b[1;32m---> 12\u001b[0m print_class_acc(outputs[test_idx], labels[test_idx], pre\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained 2 layer classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_n.state_dict()\n",
    "    saved_content['classifier'] = classifier_n.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Normal_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "encoder_s = GCN_Encoder_s(nfeat = features.shape[1],\n",
    "        nhid = args.n_hidden,\n",
    "        nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "classifier_s = GCN_Classifier_s(nembed=args.n_hidden, \n",
    "        nhid = args.n_hidden, \n",
    "        nclass = labels.max().item() + 1, \n",
    "        dropout = args.dropout, device = device, init = args.initalization)\n",
    "decoder_s = Decoder_s(nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "# optimizer_en = optim.Adam(encoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_cls = optim.Adam(classifier_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_de = optim.SGD(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.RMSprop(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.RMSprop(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.RMSprop(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder_s.train()\n",
    "    classifier_s.train()\n",
    "    decoder_s.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon' or args.setting == 'newG_cls':\n",
    "        ori_num = labels.shape[0]\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.recon_upsample(embed, labels, train_idx, adj = adj_mtx.detach().to_dense(), portion = args.up_scale, im_class_num = args.im_class_num)\n",
    "        generated_G = decoder_s(embed)\n",
    "\n",
    "        loss_rec = utils.adj_mse_loss(generated_G[: ori_num, :][:, : ori_num], adj_mtx.detach().to_dense())\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = copy.deepcopy(generated_G.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "\n",
    "            #ipdb.set_trace()\n",
    "            edge_ac = adj_new[: ori_num, : ori_num].eq(adj_mtx.to_dense()).double().sum()/(ori_num**2)\n",
    "        else:\n",
    "            adj_new = generated_G\n",
    "            edge_ac = F.l1_loss(adj_new[: ori_num, : ori_num], adj_mtx.to_dense(), reduction = 'mean')\n",
    "\n",
    "\n",
    "        #calculate generation information\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"after filtering, edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new[:ori_num, :][:, :ori_num] = adj_mtx.detach().to_dense()\n",
    "        #adj_new = adj_new.to_sparse()\n",
    "        #ipdb.set_trace()\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "        if args.setting == 'newG_cls':\n",
    "            idx_train_new = train_idx\n",
    "\n",
    "    elif args.setting == 'embed_up':\n",
    "        #perform SMOTE in embedding space\n",
    "        embed, labels_new, idx_train_new = utils.recon_upsample(embed, labels, train_idx, portion=args.up_scale, im_class_num = args.im_class_num)\n",
    "        adj_new = adj_mtx\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = train_idx\n",
    "        adj_new = adj_mtx\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    output = classifier_s(embed, adj_new)\n",
    "\n",
    "\n",
    "\n",
    "    if args.setting == 'reweight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        weight[-args.im_class_num:] = 1 + args.up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1), weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1))\n",
    "\n",
    "    acc_train = accuracy(output[train_idx], labels_new[train_idx].reshape(-1))\n",
    "    if args.setting == 'recon_newG':\n",
    "        loss = loss_train + loss_rec * args.rec_weight\n",
    "    elif args.setting == 'recon':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "\n",
    "    loss.backward()\n",
    "    if args.setting == 'newG_cls':\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_de.zero_grad()\n",
    "    else:\n",
    "        optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon':\n",
    "        optimizer_de.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "    acc_val = accuracy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_s.eval()\n",
    "    classifier_s.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "    outputs = classifier_s(embed, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(args.epochs)):\n",
    "        train(epoch, features, labels)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained GraphSMOTE's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_s.state_dict()\n",
    "    saved_content['classifier'] = classifier_s.state_dict()\n",
    "    saved_content['decoder'] = decoder_s.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/GraphSMOTE_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "from utils.reweight import next, next2\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_w(nfeat = n_features, \n",
    "        nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, \n",
    "        device = device, init = args.initalization)\n",
    "# optimizer = optim.Adam(encoder.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "def train(epoch, features, labels):\n",
    "        encoder.train()\n",
    "        # classifier.train()\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        with higher.innerloop_ctx(encoder, optimizer) as (meta_model, meta_opt):\n",
    "                meta_train_outputs = meta_model(features, adj_mtx)\n",
    "                # criterion.reduction = 'none'\n",
    "                # new_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_train_outputs[train_idx], labels[train_idx])\n",
    "                # meta_train_loss = criterion(meta_train_outputs[train_idx], new_labels[train_idx])\n",
    "                eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=device)\n",
    "                meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "                meta_opt.step(meta_train_loss)\n",
    "                sampled_val_idx, new_adj_mtx, new_features, new_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                # meta_inputs, meta_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                meta_val_idx, meta_adj_mtx, meta_features, meta_labels = sampled_val_idx.to(device=device, non_blocking=True), \\\n",
    "                        new_adj_mtx.to(device=device, non_blocking=True), \\\n",
    "                        new_features.to(device=device, non_blocking=True), \\\n",
    "                        new_labels.to(device=device, non_blocking=True)\n",
    "                meta_val_outputs = meta_model(meta_features, meta_adj_mtx.double())\n",
    "                # criterion.reduction = 'mean'\n",
    "                # new_meta_labels = F.one_hot(meta_labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_val_outputs, meta_labels)\n",
    "                # meta_val_loss = criterion(meta_val_outputs, new_meta_labels)\n",
    "                eps_grads = torch.autograd.grad(meta_train_loss, eps)[0].detach()\n",
    "        w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "        l1_norm = torch.sum(w_tilde)\n",
    "        if l1_norm != 0:\n",
    "                w = w_tilde / l1_norm\n",
    "        else:\n",
    "                w = w_tilde\n",
    "        outputs = encoder(features, adj_mtx.double())\n",
    "        # criterion.reduction = 'none'\n",
    "        # new_main_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "        loss = F.cross_entropy(outputs[train_idx], labels[train_idx])\n",
    "        # loss = criterion(outputs[train_idx], new_main_labels[train_idx])\n",
    "        loss = torch.sum(w * loss)\n",
    "\n",
    "        loss_train = F.cross_entropy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "        acc_train = accuracy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = F.cross_entropy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "        acc_val = accuracy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        # keep track of epoch loss/accuracy\n",
    "        # train_loss += loss.item() * outputs.shape[0]\n",
    "\n",
    "                # out = output[train_idx]\n",
    "                # gt = labels[train_idx].reshape(-1)\n",
    "                # if args.setting == 'reweight':\n",
    "                #         weight = \"STH\"\n",
    "                #         loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "                # else:\n",
    "                #         loss_train = F.cross_entropy(out, gt)\n",
    "                # acc_train = accuracy(out, gt)\n",
    "                # loss_train.backward()\n",
    "                # optimizer_en.step()\n",
    "                # optimizer_cls.step()\n",
    "                # gt_v = labels[test_idx].reshape(-1)\n",
    "                # out_v = output[test_idx]\n",
    "                # loss_val = F.cross_entropy(out_v, gt_v)\n",
    "                # acc_val = accuracy(out_v, gt_v)\n",
    "                # # print_class_acc(out_v, gt_v)\n",
    "                # print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "                # 'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                # 'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                # 'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                # 'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                # 'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        # return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    outputs = encoder(features, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(args.epochs)):\n",
    "        train(epoch, features, labels)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the reweight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Reweight_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arc_selection-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f122987ad9a59b0c863ec73977cb4833edd644652b774e5b01a9e2fe636c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
