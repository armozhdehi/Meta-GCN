{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.GraphConvolution import GCN_Encoder_s, GCN_Classifier_s, Decoder_s\n",
    "from utils.GraphConvolution import GraphConvolution, GCN_Encoder3\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_W = 0.01\n",
    "    dropout = 0.5\n",
    "    dropout_W = 0.5\n",
    "    gamma = 1\n",
    "    no_cuda = False\n",
    "    train_ratio=0.6\n",
    "    test_ratio=0.2\n",
    "    n_classes = 2\n",
    "    seed = 12345\n",
    "    torch.manual_seed(seed)\n",
    "    # dataset = \"cora\"\n",
    "    # dataset = \"haberman\"\n",
    "    dataset = \"diabetes\"\n",
    "    order = 4\n",
    "    n_features = 0\n",
    "    w_val_size = 10\n",
    "    imbalance_ratio = None\n",
    "    n_hidden = 64\n",
    "    setting = None\n",
    "    im_class_num = 1\n",
    "    setting = \"upsampling\"\n",
    "    opt_new_G = False\n",
    "    up_scale = 1\n",
    "    im_ratio = 0.5\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import data_loader_diabetes, data_loader_haberman, data_loader_cora\n",
    "\n",
    "cora_adj_mtx, cora_labels_df, cora_features_df, \\\n",
    "        cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features = data_loader_cora(args)\n",
    "        \n",
    "diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n",
    "        diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features = data_loader_diabetes(args)\n",
    "\n",
    "haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n",
    "        haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features = data_loader_haberman(args)\n",
    "\n",
    "if args.dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = diabetes_n_features\n",
    "    features = diabetes_features_df\n",
    "    labels = diabetes_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif args.dataset == \"cora\":\n",
    "    adj_mtx = cora_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = cora_n_features\n",
    "    features = cora_features_df\n",
    "    labels = cora_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = cora_train_idx\n",
    "    val_idx = cora_val_idx\n",
    "    test_idx = cora_test_idx\n",
    "elif args.dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = haberman_n_features\n",
    "    features = haberman_features_df\n",
    "    labels = haberman_labels_df\n",
    "    # train_X = haberman_train_X_df\n",
    "    # train_Y = haberman_train_Y_df\n",
    "    # val_X = haberman_val_X_df\n",
    "    # val_Y = haberman_val_Y_df\n",
    "    # test_X = haberman_test_X_df\n",
    "    # test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(pd.DataFrame(labels[train_idx])[0].unique()) == len(pd.DataFrame(labels[val_idx])[0].unique()) == len(pd.DataFrame(labels[test_idx])[0].unique()), \\\n",
    "#     \"There are some classes missing in one the 3 partitiones of the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if False else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to Tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.from_numpy(np.concatenate((train_X, val_X, test_X), axis=0)).to(device)\n",
    "# labels = torch.from_numpy(np.int64(np.concatenate((train_Y, val_Y, test_Y), axis=0))).to(device)\n",
    "train_idx = torch.from_numpy(np.array(train_idx, dtype = np.int64)).to(device)\n",
    "val_idx = torch.from_numpy(np.array(val_idx, dtype = np.int64)).to(device)\n",
    "test_idx = torch.from_numpy(np.array(test_idx, dtype = np.int64)).to(device)\n",
    "features = torch.from_numpy(np.array(features, dtype = np.float64)).to(device)\n",
    "labels = torch.from_numpy(np.array(labels, dtype = np.int64)).to(device)\n",
    "try:\n",
    "    adj_mtx = torch.from_numpy(np.array(adj_mtx, dtype = np.float64)).to(device)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_s(nfeat = n_features, nhid = n_hidden[-1], nembed = n_hidden[-1], dropout = args.dropout)\n",
    "classifier = GCN_Classifier_s(nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "optimizer_en = optim.Adam(encoder.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "optimizer_cls = optim.Adam(classifier.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "encoder.train()\n",
    "classifier.train()\n",
    "def train(epoch):\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        embed = encoder(features, adj_mtx)\n",
    "        output = classifier(embed, adj_mtx)\n",
    "        out = output[train_idx]\n",
    "        gt = labels[train_idx].reshape(-1)\n",
    "        if args.setting == 'reweight':\n",
    "                weight = \"STH\"\n",
    "                loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "        else:\n",
    "                loss_train = F.cross_entropy(out, gt)\n",
    "        acc_train = accuracy(out, gt)\n",
    "        loss_train.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        gt_v = labels[test_idx].reshape(-1)\n",
    "        out_v = output[test_idx]\n",
    "        loss_val = F.cross_entropy(out_v, gt_v)\n",
    "        acc_val = accuracy(out_v, gt_v)\n",
    "        # print_class_acc(out_v, gt_v)\n",
    "        print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 10208.4686 acc_train: 0.4500 loss_val: 11054.1048 acc_val: 0.4091 time: 0.0248s\n",
      "Epoch: 00002 loss_train: 22299.7362 acc_train: 0.6522 loss_val: 20013.5462 acc_val: 0.6494 time: 0.0187s\n",
      "Epoch: 00003 loss_train: 6761.6032 acc_train: 0.6413 loss_val: 6606.7487 acc_val: 0.6234 time: 0.0195s\n",
      "Epoch: 00004 loss_train: 3793.2409 acc_train: 0.5022 loss_val: 4248.3098 acc_val: 0.4870 time: 0.0186s\n",
      "Epoch: 00005 loss_train: 1047.7026 acc_train: 0.5457 loss_val: 1419.8706 acc_val: 0.5130 time: 0.0247s\n",
      "Epoch: 00006 loss_train: 1197.4908 acc_train: 0.6435 loss_val: 1082.0656 acc_val: 0.6169 time: 0.0282s\n",
      "Epoch: 00007 loss_train: 875.7768 acc_train: 0.6391 loss_val: 613.3173 acc_val: 0.6494 time: 0.0237s\n",
      "Epoch: 00008 loss_train: 673.2004 acc_train: 0.4457 loss_val: 945.5011 acc_val: 0.4675 time: 0.0274s\n",
      "Epoch: 00009 loss_train: 388.6753 acc_train: 0.5457 loss_val: 425.8187 acc_val: 0.5519 time: 0.0262s\n",
      "Epoch: 00010 loss_train: 205.2893 acc_train: 0.5652 loss_val: 166.3254 acc_val: 0.5649 time: 0.0226s\n",
      "Epoch: 00011 loss_train: 137.1050 acc_train: 0.6174 loss_val: 98.7453 acc_val: 0.5779 time: 0.0325s\n",
      "Epoch: 00012 loss_train: 148.3344 acc_train: 0.5174 loss_val: 241.7230 acc_val: 0.4675 time: 0.0429s\n",
      "Epoch: 00013 loss_train: 57.0357 acc_train: 0.6565 loss_val: 60.6980 acc_val: 0.6429 time: 0.0298s\n",
      "Epoch: 00014 loss_train: 51.3573 acc_train: 0.5174 loss_val: 70.2769 acc_val: 0.4610 time: 0.0266s\n",
      "Epoch: 00015 loss_train: 40.2898 acc_train: 0.6065 loss_val: 26.7727 acc_val: 0.6039 time: 0.0216s\n",
      "Epoch: 00016 loss_train: 23.2906 acc_train: 0.5696 loss_val: 24.1725 acc_val: 0.5519 time: 0.0235s\n",
      "Epoch: 00017 loss_train: 34.7622 acc_train: 0.5152 loss_val: 42.7447 acc_val: 0.5519 time: 0.0200s\n",
      "Epoch: 00018 loss_train: 23.3594 acc_train: 0.4826 loss_val: 26.2110 acc_val: 0.5000 time: 0.0236s\n",
      "Epoch: 00019 loss_train: 20.7128 acc_train: 0.5478 loss_val: 13.8029 acc_val: 0.5974 time: 0.0203s\n",
      "Epoch: 00020 loss_train: 21.2457 acc_train: 0.5630 loss_val: 20.2722 acc_val: 0.5584 time: 0.0223s\n",
      "Epoch: 00021 loss_train: 13.7932 acc_train: 0.5522 loss_val: 12.0681 acc_val: 0.5519 time: 0.0279s\n",
      "Epoch: 00022 loss_train: 10.2071 acc_train: 0.6391 loss_val: 12.2760 acc_val: 0.5779 time: 0.0253s\n",
      "Epoch: 00023 loss_train: 7.1546 acc_train: 0.5587 loss_val: 6.2170 acc_val: 0.5844 time: 0.0239s\n",
      "Epoch: 00024 loss_train: 4.5804 acc_train: 0.5522 loss_val: 3.6439 acc_val: 0.5390 time: 0.0222s\n",
      "Epoch: 00025 loss_train: 3.0479 acc_train: 0.5826 loss_val: 3.5036 acc_val: 0.5325 time: 0.0219s\n",
      "Epoch: 00026 loss_train: 2.6983 acc_train: 0.4848 loss_val: 2.3726 acc_val: 0.5584 time: 0.0212s\n",
      "Epoch: 00027 loss_train: 1.7489 acc_train: 0.6435 loss_val: 1.5664 acc_val: 0.6623 time: 0.0232s\n",
      "Epoch: 00028 loss_train: 1.6708 acc_train: 0.6326 loss_val: 1.3023 acc_val: 0.5779 time: 0.0221s\n",
      "Epoch: 00029 loss_train: 1.3104 acc_train: 0.5217 loss_val: 1.3528 acc_val: 0.5195 time: 0.0229s\n",
      "Epoch: 00030 loss_train: 1.0320 acc_train: 0.5783 loss_val: 0.8973 acc_val: 0.5455 time: 0.0266s\n",
      "Epoch: 00031 loss_train: 1.5606 acc_train: 0.4543 loss_val: 1.6796 acc_val: 0.3896 time: 0.0244s\n",
      "Epoch: 00032 loss_train: 1.1448 acc_train: 0.4543 loss_val: 1.4268 acc_val: 0.4481 time: 0.0282s\n",
      "Epoch: 00033 loss_train: 0.9329 acc_train: 0.5283 loss_val: 0.9558 acc_val: 0.4740 time: 0.0242s\n",
      "Epoch: 00034 loss_train: 1.7907 acc_train: 0.5500 loss_val: 1.4893 acc_val: 0.4935 time: 0.0252s\n",
      "Epoch: 00035 loss_train: 1.1553 acc_train: 0.3500 loss_val: 1.4698 acc_val: 0.3506 time: 0.0254s\n",
      "Epoch: 00036 loss_train: 0.8635 acc_train: 0.5152 loss_val: 0.9478 acc_val: 0.4286 time: 0.0240s\n",
      "Epoch: 00037 loss_train: 1.2939 acc_train: 0.6457 loss_val: 1.0717 acc_val: 0.6364 time: 0.0285s\n",
      "Epoch: 00038 loss_train: 0.9505 acc_train: 0.6478 loss_val: 0.8896 acc_val: 0.6429 time: 0.0276s\n",
      "Epoch: 00039 loss_train: 0.7585 acc_train: 0.6522 loss_val: 0.7026 acc_val: 0.6494 time: 0.0304s\n",
      "Epoch: 00040 loss_train: 0.6770 acc_train: 0.6522 loss_val: 0.6617 acc_val: 0.6494 time: 0.0280s\n",
      "Epoch: 00041 loss_train: 0.6850 acc_train: 0.6522 loss_val: 0.6817 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00042 loss_train: 0.6882 acc_train: 0.6522 loss_val: 0.6886 acc_val: 0.6429 time: 0.0238s\n",
      "Epoch: 00043 loss_train: 0.6871 acc_train: 0.6522 loss_val: 0.6872 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00044 loss_train: 0.6859 acc_train: 0.6522 loss_val: 0.6860 acc_val: 0.6494 time: 0.0268s\n",
      "Epoch: 00045 loss_train: 0.6844 acc_train: 0.6522 loss_val: 0.6846 acc_val: 0.6494 time: 0.0302s\n",
      "Epoch: 00046 loss_train: 0.6831 acc_train: 0.6522 loss_val: 0.6833 acc_val: 0.6494 time: 0.0264s\n",
      "Epoch: 00047 loss_train: 0.6817 acc_train: 0.6522 loss_val: 0.6820 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00048 loss_train: 0.6802 acc_train: 0.6522 loss_val: 0.6805 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00049 loss_train: 0.6785 acc_train: 0.6522 loss_val: 0.6788 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00050 loss_train: 0.6771 acc_train: 0.6522 loss_val: 0.6774 acc_val: 0.6494 time: 0.0247s\n",
      "Epoch: 00051 loss_train: 0.6757 acc_train: 0.6522 loss_val: 0.6768 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00052 loss_train: 0.6812 acc_train: 0.6348 loss_val: 0.6832 acc_val: 0.6299 time: 0.0234s\n",
      "Epoch: 00053 loss_train: 0.6724 acc_train: 0.6522 loss_val: 0.6729 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00054 loss_train: 0.6710 acc_train: 0.6522 loss_val: 0.6716 acc_val: 0.6494 time: 0.0231s\n",
      "Epoch: 00055 loss_train: 0.6696 acc_train: 0.6522 loss_val: 0.6700 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00056 loss_train: 0.6680 acc_train: 0.6522 loss_val: 0.6687 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00057 loss_train: 0.6665 acc_train: 0.6522 loss_val: 0.6674 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00058 loss_train: 0.6654 acc_train: 0.6522 loss_val: 0.6660 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00059 loss_train: 0.6642 acc_train: 0.6522 loss_val: 0.6649 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00060 loss_train: 0.6629 acc_train: 0.6522 loss_val: 0.6636 acc_val: 0.6494 time: 0.0197s\n",
      "Epoch: 00061 loss_train: 0.6617 acc_train: 0.6522 loss_val: 0.6624 acc_val: 0.6494 time: 0.0174s\n",
      "Epoch: 00062 loss_train: 0.6601 acc_train: 0.6522 loss_val: 0.6609 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00063 loss_train: 0.6595 acc_train: 0.6522 loss_val: 0.6604 acc_val: 0.6494 time: 0.0189s\n",
      "Epoch: 00064 loss_train: 0.6584 acc_train: 0.6522 loss_val: 0.6593 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00065 loss_train: 0.6574 acc_train: 0.6522 loss_val: 0.6583 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00066 loss_train: 0.6566 acc_train: 0.6522 loss_val: 0.6575 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00067 loss_train: 0.6556 acc_train: 0.6522 loss_val: 0.6564 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00068 loss_train: 0.6549 acc_train: 0.6522 loss_val: 0.6559 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00069 loss_train: 0.6538 acc_train: 0.6522 loss_val: 0.6551 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00070 loss_train: 0.6534 acc_train: 0.6522 loss_val: 0.6546 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00071 loss_train: 0.6527 acc_train: 0.6522 loss_val: 0.6539 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00072 loss_train: 0.6512 acc_train: 0.6522 loss_val: 0.6534 acc_val: 0.6494 time: 0.0237s\n",
      "Epoch: 00073 loss_train: 0.6516 acc_train: 0.6522 loss_val: 0.6528 acc_val: 0.6494 time: 0.0146s\n",
      "Epoch: 00074 loss_train: 0.6512 acc_train: 0.6522 loss_val: 0.6524 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00075 loss_train: 0.6507 acc_train: 0.6522 loss_val: 0.6519 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00076 loss_train: 0.6502 acc_train: 0.6522 loss_val: 0.6517 acc_val: 0.6494 time: 0.0263s\n",
      "Epoch: 00077 loss_train: 0.6493 acc_train: 0.6522 loss_val: 0.6507 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00078 loss_train: 0.6493 acc_train: 0.6522 loss_val: 0.6507 acc_val: 0.6494 time: 0.0192s\n",
      "Epoch: 00079 loss_train: 0.6490 acc_train: 0.6522 loss_val: 0.6504 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00080 loss_train: 0.6489 acc_train: 0.6522 loss_val: 0.6505 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00081 loss_train: 0.6484 acc_train: 0.6522 loss_val: 0.6498 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00082 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00083 loss_train: 0.6474 acc_train: 0.6522 loss_val: 0.6484 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00084 loss_train: 0.6476 acc_train: 0.6522 loss_val: 0.6492 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00085 loss_train: 0.6475 acc_train: 0.6522 loss_val: 0.6491 acc_val: 0.6494 time: 0.0237s\n",
      "Epoch: 00086 loss_train: 0.6473 acc_train: 0.6522 loss_val: 0.6489 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00087 loss_train: 0.6471 acc_train: 0.6522 loss_val: 0.6487 acc_val: 0.6494 time: 0.0275s\n",
      "Epoch: 00088 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6486 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00089 loss_train: 0.6489 acc_train: 0.6500 loss_val: 0.6468 acc_val: 0.6558 time: 0.0250s\n",
      "Epoch: 00090 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6482 acc_val: 0.6494 time: 0.0284s\n",
      "Epoch: 00091 loss_train: 0.6833 acc_train: 0.6152 loss_val: 0.6996 acc_val: 0.5909 time: 0.0252s\n",
      "Epoch: 00092 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6481 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00093 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6480 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00094 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6498 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00095 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6480 acc_val: 0.6494 time: 0.0237s\n",
      "Epoch: 00096 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0267s\n",
      "Epoch: 00097 loss_train: 0.6490 acc_train: 0.6522 loss_val: 0.6540 acc_val: 0.6494 time: 0.0326s\n",
      "Epoch: 00098 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00099 loss_train: 6.0648 acc_train: 0.4870 loss_val: 4.9545 acc_val: 0.5455 time: 0.0216s\n",
      "Epoch: 00100 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00101 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00102 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00103 loss_train: 0.6485 acc_train: 0.6543 loss_val: 0.6480 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00104 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00105 loss_train: 0.6460 acc_train: 0.6435 loss_val: 0.6675 acc_val: 0.6429 time: 0.0221s\n",
      "Epoch: 00106 loss_train: 0.9164 acc_train: 0.6543 loss_val: 1.1861 acc_val: 0.6299 time: 0.0218s\n",
      "Epoch: 00107 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00108 loss_train: 0.6425 acc_train: 0.6587 loss_val: 0.6409 acc_val: 0.6623 time: 0.0191s\n",
      "Epoch: 00109 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6471 acc_val: 0.6494 time: 0.0239s\n",
      "Epoch: 00110 loss_train: 0.6439 acc_train: 0.6587 loss_val: 0.6490 acc_val: 0.6429 time: 0.0201s\n",
      "Epoch: 00111 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00112 loss_train: 0.6739 acc_train: 0.6522 loss_val: 0.6895 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00113 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00114 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00115 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6472 acc_val: 0.6494 time: 0.0252s\n",
      "Epoch: 00116 loss_train: 0.6508 acc_train: 0.6522 loss_val: 0.6564 acc_val: 0.6494 time: 0.0256s\n",
      "Epoch: 00117 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6481 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00118 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00119 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6468 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00120 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00121 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6480 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00122 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6480 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00123 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6472 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00124 loss_train: 0.6428 acc_train: 0.6565 loss_val: 0.6421 acc_val: 0.6558 time: 0.0245s\n",
      "Epoch: 00125 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00126 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0259s\n",
      "Epoch: 00127 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00128 loss_train: 0.6480 acc_train: 0.6500 loss_val: 0.6473 acc_val: 0.6494 time: 0.0192s\n",
      "Epoch: 00129 loss_train: 0.6491 acc_train: 0.6522 loss_val: 0.6529 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00130 loss_train: 0.6422 acc_train: 0.6565 loss_val: 0.6554 acc_val: 0.6429 time: 0.0204s\n",
      "Epoch: 00131 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6506 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00132 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00133 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00134 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0307s\n",
      "Epoch: 00135 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0254s\n",
      "Epoch: 00136 loss_train: 0.6419 acc_train: 0.6565 loss_val: 0.6470 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00137 loss_train: 0.6485 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0235s\n",
      "Epoch: 00138 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00139 loss_train: 0.6469 acc_train: 0.6522 loss_val: 0.6467 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00140 loss_train: 0.6468 acc_train: 0.6522 loss_val: 0.6481 acc_val: 0.6494 time: 0.0254s\n",
      "Epoch: 00141 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6485 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00142 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0186s\n",
      "Epoch: 00143 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00144 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00145 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6299 acc_val: 0.6494 time: 0.0182s\n",
      "Epoch: 00146 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6472 acc_val: 0.6494 time: 0.0235s\n",
      "Epoch: 00147 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6482 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00148 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0191s\n",
      "Epoch: 00149 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00150 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6481 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00151 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00152 loss_train: 0.7305 acc_train: 0.6522 loss_val: 1.0160 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00153 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00154 loss_train: 0.6490 acc_train: 0.6522 loss_val: 0.6489 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00155 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00156 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00157 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6455 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00158 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00159 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00160 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6468 acc_val: 0.6494 time: 0.0171s\n",
      "Epoch: 00161 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6455 acc_val: 0.6494 time: 0.0193s\n",
      "Epoch: 00162 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00163 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00164 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6465 acc_val: 0.6494 time: 0.0198s\n",
      "Epoch: 00165 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6465 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00166 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00167 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6450 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00168 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6451 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00169 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00170 loss_train: 0.6430 acc_train: 0.6587 loss_val: 0.6389 acc_val: 0.6558 time: 0.0207s\n",
      "Epoch: 00171 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6455 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00172 loss_train: 0.6419 acc_train: 0.6522 loss_val: 0.6446 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00173 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00174 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6447 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00175 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00176 loss_train: 0.6371 acc_train: 0.6652 loss_val: 0.6715 acc_val: 0.6429 time: 0.0222s\n",
      "Epoch: 00177 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0237s\n",
      "Epoch: 00178 loss_train: 0.6426 acc_train: 0.6565 loss_val: 0.6429 acc_val: 0.6558 time: 0.0234s\n",
      "Epoch: 00179 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0177s\n",
      "Epoch: 00180 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6453 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00181 loss_train: 0.6391 acc_train: 0.6565 loss_val: 0.6384 acc_val: 0.6623 time: 0.0306s\n",
      "Epoch: 00182 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00183 loss_train: 0.6414 acc_train: 0.6565 loss_val: 0.6476 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00184 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6447 acc_val: 0.6494 time: 0.0281s\n",
      "Epoch: 00185 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6437 acc_val: 0.6494 time: 0.0329s\n",
      "Epoch: 00186 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0253s\n",
      "Epoch: 00187 loss_train: 0.6432 acc_train: 0.6717 loss_val: 0.6492 acc_val: 0.6494 time: 0.0292s\n",
      "Epoch: 00188 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6450 acc_val: 0.6494 time: 0.0279s\n",
      "Epoch: 00189 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0267s\n",
      "Epoch: 00190 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6540 acc_val: 0.6364 time: 0.0291s\n",
      "Epoch: 00191 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6472 acc_val: 0.6494 time: 0.0289s\n",
      "Epoch: 00192 loss_train: 0.6409 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6429 time: 0.0259s\n",
      "Epoch: 00193 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0288s\n",
      "Epoch: 00194 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0259s\n",
      "Epoch: 00195 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6448 acc_val: 0.6494 time: 0.0245s\n",
      "Epoch: 00196 loss_train: 0.6428 acc_train: 0.6522 loss_val: 0.6464 acc_val: 0.6494 time: 0.0186s\n",
      "Epoch: 00197 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0238s\n",
      "Epoch: 00198 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00199 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00200 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00201 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6468 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00202 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6496 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00203 loss_train: 0.6574 acc_train: 0.6522 loss_val: 0.6307 acc_val: 0.6623 time: 0.0219s\n",
      "Epoch: 00204 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00205 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00206 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6466 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00207 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0233s\n",
      "Epoch: 00208 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00209 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00210 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6449 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00211 loss_train: 0.6468 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00212 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6447 acc_val: 0.6494 time: 0.0273s\n",
      "Epoch: 00213 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00214 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6447 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00215 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00216 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0198s\n",
      "Epoch: 00217 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00218 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00219 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6466 acc_val: 0.6494 time: 0.0233s\n",
      "Epoch: 00220 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00221 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0248s\n",
      "Epoch: 00222 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0296s\n",
      "Epoch: 00223 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0291s\n",
      "Epoch: 00224 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00225 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0265s\n",
      "Epoch: 00226 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0292s\n",
      "Epoch: 00227 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00228 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00229 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0290s\n",
      "Epoch: 00230 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6466 acc_val: 0.6494 time: 0.0286s\n",
      "Epoch: 00231 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0243s\n",
      "Epoch: 00232 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00233 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00234 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6453 acc_val: 0.6494 time: 0.0245s\n",
      "Epoch: 00235 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00236 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00237 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6469 acc_val: 0.6494 time: 0.0289s\n",
      "Epoch: 00238 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0247s\n",
      "Epoch: 00239 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6450 acc_val: 0.6494 time: 0.0253s\n",
      "Epoch: 00240 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0251s\n",
      "Epoch: 00241 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00242 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00243 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0242s\n",
      "Epoch: 00244 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0245s\n",
      "Epoch: 00245 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6453 acc_val: 0.6494 time: 0.0182s\n",
      "Epoch: 00246 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6480 acc_val: 0.6494 time: 0.0240s\n",
      "Epoch: 00247 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00248 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0271s\n",
      "Epoch: 00249 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6455 acc_val: 0.6494 time: 0.0239s\n",
      "Epoch: 00250 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0260s\n",
      "Epoch: 00251 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00252 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0272s\n",
      "Epoch: 00253 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6451 acc_val: 0.6494 time: 0.0305s\n",
      "Epoch: 00254 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0248s\n",
      "Epoch: 00255 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0260s\n",
      "Epoch: 00256 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6450 acc_val: 0.6494 time: 0.0252s\n",
      "Epoch: 00257 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0253s\n",
      "Epoch: 00258 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6453 acc_val: 0.6494 time: 0.0282s\n",
      "Epoch: 00259 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6452 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00260 loss_train: 0.6464 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0247s\n",
      "Epoch: 00261 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00262 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6451 acc_val: 0.6494 time: 0.0311s\n",
      "Epoch: 00263 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0287s\n",
      "Epoch: 00264 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0281s\n",
      "Epoch: 00265 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0291s\n",
      "Epoch: 00266 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0304s\n",
      "Epoch: 00267 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6471 acc_val: 0.6494 time: 0.0298s\n",
      "Epoch: 00268 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6496 acc_val: 0.6494 time: 0.0257s\n",
      "Epoch: 00269 loss_train: 26.4460 acc_train: 0.6522 loss_val: 24.1076 acc_val: 0.6494 time: 0.0256s\n",
      "Epoch: 00270 loss_train: 0.7614 acc_train: 0.6522 loss_val: 0.6660 acc_val: 0.6494 time: 0.0264s\n",
      "Epoch: 00271 loss_train: 38.7759 acc_train: 0.4413 loss_val: 44.5413 acc_val: 0.4610 time: 0.0255s\n",
      "Epoch: 00272 loss_train: 0.7200 acc_train: 0.5870 loss_val: 0.7487 acc_val: 0.5779 time: 0.0248s\n",
      "Epoch: 00273 loss_train: 8.2624 acc_train: 0.4935 loss_val: 9.3704 acc_val: 0.4545 time: 0.0242s\n",
      "Epoch: 00274 loss_train: 0.6683 acc_train: 0.6522 loss_val: 0.6214 acc_val: 0.6494 time: 0.0189s\n",
      "Epoch: 00275 loss_train: 0.7256 acc_train: 0.6522 loss_val: 0.6721 acc_val: 0.6494 time: 0.0303s\n",
      "Epoch: 00276 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6464 acc_val: 0.6494 time: 0.0285s\n",
      "Epoch: 00277 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0256s\n",
      "Epoch: 00278 loss_train: 0.6225 acc_train: 0.6522 loss_val: 0.6213 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00279 loss_train: 2.9394 acc_train: 0.6522 loss_val: 3.3578 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00280 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0246s\n",
      "Epoch: 00281 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0238s\n",
      "Epoch: 00282 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00283 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00284 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00285 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0182s\n",
      "Epoch: 00286 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6464 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00287 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00288 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00289 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00290 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00291 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00292 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00293 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00294 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00295 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00296 loss_train: 0.6425 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00297 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00298 loss_train: 0.6464 acc_train: 0.6522 loss_val: 0.6430 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00299 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00300 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0185s\n",
      "Epoch: 00301 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00302 loss_train: 0.6425 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0176s\n",
      "Epoch: 00303 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0197s\n",
      "Epoch: 00304 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00305 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0231s\n",
      "Epoch: 00306 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0287s\n",
      "Epoch: 00307 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00308 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0275s\n",
      "Epoch: 00309 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0246s\n",
      "Epoch: 00310 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0185s\n",
      "Epoch: 00311 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00312 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00313 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0188s\n",
      "Epoch: 00314 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0171s\n",
      "Epoch: 00315 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00316 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00317 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00318 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0155s\n",
      "Epoch: 00319 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0185s\n",
      "Epoch: 00320 loss_train: 0.6423 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00321 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0182s\n",
      "Epoch: 00322 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0162s\n",
      "Epoch: 00323 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00324 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00325 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0193s\n",
      "Epoch: 00326 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00327 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00328 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6449 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00329 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0190s\n",
      "Epoch: 00330 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0140s\n",
      "Epoch: 00331 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0150s\n",
      "Epoch: 00332 loss_train: 0.6430 acc_train: 0.6522 loss_val: 0.6463 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00333 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00334 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0198s\n",
      "Epoch: 00335 loss_train: 0.6469 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00336 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00337 loss_train: 0.6419 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00338 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6439 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00339 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00340 loss_train: 0.6471 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00341 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00342 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6471 acc_val: 0.6494 time: 0.0253s\n",
      "Epoch: 00343 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00344 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0180s\n",
      "Epoch: 00345 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00346 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0178s\n",
      "Epoch: 00347 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0176s\n",
      "Epoch: 00348 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00349 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00350 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00351 loss_train: 0.6428 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00352 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00353 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00354 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0257s\n",
      "Epoch: 00355 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0246s\n",
      "Epoch: 00356 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0184s\n",
      "Epoch: 00357 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0262s\n",
      "Epoch: 00358 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00359 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0351s\n",
      "Epoch: 00360 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00361 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0230s\n",
      "Epoch: 00362 loss_train: 0.6426 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00363 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00364 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00365 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0198s\n",
      "Epoch: 00366 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6470 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00367 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00368 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0190s\n",
      "Epoch: 00369 loss_train: 0.6468 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00370 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00371 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00372 loss_train: 0.6422 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0185s\n",
      "Epoch: 00373 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0152s\n",
      "Epoch: 00374 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00375 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00376 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00377 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00378 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00379 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00380 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00381 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00382 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0145s\n",
      "Epoch: 00383 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0176s\n",
      "Epoch: 00384 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0157s\n",
      "Epoch: 00385 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00386 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00387 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00388 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00389 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00390 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00391 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00392 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00393 loss_train: 0.6478 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00394 loss_train: 0.6413 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00395 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00396 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6447 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00397 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00398 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00399 loss_train: 0.6472 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0170s\n",
      "Epoch: 00400 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00401 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0245s\n",
      "Epoch: 00402 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0337s\n",
      "Epoch: 00403 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0269s\n",
      "Epoch: 00404 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00405 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00406 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00407 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00408 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00409 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00410 loss_train: 0.6468 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00411 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0256s\n",
      "Epoch: 00412 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6467 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00413 loss_train: 0.6476 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00414 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00415 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6471 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00416 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00417 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00418 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00419 loss_train: 0.6471 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00420 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0238s\n",
      "Epoch: 00421 loss_train: 0.6476 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00422 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00423 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00424 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00425 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00426 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00427 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00428 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00429 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00430 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0179s\n",
      "Epoch: 00431 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00432 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00433 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00434 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6462 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00435 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0193s\n",
      "Epoch: 00436 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00437 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0195s\n",
      "Epoch: 00438 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00439 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0347s\n",
      "Epoch: 00440 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0306s\n",
      "Epoch: 00441 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0242s\n",
      "Epoch: 00442 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0302s\n",
      "Epoch: 00443 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0293s\n",
      "Epoch: 00444 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6461 acc_val: 0.6494 time: 0.0276s\n",
      "Epoch: 00445 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0313s\n",
      "Epoch: 00446 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0322s\n",
      "Epoch: 00447 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0310s\n",
      "Epoch: 00448 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00449 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00450 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0197s\n",
      "Epoch: 00451 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00452 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00453 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0229s\n",
      "Epoch: 00454 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0271s\n",
      "Epoch: 00455 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0259s\n",
      "Epoch: 00456 loss_train: 0.6430 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0254s\n",
      "Epoch: 00457 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0235s\n",
      "Epoch: 00458 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0303s\n",
      "Epoch: 00459 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0233s\n",
      "Epoch: 00460 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0240s\n",
      "Epoch: 00461 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00462 loss_train: 0.6424 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00463 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00464 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00465 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00466 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00467 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0252s\n",
      "Epoch: 00468 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0190s\n",
      "Epoch: 00469 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00470 loss_train: 0.6464 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00471 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0188s\n",
      "Epoch: 00472 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00473 loss_train: 0.6419 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00474 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00475 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0257s\n",
      "Epoch: 00476 loss_train: 0.6428 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00477 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00478 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00479 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0165s\n",
      "Epoch: 00480 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00481 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00482 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00483 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00484 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00485 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00486 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00487 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00488 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0235s\n",
      "Epoch: 00489 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0274s\n",
      "Epoch: 00490 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0262s\n",
      "Epoch: 00491 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0231s\n",
      "Epoch: 00492 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00493 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00494 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00495 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00496 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0248s\n",
      "Epoch: 00497 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00498 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00499 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00500 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00501 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00502 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00503 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0190s\n",
      "Epoch: 00504 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00505 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0192s\n",
      "Epoch: 00506 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00507 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00508 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00509 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00510 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00511 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0196s\n",
      "Epoch: 00512 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00513 loss_train: 0.6470 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0195s\n",
      "Epoch: 00514 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0149s\n",
      "Epoch: 00515 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00516 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00517 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6460 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00518 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00519 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0158s\n",
      "Epoch: 00520 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0141s\n",
      "Epoch: 00521 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0177s\n",
      "Epoch: 00522 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0188s\n",
      "Epoch: 00523 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00524 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0199s\n",
      "Epoch: 00525 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00526 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00527 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00528 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00529 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00530 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0175s\n",
      "Epoch: 00531 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00532 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00533 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00534 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00535 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00536 loss_train: 0.6420 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0255s\n",
      "Epoch: 00537 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0263s\n",
      "Epoch: 00538 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0316s\n",
      "Epoch: 00539 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0257s\n",
      "Epoch: 00540 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00541 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0247s\n",
      "Epoch: 00542 loss_train: 0.6464 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00543 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0190s\n",
      "Epoch: 00544 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00545 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00546 loss_train: 0.6462 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00547 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0196s\n",
      "Epoch: 00548 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00549 loss_train: 0.6430 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00550 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00551 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00552 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0270s\n",
      "Epoch: 00553 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00554 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00555 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00556 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00557 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0193s\n",
      "Epoch: 00558 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0242s\n",
      "Epoch: 00559 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00560 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00561 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00562 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00563 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00564 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00565 loss_train: 0.6426 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00566 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00567 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0277s\n",
      "Epoch: 00568 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00569 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0220s\n",
      "Epoch: 00570 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00571 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00572 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00573 loss_train: 0.6435 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0170s\n",
      "Epoch: 00574 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0195s\n",
      "Epoch: 00575 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0203s\n",
      "Epoch: 00576 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00577 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0227s\n",
      "Epoch: 00578 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00579 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00580 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00581 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0304s\n",
      "Epoch: 00582 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0317s\n",
      "Epoch: 00583 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0242s\n",
      "Epoch: 00584 loss_train: 0.6478 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00585 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0251s\n",
      "Epoch: 00586 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00587 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0218s\n",
      "Epoch: 00588 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0244s\n",
      "Epoch: 00589 loss_train: 0.6430 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0274s\n",
      "Epoch: 00590 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00591 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00592 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00593 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00594 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00595 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00596 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0206s\n",
      "Epoch: 00597 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0188s\n",
      "Epoch: 00598 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00599 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00600 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00601 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00602 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0192s\n",
      "Epoch: 00603 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0204s\n",
      "Epoch: 00604 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00605 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0210s\n",
      "Epoch: 00606 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00607 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0159s\n",
      "Epoch: 00608 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00609 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0231s\n",
      "Epoch: 00610 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00611 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00612 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00613 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00614 loss_train: 0.6432 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0239s\n",
      "Epoch: 00615 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0200s\n",
      "Epoch: 00616 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00617 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00618 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0225s\n",
      "Epoch: 00619 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00620 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00621 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0207s\n",
      "Epoch: 00622 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0215s\n",
      "Epoch: 00623 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0201s\n",
      "Epoch: 00624 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0213s\n",
      "Epoch: 00625 loss_train: 0.6437 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00626 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0214s\n",
      "Epoch: 00627 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0216s\n",
      "Epoch: 00628 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0271s\n",
      "Epoch: 00629 loss_train: 0.6422 acc_train: 0.6522 loss_val: 0.6455 acc_val: 0.6494 time: 0.0364s\n",
      "Epoch: 00630 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6479 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00631 loss_train: 0.6458 acc_train: 0.6522 loss_val: 0.6476 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00632 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0221s\n",
      "Epoch: 00633 loss_train: 0.6469 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00634 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0219s\n",
      "Epoch: 00635 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0226s\n",
      "Epoch: 00636 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00637 loss_train: 0.6426 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00638 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0198s\n",
      "Epoch: 00639 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00640 loss_train: 0.6465 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0194s\n",
      "Epoch: 00641 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0168s\n",
      "Epoch: 00642 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0183s\n",
      "Epoch: 00643 loss_train: 0.6467 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0212s\n",
      "Epoch: 00644 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0208s\n",
      "Epoch: 00645 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0209s\n",
      "Epoch: 00646 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0197s\n",
      "Epoch: 00647 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0222s\n",
      "Epoch: 00648 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0205s\n",
      "Epoch: 00649 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0230s\n",
      "Epoch: 00650 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0299s\n",
      "Epoch: 00651 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0256s\n",
      "Epoch: 00652 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0275s\n",
      "Epoch: 00653 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0339s\n",
      "Epoch: 00654 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0253s\n",
      "Epoch: 00655 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00656 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0326s\n",
      "Epoch: 00657 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0386s\n",
      "Epoch: 00658 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0286s\n",
      "Epoch: 00659 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0266s\n",
      "Epoch: 00660 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00661 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0249s\n",
      "Epoch: 00662 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0231s\n",
      "Epoch: 00663 loss_train: 0.6464 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0240s\n",
      "Epoch: 00664 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0275s\n",
      "Epoch: 00665 loss_train: 0.6451 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0276s\n",
      "Epoch: 00666 loss_train: 0.6454 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0291s\n",
      "Epoch: 00667 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0414s\n",
      "Epoch: 00668 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0398s\n",
      "Epoch: 00669 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0373s\n",
      "Epoch: 00670 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0367s\n",
      "Epoch: 00671 loss_train: 0.6471 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0286s\n",
      "Epoch: 00672 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0313s\n",
      "Epoch: 00673 loss_train: 0.6469 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0306s\n",
      "Epoch: 00674 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0286s\n",
      "Epoch: 00675 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0262s\n",
      "Epoch: 00676 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0329s\n",
      "Epoch: 00677 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0301s\n",
      "Epoch: 00678 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0250s\n",
      "Epoch: 00679 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0326s\n",
      "Epoch: 00680 loss_train: 0.6463 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0345s\n",
      "Epoch: 00681 loss_train: 0.6443 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0303s\n",
      "Epoch: 00682 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0293s\n",
      "Epoch: 00683 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0330s\n",
      "Epoch: 00684 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6477 acc_val: 0.6494 time: 0.0308s\n",
      "Epoch: 00685 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0302s\n",
      "Epoch: 00686 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6459 acc_val: 0.6494 time: 0.0365s\n",
      "Epoch: 00687 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0258s\n",
      "Epoch: 00688 loss_train: 0.6456 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0292s\n",
      "Epoch: 00689 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0267s\n",
      "Epoch: 00690 loss_train: 0.6424 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0308s\n",
      "Epoch: 00691 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0414s\n",
      "Epoch: 00692 loss_train: 0.6436 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0304s\n",
      "Epoch: 00693 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0343s\n",
      "Epoch: 00694 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0259s\n",
      "Epoch: 00695 loss_train: 0.6446 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0167s\n",
      "Epoch: 00696 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0202s\n",
      "Epoch: 00697 loss_train: 0.6448 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0211s\n",
      "Epoch: 00698 loss_train: 0.6421 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0189s\n",
      "Epoch: 00699 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0228s\n",
      "Epoch: 00700 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00701 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0254s\n",
      "Epoch: 00702 loss_train: 0.6439 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0409s\n",
      "Epoch: 00703 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0285s\n",
      "Epoch: 00704 loss_train: 0.6441 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0364s\n",
      "Epoch: 00705 loss_train: 0.6450 acc_train: 0.6522 loss_val: 0.6473 acc_val: 0.6494 time: 0.0341s\n",
      "Epoch: 00706 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6475 acc_val: 0.6494 time: 0.0368s\n",
      "Epoch: 00707 loss_train: 0.6466 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0282s\n",
      "Epoch: 00708 loss_train: 0.6431 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0489s\n",
      "Epoch: 00709 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6454 acc_val: 0.6494 time: 0.0409s\n",
      "Epoch: 00710 loss_train: 0.6452 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0261s\n",
      "Epoch: 00711 loss_train: 0.6455 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0259s\n",
      "Epoch: 00712 loss_train: 0.6453 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00713 loss_train: 0.6429 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00714 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0238s\n",
      "Epoch: 00715 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6457 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00716 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0217s\n",
      "Epoch: 00717 loss_train: 0.6445 acc_train: 0.6522 loss_val: 0.6456 acc_val: 0.6494 time: 0.0224s\n",
      "Epoch: 00718 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0325s\n",
      "Epoch: 00719 loss_train: 0.6460 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0270s\n",
      "Epoch: 00720 loss_train: 0.6433 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0296s\n",
      "Epoch: 00721 loss_train: 0.6461 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0312s\n",
      "Epoch: 00722 loss_train: 0.6424 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0303s\n",
      "Epoch: 00723 loss_train: 0.6444 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0314s\n",
      "Epoch: 00724 loss_train: 0.6427 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0374s\n",
      "Epoch: 00725 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0298s\n",
      "Epoch: 00726 loss_train: 0.6442 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0255s\n",
      "Epoch: 00727 loss_train: 0.6449 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0419s\n",
      "Epoch: 00728 loss_train: 0.6459 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0246s\n",
      "Epoch: 00729 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0232s\n",
      "Epoch: 00730 loss_train: 0.6440 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0241s\n",
      "Epoch: 00731 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0223s\n",
      "Epoch: 00732 loss_train: 0.6457 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00733 loss_train: 0.6447 acc_train: 0.6522 loss_val: 0.6474 acc_val: 0.6494 time: 0.0236s\n",
      "Epoch: 00734 loss_train: 0.6434 acc_train: 0.6522 loss_val: 0.6478 acc_val: 0.6494 time: 0.0234s\n",
      "Epoch: 00735 loss_train: 0.6438 acc_train: 0.6522 loss_val: 0.6458 acc_val: 0.6494 time: 0.0275s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loss_vals \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m----> 7\u001b[0m         acc_train, acc_val, loss_train, loss_val \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m      8\u001b[0m         acc_trains\u001b[39m.\u001b[39mappend(acc_train)\n\u001b[0;32m      9\u001b[0m         acc_vals\u001b[39m.\u001b[39mappend(acc_val)\n",
      "Cell \u001b[1;32mIn [11], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     27\u001b[0m optimizer_en\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     28\u001b[0m optimizer_cls\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m embed \u001b[39m=\u001b[39m encoder(features, adj_mtx)\n\u001b[0;32m     30\u001b[0m output \u001b[39m=\u001b[39m classifier(embed, adj_mtx)\n\u001b[0;32m     31\u001b[0m out \u001b[39m=\u001b[39m output[train_idx]\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\ENSF 619\\ENSF-619-Final-Project\\utils\\GraphConvolution.py:312\u001b[0m, in \u001b[0;36mGCN_Encoder3.forward\u001b[1;34m(self, x, adj, func)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(end_layer):\n\u001b[0;32m    311\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m--> 312\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgc[i](x, adj)\n\u001b[0;32m    313\u001b[0m     x \u001b[39m=\u001b[39m func(x)\n\u001b[0;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\ENSF 619\\ENSF-619-Final-Project\\utils\\GraphConvolution.py:188\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, adj):\n\u001b[0;32m    187\u001b[0m     support \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[1;32m--> 188\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mspmm(adj, support)\n\u001b[0;32m    189\u001b[0m     \u001b[39m#for 3_D batch, need a loop!!!\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 7260.1679 loss_rec: 7260.1679 acc_train: 0.4412 loss_val: 10124.0106 acc_val: 0.4355 time: 0.0050s\n",
      "Epoch: 00002 loss_train: 320.8344 loss_rec: 320.8344 acc_train: 0.9853 loss_val: 17279.4603 acc_val: 0.7097 time: 0.0040s\n",
      "Epoch: 00003 loss_train: 428.6254 loss_rec: 428.6254 acc_train: 0.9926 loss_val: 25191.0579 acc_val: 0.7097 time: 0.0033s\n",
      "Epoch: 00004 loss_train: 797.2419 loss_rec: 797.2419 acc_train: 0.9926 loss_val: 28697.5457 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00005 loss_train: 508.1976 loss_rec: 508.1976 acc_train: 0.9926 loss_val: 31478.7595 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00006 loss_train: 392.8181 loss_rec: 392.8181 acc_train: 0.9926 loss_val: 36842.1025 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00007 loss_train: 612.0097 loss_rec: 612.0097 acc_train: 0.9926 loss_val: 36808.0675 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00008 loss_train: 726.7453 loss_rec: 726.7453 acc_train: 0.9926 loss_val: 40227.7039 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00009 loss_train: 884.0822 loss_rec: 884.0822 acc_train: 0.9926 loss_val: 50048.2483 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00010 loss_train: 682.6430 loss_rec: 682.6430 acc_train: 0.9926 loss_val: 52570.2333 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00011 loss_train: 1188.6189 loss_rec: 1188.6189 acc_train: 0.9926 loss_val: 41014.9184 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00012 loss_train: 691.6822 loss_rec: 691.6822 acc_train: 0.9926 loss_val: 44741.4214 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00013 loss_train: 922.0090 loss_rec: 922.0090 acc_train: 0.9926 loss_val: 44775.2515 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00014 loss_train: 1048.3764 loss_rec: 1048.3764 acc_train: 0.9926 loss_val: 40077.7571 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00015 loss_train: 920.0035 loss_rec: 920.0035 acc_train: 0.9926 loss_val: 49579.1370 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00016 loss_train: 1051.4610 loss_rec: 1051.4610 acc_train: 0.9926 loss_val: 43526.5940 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00017 loss_train: 1053.9480 loss_rec: 1053.9480 acc_train: 0.9926 loss_val: 45192.7120 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00018 loss_train: 537.6906 loss_rec: 537.6906 acc_train: 0.9926 loss_val: 39679.5486 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00019 loss_train: 1180.2208 loss_rec: 1180.2208 acc_train: 0.9779 loss_val: 26840.1224 acc_val: 0.7419 time: 0.0025s\n",
      "Epoch: 00020 loss_train: 820.4877 loss_rec: 820.4877 acc_train: 0.9853 loss_val: 32805.0544 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00021 loss_train: 1032.6402 loss_rec: 1032.6402 acc_train: 0.9853 loss_val: 38250.1204 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00022 loss_train: 623.9952 loss_rec: 623.9952 acc_train: 0.9926 loss_val: 35429.0110 acc_val: 0.7097 time: 0.0033s\n",
      "Epoch: 00023 loss_train: 421.1575 loss_rec: 421.1575 acc_train: 0.9926 loss_val: 32137.0905 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00024 loss_train: 297.2549 loss_rec: 297.2549 acc_train: 0.9926 loss_val: 42209.9013 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00025 loss_train: 647.9462 loss_rec: 647.9462 acc_train: 0.9926 loss_val: 39714.5131 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00026 loss_train: 803.0297 loss_rec: 803.0297 acc_train: 0.9926 loss_val: 39728.7806 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00027 loss_train: 695.2332 loss_rec: 695.2332 acc_train: 0.9926 loss_val: 35990.5841 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00028 loss_train: 750.8983 loss_rec: 750.8983 acc_train: 0.9926 loss_val: 31584.2882 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00029 loss_train: 634.3022 loss_rec: 634.3022 acc_train: 0.9853 loss_val: 28088.8441 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00030 loss_train: 53.0807 loss_rec: 53.0807 acc_train: 0.9926 loss_val: 34423.2688 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00031 loss_train: 615.1045 loss_rec: 615.1045 acc_train: 0.9926 loss_val: 25228.5819 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00032 loss_train: 535.1567 loss_rec: 535.1567 acc_train: 0.9926 loss_val: 25194.2499 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00033 loss_train: 295.5944 loss_rec: 295.5944 acc_train: 0.9926 loss_val: 23480.7251 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00034 loss_train: 212.5621 loss_rec: 212.5621 acc_train: 0.9926 loss_val: 24757.0454 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00035 loss_train: 265.8074 loss_rec: 265.8074 acc_train: 0.9926 loss_val: 21794.0352 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00036 loss_train: 343.4587 loss_rec: 343.4587 acc_train: 0.9926 loss_val: 16623.5529 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00037 loss_train: 273.9823 loss_rec: 273.9823 acc_train: 0.9926 loss_val: 17232.0822 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00038 loss_train: 180.3542 loss_rec: 180.3542 acc_train: 0.9926 loss_val: 15073.6591 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00039 loss_train: 484.2832 loss_rec: 484.2832 acc_train: 0.9779 loss_val: 13774.6149 acc_val: 0.6774 time: 0.0037s\n",
      "Epoch: 00040 loss_train: 393.2436 loss_rec: 393.2436 acc_train: 0.9559 loss_val: 9369.3348 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00041 loss_train: 287.4251 loss_rec: 287.4251 acc_train: 0.9779 loss_val: 11198.0891 acc_val: 0.7097 time: 0.0020s\n",
      "Epoch: 00042 loss_train: 349.9870 loss_rec: 349.9870 acc_train: 0.9853 loss_val: 10831.3156 acc_val: 0.7419 time: 0.0037s\n",
      "Epoch: 00043 loss_train: 107.0821 loss_rec: 107.0821 acc_train: 0.9926 loss_val: 11857.3382 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00044 loss_train: 121.9554 loss_rec: 121.9554 acc_train: 0.9926 loss_val: 10982.8889 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00045 loss_train: 135.3777 loss_rec: 135.3777 acc_train: 0.9926 loss_val: 10795.6133 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00046 loss_train: 149.0545 loss_rec: 149.0545 acc_train: 0.9926 loss_val: 11217.7386 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00047 loss_train: 220.9223 loss_rec: 220.9223 acc_train: 0.9926 loss_val: 11495.0255 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00048 loss_train: 265.0101 loss_rec: 265.0101 acc_train: 0.9926 loss_val: 10193.5300 acc_val: 0.7258 time: 0.0015s\n",
      "Epoch: 00049 loss_train: 187.6393 loss_rec: 187.6393 acc_train: 0.9926 loss_val: 7146.4006 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00050 loss_train: 187.1144 loss_rec: 187.1144 acc_train: 0.9926 loss_val: 11561.7376 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00051 loss_train: 287.1753 loss_rec: 287.1753 acc_train: 0.9926 loss_val: 9784.0190 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00052 loss_train: 124.6363 loss_rec: 124.6363 acc_train: 0.9926 loss_val: 11142.8122 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00053 loss_train: 96.3469 loss_rec: 96.3469 acc_train: 0.9926 loss_val: 9921.9262 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00054 loss_train: 200.0540 loss_rec: 200.0540 acc_train: 0.9926 loss_val: 8104.6209 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00055 loss_train: 191.3067 loss_rec: 191.3067 acc_train: 0.9926 loss_val: 7519.6186 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00056 loss_train: 217.2556 loss_rec: 217.2556 acc_train: 0.9926 loss_val: 5660.5834 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00057 loss_train: 132.5529 loss_rec: 132.5529 acc_train: 0.9559 loss_val: 6768.0050 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00058 loss_train: 58.5244 loss_rec: 58.5244 acc_train: 0.9926 loss_val: 4169.8273 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00059 loss_train: 66.9033 loss_rec: 66.9033 acc_train: 0.9853 loss_val: 6013.2625 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00060 loss_train: 194.0160 loss_rec: 194.0160 acc_train: 0.9706 loss_val: 5239.0664 acc_val: 0.6935 time: 0.0035s\n",
      "Epoch: 00061 loss_train: 121.5256 loss_rec: 121.5256 acc_train: 0.9926 loss_val: 6239.8444 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00062 loss_train: 91.8561 loss_rec: 91.8561 acc_train: 0.9926 loss_val: 3802.0221 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00063 loss_train: 62.7417 loss_rec: 62.7417 acc_train: 0.9926 loss_val: 3950.1100 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00064 loss_train: 147.7437 loss_rec: 147.7437 acc_train: 0.9926 loss_val: 3883.1060 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00065 loss_train: 70.4887 loss_rec: 70.4887 acc_train: 0.9926 loss_val: 3748.6683 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00066 loss_train: 89.9817 loss_rec: 89.9817 acc_train: 0.9926 loss_val: 2255.5546 acc_val: 0.7097 time: 0.0038s\n",
      "Epoch: 00067 loss_train: 58.0721 loss_rec: 58.0721 acc_train: 0.9926 loss_val: 2915.5832 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00068 loss_train: 51.6641 loss_rec: 51.6641 acc_train: 0.9779 loss_val: 3050.6590 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00069 loss_train: 87.2790 loss_rec: 87.2790 acc_train: 0.9779 loss_val: 2509.9202 acc_val: 0.7097 time: 0.0035s\n",
      "Epoch: 00070 loss_train: 176.2869 loss_rec: 176.2869 acc_train: 0.9412 loss_val: 1544.4260 acc_val: 0.7258 time: 0.0076s\n",
      "Epoch: 00071 loss_train: 87.2311 loss_rec: 87.2311 acc_train: 0.9926 loss_val: 2208.4819 acc_val: 0.7258 time: 0.0060s\n",
      "Epoch: 00072 loss_train: 55.7433 loss_rec: 55.7433 acc_train: 0.9926 loss_val: 2477.5180 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00073 loss_train: 58.4210 loss_rec: 58.4210 acc_train: 0.9926 loss_val: 2633.4948 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00074 loss_train: 46.2373 loss_rec: 46.2373 acc_train: 0.9926 loss_val: 2580.8587 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00075 loss_train: 31.8621 loss_rec: 31.8621 acc_train: 0.9926 loss_val: 2462.2827 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00076 loss_train: 14.6339 loss_rec: 14.6339 acc_train: 0.9926 loss_val: 2488.4209 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00077 loss_train: 45.1548 loss_rec: 45.1548 acc_train: 0.9853 loss_val: 1610.4750 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00078 loss_train: 52.7604 loss_rec: 52.7604 acc_train: 0.9632 loss_val: 2123.1921 acc_val: 0.7419 time: 0.0030s\n",
      "Epoch: 00079 loss_train: 38.5764 loss_rec: 38.5764 acc_train: 0.9853 loss_val: 1746.6433 acc_val: 0.7419 time: 0.0031s\n",
      "Epoch: 00080 loss_train: 119.1493 loss_rec: 119.1493 acc_train: 0.9338 loss_val: 1405.9608 acc_val: 0.6935 time: 0.0019s\n",
      "Epoch: 00081 loss_train: 46.2218 loss_rec: 46.2218 acc_train: 0.9926 loss_val: 1996.7442 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00082 loss_train: 33.7978 loss_rec: 33.7978 acc_train: 0.9926 loss_val: 2803.2279 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00083 loss_train: 46.4669 loss_rec: 46.4669 acc_train: 0.9926 loss_val: 3547.7944 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00084 loss_train: 52.3729 loss_rec: 52.3729 acc_train: 0.9926 loss_val: 3326.0224 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00085 loss_train: 86.7689 loss_rec: 86.7689 acc_train: 0.9926 loss_val: 2864.2731 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00086 loss_train: 47.0576 loss_rec: 47.0576 acc_train: 0.9926 loss_val: 3366.8858 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00087 loss_train: 34.3525 loss_rec: 34.3525 acc_train: 0.9926 loss_val: 3176.6532 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00088 loss_train: 56.8758 loss_rec: 56.8758 acc_train: 0.9926 loss_val: 3394.1561 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00089 loss_train: 69.9663 loss_rec: 69.9663 acc_train: 0.9926 loss_val: 2956.9517 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00090 loss_train: 73.9178 loss_rec: 73.9178 acc_train: 0.9926 loss_val: 2843.5065 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00091 loss_train: 66.1400 loss_rec: 66.1400 acc_train: 0.9926 loss_val: 3372.0464 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00092 loss_train: 40.9093 loss_rec: 40.9093 acc_train: 0.9926 loss_val: 3175.6825 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00093 loss_train: 39.1459 loss_rec: 39.1459 acc_train: 0.9926 loss_val: 2079.4183 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00094 loss_train: 6.0611 loss_rec: 6.0611 acc_train: 0.9926 loss_val: 2074.9584 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00095 loss_train: 28.3714 loss_rec: 28.3714 acc_train: 0.9559 loss_val: 1256.1306 acc_val: 0.6774 time: 0.0020s\n",
      "Epoch: 00096 loss_train: 67.9837 loss_rec: 67.9837 acc_train: 0.9559 loss_val: 2001.0517 acc_val: 0.7097 time: 0.0020s\n",
      "Epoch: 00097 loss_train: 33.7982 loss_rec: 33.7982 acc_train: 0.9926 loss_val: 1815.2754 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00098 loss_train: 65.8269 loss_rec: 65.8269 acc_train: 0.9926 loss_val: 2694.6080 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00099 loss_train: 43.4570 loss_rec: 43.4570 acc_train: 0.9926 loss_val: 1885.1778 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00100 loss_train: 9.6843 loss_rec: 9.6843 acc_train: 0.9926 loss_val: 2142.1726 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00101 loss_train: 17.3404 loss_rec: 17.3404 acc_train: 0.9926 loss_val: 1104.1867 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00102 loss_train: 42.4499 loss_rec: 42.4499 acc_train: 0.9926 loss_val: 1864.1908 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00103 loss_train: 4.1408 loss_rec: 4.1408 acc_train: 0.9853 loss_val: 1599.9193 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00104 loss_train: 33.7816 loss_rec: 33.7816 acc_train: 0.9779 loss_val: 1341.3096 acc_val: 0.7097 time: 0.0034s\n",
      "Epoch: 00105 loss_train: 53.2723 loss_rec: 53.2723 acc_train: 0.9926 loss_val: 1707.5469 acc_val: 0.7097 time: 0.0020s\n",
      "Epoch: 00106 loss_train: 70.1746 loss_rec: 70.1746 acc_train: 0.9559 loss_val: 1303.3533 acc_val: 0.6935 time: 0.0030s\n",
      "Epoch: 00107 loss_train: 35.2012 loss_rec: 35.2012 acc_train: 0.9926 loss_val: 1615.3867 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00108 loss_train: 45.6464 loss_rec: 45.6464 acc_train: 0.9926 loss_val: 1280.8485 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00109 loss_train: 13.5710 loss_rec: 13.5710 acc_train: 0.9926 loss_val: 1176.3132 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00110 loss_train: 31.6982 loss_rec: 31.6982 acc_train: 0.9926 loss_val: 1460.5988 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00111 loss_train: 32.7293 loss_rec: 32.7293 acc_train: 0.9632 loss_val: 1144.2260 acc_val: 0.6774 time: 0.0032s\n",
      "Epoch: 00112 loss_train: 42.4889 loss_rec: 42.4889 acc_train: 0.9559 loss_val: 1112.3738 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00113 loss_train: 22.1125 loss_rec: 22.1125 acc_train: 0.9853 loss_val: 1531.4788 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00114 loss_train: 49.1523 loss_rec: 49.1523 acc_train: 0.9853 loss_val: 1310.5870 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00115 loss_train: 4.2855 loss_rec: 4.2855 acc_train: 0.9926 loss_val: 1417.6342 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00116 loss_train: 45.5556 loss_rec: 45.5556 acc_train: 0.9926 loss_val: 1408.1936 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00117 loss_train: 27.0987 loss_rec: 27.0987 acc_train: 0.9926 loss_val: 1494.3772 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00118 loss_train: 30.8287 loss_rec: 30.8287 acc_train: 0.9853 loss_val: 1181.6194 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00119 loss_train: 11.8706 loss_rec: 11.8706 acc_train: 0.9926 loss_val: 1850.2118 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00120 loss_train: 38.4327 loss_rec: 38.4327 acc_train: 0.9706 loss_val: 1472.3769 acc_val: 0.6774 time: 0.0040s\n",
      "Epoch: 00121 loss_train: 25.3045 loss_rec: 25.3045 acc_train: 0.9853 loss_val: 1294.7280 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00122 loss_train: 33.8953 loss_rec: 33.8953 acc_train: 0.9853 loss_val: 1766.9372 acc_val: 0.7097 time: 0.0035s\n",
      "Epoch: 00123 loss_train: 25.2716 loss_rec: 25.2716 acc_train: 0.9853 loss_val: 1211.9320 acc_val: 0.6935 time: 0.0032s\n",
      "Epoch: 00124 loss_train: 28.0125 loss_rec: 28.0125 acc_train: 0.9926 loss_val: 1312.4858 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00125 loss_train: 20.5481 loss_rec: 20.5481 acc_train: 0.9926 loss_val: 1332.8242 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00126 loss_train: 25.4330 loss_rec: 25.4330 acc_train: 0.9926 loss_val: 856.6605 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00127 loss_train: 21.5442 loss_rec: 21.5442 acc_train: 0.9926 loss_val: 1373.7207 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00128 loss_train: 32.0972 loss_rec: 32.0972 acc_train: 0.9926 loss_val: 913.6968 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00129 loss_train: 4.3086 loss_rec: 4.3086 acc_train: 0.9926 loss_val: 776.1726 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00130 loss_train: 17.4524 loss_rec: 17.4524 acc_train: 0.9926 loss_val: 899.5775 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00131 loss_train: 9.7393 loss_rec: 9.7393 acc_train: 0.9779 loss_val: 662.1905 acc_val: 0.7419 time: 0.0030s\n",
      "Epoch: 00132 loss_train: 9.5979 loss_rec: 9.5979 acc_train: 0.9926 loss_val: 690.2739 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00133 loss_train: 14.6125 loss_rec: 14.6125 acc_train: 0.9926 loss_val: 772.6325 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00134 loss_train: 7.1531 loss_rec: 7.1531 acc_train: 0.9926 loss_val: 769.1297 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00135 loss_train: 3.5505 loss_rec: 3.5505 acc_train: 0.9853 loss_val: 413.2449 acc_val: 0.6774 time: 0.0020s\n",
      "Epoch: 00136 loss_train: 10.6913 loss_rec: 10.6913 acc_train: 0.9926 loss_val: 504.2850 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00137 loss_train: 12.5611 loss_rec: 12.5611 acc_train: 0.9559 loss_val: 405.0321 acc_val: 0.6935 time: 0.0040s\n",
      "Epoch: 00138 loss_train: 0.0206 loss_rec: 0.0206 acc_train: 0.9926 loss_val: 395.8013 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00139 loss_train: 3.2758 loss_rec: 3.2758 acc_train: 0.9926 loss_val: 381.7916 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00140 loss_train: 5.6292 loss_rec: 5.6292 acc_train: 0.9926 loss_val: 384.3517 acc_val: 0.7258 time: 0.0101s\n",
      "Epoch: 00141 loss_train: 5.8809 loss_rec: 5.8809 acc_train: 0.9853 loss_val: 365.2366 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00142 loss_train: 6.8811 loss_rec: 6.8811 acc_train: 0.9926 loss_val: 297.0266 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00143 loss_train: 1.1774 loss_rec: 1.1774 acc_train: 0.9853 loss_val: 308.3012 acc_val: 0.7258 time: 0.0015s\n",
      "Epoch: 00144 loss_train: 12.3236 loss_rec: 12.3236 acc_train: 0.9779 loss_val: 460.4593 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00145 loss_train: 3.2459 loss_rec: 3.2459 acc_train: 0.9926 loss_val: 265.4586 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00146 loss_train: 11.3843 loss_rec: 11.3843 acc_train: 0.9926 loss_val: 304.1865 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00147 loss_train: 6.9879 loss_rec: 6.9879 acc_train: 0.9926 loss_val: 322.5154 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00148 loss_train: 10.3696 loss_rec: 10.3696 acc_train: 0.9191 loss_val: 184.7691 acc_val: 0.6935 time: 0.0020s\n",
      "Epoch: 00149 loss_train: 7.4580 loss_rec: 7.4580 acc_train: 0.9779 loss_val: 321.3078 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00150 loss_train: 11.1089 loss_rec: 11.1089 acc_train: 0.9926 loss_val: 424.3470 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00151 loss_train: 9.9655 loss_rec: 9.9655 acc_train: 0.9926 loss_val: 299.9058 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00152 loss_train: 14.8857 loss_rec: 14.8857 acc_train: 0.9632 loss_val: 397.2308 acc_val: 0.7419 time: 0.0032s\n",
      "Epoch: 00153 loss_train: 7.3703 loss_rec: 7.3703 acc_train: 0.9926 loss_val: 388.9339 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00154 loss_train: 18.2033 loss_rec: 18.2033 acc_train: 0.9926 loss_val: 658.4808 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00155 loss_train: 19.2290 loss_rec: 19.2290 acc_train: 0.9926 loss_val: 851.2247 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00156 loss_train: 13.9093 loss_rec: 13.9093 acc_train: 0.9926 loss_val: 757.1006 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00157 loss_train: 6.2719 loss_rec: 6.2719 acc_train: 0.9926 loss_val: 780.2031 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00158 loss_train: 7.8769 loss_rec: 7.8769 acc_train: 0.9632 loss_val: 736.1252 acc_val: 0.6935 time: 0.0030s\n",
      "Epoch: 00159 loss_train: 25.3733 loss_rec: 25.3733 acc_train: 0.9926 loss_val: 333.5458 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00160 loss_train: 13.5259 loss_rec: 13.5259 acc_train: 0.9926 loss_val: 532.4039 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00161 loss_train: 10.2697 loss_rec: 10.2697 acc_train: 0.9853 loss_val: 824.1442 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00162 loss_train: 22.5063 loss_rec: 22.5063 acc_train: 0.9779 loss_val: 346.2656 acc_val: 0.6935 time: 0.0033s\n",
      "Epoch: 00163 loss_train: 27.4949 loss_rec: 27.4949 acc_train: 0.9926 loss_val: 792.1035 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00164 loss_train: 16.1062 loss_rec: 16.1062 acc_train: 0.9926 loss_val: 829.2350 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00165 loss_train: 9.8693 loss_rec: 9.8693 acc_train: 0.9926 loss_val: 807.1796 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00166 loss_train: 8.8846 loss_rec: 8.8846 acc_train: 0.9926 loss_val: 484.6596 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00167 loss_train: 10.5696 loss_rec: 10.5696 acc_train: 0.9338 loss_val: 508.1155 acc_val: 0.6774 time: 0.0026s\n",
      "Epoch: 00168 loss_train: 13.8859 loss_rec: 13.8859 acc_train: 0.9926 loss_val: 793.4208 acc_val: 0.7097 time: 0.0033s\n",
      "Epoch: 00169 loss_train: 31.0733 loss_rec: 31.0733 acc_train: 0.9706 loss_val: 1201.1671 acc_val: 0.6935 time: 0.0020s\n",
      "Epoch: 00170 loss_train: 31.3603 loss_rec: 31.3603 acc_train: 0.9926 loss_val: 1422.4747 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00171 loss_train: 45.9469 loss_rec: 45.9469 acc_train: 0.9926 loss_val: 1546.2359 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00172 loss_train: 21.6935 loss_rec: 21.6935 acc_train: 0.9926 loss_val: 2067.1387 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00173 loss_train: 33.9454 loss_rec: 33.9454 acc_train: 0.9926 loss_val: 1131.0800 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00174 loss_train: 12.8285 loss_rec: 12.8285 acc_train: 0.9926 loss_val: 1391.2871 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00175 loss_train: 0.0131 loss_rec: 0.0131 acc_train: 0.9926 loss_val: 1774.5142 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00176 loss_train: 1.3509 loss_rec: 1.3509 acc_train: 0.9926 loss_val: 1684.6430 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00177 loss_train: 30.3326 loss_rec: 30.3326 acc_train: 0.9926 loss_val: 1945.6899 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00178 loss_train: 68.8041 loss_rec: 68.8041 acc_train: 0.9926 loss_val: 1533.7354 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00179 loss_train: 30.1980 loss_rec: 30.1980 acc_train: 0.9926 loss_val: 2909.6413 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00180 loss_train: 26.0579 loss_rec: 26.0579 acc_train: 0.9926 loss_val: 2452.5234 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00181 loss_train: 0.0138 loss_rec: 0.0138 acc_train: 0.9926 loss_val: 2712.2071 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00182 loss_train: 63.3061 loss_rec: 63.3061 acc_train: 0.9926 loss_val: 2073.9763 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00183 loss_train: 0.9716 loss_rec: 0.9716 acc_train: 0.9853 loss_val: 1207.2853 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00184 loss_train: 45.6834 loss_rec: 45.6834 acc_train: 0.9926 loss_val: 1836.3467 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00185 loss_train: 61.7251 loss_rec: 61.7251 acc_train: 0.9926 loss_val: 2114.2578 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00186 loss_train: 33.0718 loss_rec: 33.0718 acc_train: 0.9926 loss_val: 1991.9930 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00187 loss_train: 1.5087 loss_rec: 1.5087 acc_train: 0.9926 loss_val: 1447.6839 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00188 loss_train: 25.7501 loss_rec: 25.7501 acc_train: 0.9926 loss_val: 1069.7071 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00189 loss_train: 23.5127 loss_rec: 23.5127 acc_train: 0.9926 loss_val: 1518.6269 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00190 loss_train: 18.0467 loss_rec: 18.0467 acc_train: 0.9926 loss_val: 1226.8433 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00191 loss_train: 24.3037 loss_rec: 24.3037 acc_train: 0.9853 loss_val: 1593.3038 acc_val: 0.6935 time: 0.0021s\n",
      "Epoch: 00192 loss_train: 36.0689 loss_rec: 36.0689 acc_train: 0.9926 loss_val: 837.0808 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00193 loss_train: 0.2021 loss_rec: 0.2021 acc_train: 0.9779 loss_val: 1251.9036 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00194 loss_train: 15.5360 loss_rec: 15.5360 acc_train: 0.9926 loss_val: 859.0641 acc_val: 0.7097 time: 0.0031s\n",
      "Epoch: 00195 loss_train: 0.1881 loss_rec: 0.1881 acc_train: 0.9559 loss_val: 572.4444 acc_val: 0.7097 time: 0.0020s\n",
      "Epoch: 00196 loss_train: 3.0111 loss_rec: 3.0111 acc_train: 0.9926 loss_val: 594.1814 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00197 loss_train: 2.7207 loss_rec: 2.7207 acc_train: 0.9632 loss_val: 603.9982 acc_val: 0.7097 time: 0.0020s\n",
      "Epoch: 00198 loss_train: 2.4163 loss_rec: 2.4163 acc_train: 0.9926 loss_val: 621.7462 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00199 loss_train: 7.2383 loss_rec: 7.2383 acc_train: 0.9926 loss_val: 456.7084 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00200 loss_train: 14.9646 loss_rec: 14.9646 acc_train: 0.9926 loss_val: 755.2020 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00201 loss_train: 6.1636 loss_rec: 6.1636 acc_train: 0.9559 loss_val: 480.4519 acc_val: 0.6935 time: 0.0040s\n",
      "Epoch: 00202 loss_train: 0.1405 loss_rec: 0.1405 acc_train: 0.9926 loss_val: 357.1651 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00203 loss_train: 17.4388 loss_rec: 17.4388 acc_train: 0.9926 loss_val: 523.7539 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00204 loss_train: 17.6342 loss_rec: 17.6342 acc_train: 0.9779 loss_val: 554.0527 acc_val: 0.7097 time: 0.0011s\n",
      "Epoch: 00205 loss_train: 19.4634 loss_rec: 19.4634 acc_train: 0.9926 loss_val: 436.9274 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00206 loss_train: 0.2901 loss_rec: 0.2901 acc_train: 0.9853 loss_val: 526.8690 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00207 loss_train: 16.8768 loss_rec: 16.8768 acc_train: 0.9485 loss_val: 530.9410 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00208 loss_train: 19.7881 loss_rec: 19.7881 acc_train: 0.9926 loss_val: 373.6153 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00209 loss_train: 22.3098 loss_rec: 22.3098 acc_train: 0.9926 loss_val: 678.7768 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00210 loss_train: 18.8057 loss_rec: 18.8057 acc_train: 0.9926 loss_val: 703.7672 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00211 loss_train: 15.9930 loss_rec: 15.9930 acc_train: 0.9926 loss_val: 551.1749 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00212 loss_train: 14.7026 loss_rec: 14.7026 acc_train: 0.9926 loss_val: 391.4740 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00213 loss_train: 14.9077 loss_rec: 14.9077 acc_train: 0.9926 loss_val: 518.6603 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00214 loss_train: 9.3454 loss_rec: 9.3454 acc_train: 0.9926 loss_val: 475.4761 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00215 loss_train: 17.8853 loss_rec: 17.8853 acc_train: 0.9926 loss_val: 316.6093 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00216 loss_train: 1.9985 loss_rec: 1.9985 acc_train: 0.9926 loss_val: 595.1349 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00217 loss_train: 7.4986 loss_rec: 7.4986 acc_train: 0.9926 loss_val: 429.8751 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00218 loss_train: 2.3118 loss_rec: 2.3118 acc_train: 0.9926 loss_val: 534.0051 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00219 loss_train: 0.5823 loss_rec: 0.5823 acc_train: 0.9926 loss_val: 325.3192 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00220 loss_train: 12.3410 loss_rec: 12.3410 acc_train: 0.9779 loss_val: 478.2155 acc_val: 0.7419 time: 0.0020s\n",
      "Epoch: 00221 loss_train: 9.7343 loss_rec: 9.7343 acc_train: 0.9632 loss_val: 426.2282 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00222 loss_train: 11.9897 loss_rec: 11.9897 acc_train: 0.9926 loss_val: 225.2532 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00223 loss_train: 5.0881 loss_rec: 5.0881 acc_train: 0.9926 loss_val: 195.6810 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00224 loss_train: 0.8697 loss_rec: 0.8697 acc_train: 0.9926 loss_val: 211.9322 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00225 loss_train: 1.1069 loss_rec: 1.1069 acc_train: 0.9926 loss_val: 250.8178 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00226 loss_train: 2.7395 loss_rec: 2.7395 acc_train: 0.9926 loss_val: 185.0867 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00227 loss_train: 0.9247 loss_rec: 0.9247 acc_train: 0.9926 loss_val: 140.0539 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00228 loss_train: 2.8508 loss_rec: 2.8508 acc_train: 0.9853 loss_val: 112.8773 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00229 loss_train: 1.6716 loss_rec: 1.6716 acc_train: 0.9559 loss_val: 146.0271 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00230 loss_train: 1.3014 loss_rec: 1.3014 acc_train: 0.9926 loss_val: 61.2399 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00231 loss_train: 2.3867 loss_rec: 2.3867 acc_train: 0.9485 loss_val: 55.7123 acc_val: 0.7742 time: 0.0022s\n",
      "Epoch: 00232 loss_train: 3.5200 loss_rec: 3.5200 acc_train: 0.9926 loss_val: 148.2624 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00233 loss_train: 2.7931 loss_rec: 2.7931 acc_train: 0.9926 loss_val: 90.7406 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00234 loss_train: 4.8631 loss_rec: 4.8631 acc_train: 0.9926 loss_val: 127.4507 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00235 loss_train: 0.5291 loss_rec: 0.5291 acc_train: 0.9926 loss_val: 191.2309 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00236 loss_train: 1.7532 loss_rec: 1.7532 acc_train: 0.9926 loss_val: 168.9260 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00237 loss_train: 6.1655 loss_rec: 6.1655 acc_train: 0.9926 loss_val: 155.3688 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00238 loss_train: 0.0434 loss_rec: 0.0434 acc_train: 0.9926 loss_val: 131.1240 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00239 loss_train: 6.3660 loss_rec: 6.3660 acc_train: 0.9853 loss_val: 226.8881 acc_val: 0.7097 time: 0.0043s\n",
      "Epoch: 00240 loss_train: 5.4274 loss_rec: 5.4274 acc_train: 0.9926 loss_val: 181.0949 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00241 loss_train: 1.5139 loss_rec: 1.5139 acc_train: 0.9779 loss_val: 122.1635 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00242 loss_train: 7.1456 loss_rec: 7.1456 acc_train: 0.9412 loss_val: 128.5041 acc_val: 0.6935 time: 0.0020s\n",
      "Epoch: 00243 loss_train: 0.4803 loss_rec: 0.4803 acc_train: 0.9926 loss_val: 162.3581 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00244 loss_train: 0.4526 loss_rec: 0.4526 acc_train: 0.9926 loss_val: 137.9523 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00245 loss_train: 0.8808 loss_rec: 0.8808 acc_train: 0.9926 loss_val: 155.4770 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00246 loss_train: 4.4189 loss_rec: 4.4189 acc_train: 0.9926 loss_val: 150.8936 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00247 loss_train: 4.0722 loss_rec: 4.0722 acc_train: 0.9926 loss_val: 55.7453 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00248 loss_train: 4.0825 loss_rec: 4.0825 acc_train: 0.9926 loss_val: 109.2209 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00249 loss_train: 3.5951 loss_rec: 3.5951 acc_train: 0.9926 loss_val: 74.2247 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00250 loss_train: 4.0084 loss_rec: 4.0084 acc_train: 0.9926 loss_val: 87.9522 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00251 loss_train: 0.2791 loss_rec: 0.2791 acc_train: 0.9926 loss_val: 78.4647 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00252 loss_train: 2.4126 loss_rec: 2.4126 acc_train: 0.9926 loss_val: 75.0792 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00253 loss_train: 2.4394 loss_rec: 2.4394 acc_train: 0.9926 loss_val: 65.1467 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00254 loss_train: 0.5383 loss_rec: 0.5383 acc_train: 0.9926 loss_val: 42.1862 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00255 loss_train: 0.6618 loss_rec: 0.6618 acc_train: 0.9926 loss_val: 50.0497 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00256 loss_train: 0.9894 loss_rec: 0.9894 acc_train: 0.9926 loss_val: 53.4258 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00257 loss_train: 0.2176 loss_rec: 0.2176 acc_train: 0.9926 loss_val: 23.4294 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00258 loss_train: 1.1806 loss_rec: 1.1806 acc_train: 0.9926 loss_val: 30.1465 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00259 loss_train: 0.5921 loss_rec: 0.5921 acc_train: 0.9926 loss_val: 22.8883 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00260 loss_train: 0.8625 loss_rec: 0.8625 acc_train: 0.9926 loss_val: 22.5103 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00261 loss_train: 0.1990 loss_rec: 0.1990 acc_train: 0.9926 loss_val: 9.0676 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00262 loss_train: 0.8738 loss_rec: 0.8738 acc_train: 0.9118 loss_val: 29.8300 acc_val: 0.6452 time: 0.0020s\n",
      "Epoch: 00263 loss_train: 0.5093 loss_rec: 0.5093 acc_train: 0.9926 loss_val: 21.6833 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00264 loss_train: 0.7548 loss_rec: 0.7548 acc_train: 0.9926 loss_val: 53.1930 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00265 loss_train: 0.4283 loss_rec: 0.4283 acc_train: 0.9926 loss_val: 39.3850 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00266 loss_train: 0.2694 loss_rec: 0.2694 acc_train: 0.9926 loss_val: 27.1936 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00267 loss_train: 1.0805 loss_rec: 1.0805 acc_train: 0.9926 loss_val: 24.8405 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00268 loss_train: 0.1815 loss_rec: 0.1815 acc_train: 0.9926 loss_val: 30.9230 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00269 loss_train: 1.1997 loss_rec: 1.1997 acc_train: 0.9926 loss_val: 56.0000 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00270 loss_train: 0.1210 loss_rec: 0.1210 acc_train: 0.9926 loss_val: 30.2589 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00271 loss_train: 0.1080 loss_rec: 0.1080 acc_train: 0.9926 loss_val: 34.0105 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00272 loss_train: 0.8674 loss_rec: 0.8674 acc_train: 0.9926 loss_val: 38.9921 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00273 loss_train: 1.1013 loss_rec: 1.1013 acc_train: 0.9926 loss_val: 25.1542 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00274 loss_train: 0.2964 loss_rec: 0.2964 acc_train: 0.8971 loss_val: 35.9834 acc_val: 0.6774 time: 0.0030s\n",
      "Epoch: 00275 loss_train: 0.0634 loss_rec: 0.0634 acc_train: 0.9926 loss_val: 54.6255 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00276 loss_train: 0.5651 loss_rec: 0.5651 acc_train: 0.9926 loss_val: 12.1241 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00277 loss_train: 1.5582 loss_rec: 1.5582 acc_train: 0.9926 loss_val: 58.0074 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00278 loss_train: 0.6417 loss_rec: 0.6417 acc_train: 0.9926 loss_val: 38.1993 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00279 loss_train: 0.0493 loss_rec: 0.0493 acc_train: 0.9926 loss_val: 39.7888 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00280 loss_train: 0.0604 loss_rec: 0.0604 acc_train: 0.9926 loss_val: 35.1917 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00281 loss_train: 0.9928 loss_rec: 0.9928 acc_train: 0.9926 loss_val: 12.3248 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00282 loss_train: 2.3774 loss_rec: 2.3774 acc_train: 0.9926 loss_val: 22.2641 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00283 loss_train: 0.7041 loss_rec: 0.7041 acc_train: 0.9926 loss_val: 6.9648 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00284 loss_train: 0.1407 loss_rec: 0.1407 acc_train: 0.9926 loss_val: 15.1246 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00285 loss_train: 0.7776 loss_rec: 0.7776 acc_train: 0.9926 loss_val: 12.3776 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00286 loss_train: 0.0938 loss_rec: 0.0938 acc_train: 0.9926 loss_val: 8.3883 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00287 loss_train: 0.1858 loss_rec: 0.1858 acc_train: 0.9926 loss_val: 2.9521 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00288 loss_train: 0.1026 loss_rec: 0.1026 acc_train: 0.9926 loss_val: 3.5446 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00289 loss_train: 0.6915 loss_rec: 0.6915 acc_train: 0.9926 loss_val: 9.6095 acc_val: 0.7258 time: 0.0056s\n",
      "Epoch: 00290 loss_train: 0.1747 loss_rec: 0.1747 acc_train: 0.9926 loss_val: 1.5083 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00291 loss_train: 0.1263 loss_rec: 0.1263 acc_train: 0.9926 loss_val: 5.0704 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00292 loss_train: 0.1654 loss_rec: 0.1654 acc_train: 0.9926 loss_val: 2.8836 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00293 loss_train: 0.1396 loss_rec: 0.1396 acc_train: 0.9926 loss_val: 3.0305 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00294 loss_train: 0.2680 loss_rec: 0.2680 acc_train: 0.9926 loss_val: 1.3837 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00295 loss_train: 0.1649 loss_rec: 0.1649 acc_train: 0.9926 loss_val: 5.0430 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00296 loss_train: 0.1623 loss_rec: 0.1623 acc_train: 0.9926 loss_val: 0.8955 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00297 loss_train: 0.1637 loss_rec: 0.1637 acc_train: 0.9926 loss_val: 3.4398 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00298 loss_train: 0.1549 loss_rec: 0.1549 acc_train: 0.9926 loss_val: 0.8695 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00299 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.9926 loss_val: 0.8931 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00300 loss_train: 0.1546 loss_rec: 0.1546 acc_train: 0.9926 loss_val: 0.8536 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00301 loss_train: 0.2105 loss_rec: 0.2105 acc_train: 0.9926 loss_val: 0.9311 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00302 loss_train: 0.2990 loss_rec: 0.2990 acc_train: 0.9926 loss_val: 3.4692 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00303 loss_train: 0.1328 loss_rec: 0.1328 acc_train: 0.9926 loss_val: 3.6750 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00304 loss_train: 0.1388 loss_rec: 0.1388 acc_train: 0.9926 loss_val: 0.8995 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00305 loss_train: 0.1286 loss_rec: 0.1286 acc_train: 0.9926 loss_val: 0.9334 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00306 loss_train: 0.3386 loss_rec: 0.3386 acc_train: 0.9926 loss_val: 2.9919 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00307 loss_train: 0.1187 loss_rec: 0.1187 acc_train: 0.9926 loss_val: 1.1241 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00308 loss_train: 0.2193 loss_rec: 0.2193 acc_train: 0.9926 loss_val: 1.3308 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00309 loss_train: 0.1201 loss_rec: 0.1201 acc_train: 0.9926 loss_val: 0.9817 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00310 loss_train: 0.1156 loss_rec: 0.1156 acc_train: 0.9926 loss_val: 0.9092 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00311 loss_train: 0.1108 loss_rec: 0.1108 acc_train: 0.9926 loss_val: 1.0485 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00312 loss_train: 0.1115 loss_rec: 0.1115 acc_train: 0.9926 loss_val: 1.0290 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00313 loss_train: 0.1077 loss_rec: 0.1077 acc_train: 0.9926 loss_val: 0.9658 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00314 loss_train: 0.5591 loss_rec: 0.5591 acc_train: 0.9926 loss_val: 4.9098 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00315 loss_train: 0.0970 loss_rec: 0.0970 acc_train: 0.9926 loss_val: 1.0548 acc_val: 0.7258 time: 0.0064s\n",
      "Epoch: 00316 loss_train: 0.0991 loss_rec: 0.0991 acc_train: 0.9926 loss_val: 0.9872 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00317 loss_train: 0.0950 loss_rec: 0.0950 acc_train: 0.9926 loss_val: 1.0480 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00318 loss_train: 0.0977 loss_rec: 0.0977 acc_train: 0.9926 loss_val: 0.9302 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00319 loss_train: 0.0949 loss_rec: 0.0949 acc_train: 0.9926 loss_val: 0.9693 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00320 loss_train: 0.0917 loss_rec: 0.0917 acc_train: 0.9926 loss_val: 0.9509 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00321 loss_train: 0.0916 loss_rec: 0.0916 acc_train: 0.9926 loss_val: 1.0547 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00322 loss_train: 0.0927 loss_rec: 0.0927 acc_train: 0.9926 loss_val: 0.9675 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00323 loss_train: 0.0894 loss_rec: 0.0894 acc_train: 0.9926 loss_val: 0.9707 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00324 loss_train: 0.0875 loss_rec: 0.0875 acc_train: 0.9926 loss_val: 1.0698 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00325 loss_train: 0.0819 loss_rec: 0.0819 acc_train: 0.9926 loss_val: 1.1757 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00326 loss_train: 0.0851 loss_rec: 0.0851 acc_train: 0.9926 loss_val: 0.9919 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00327 loss_train: 0.0860 loss_rec: 0.0860 acc_train: 0.9926 loss_val: 1.0005 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00328 loss_train: 0.0843 loss_rec: 0.0843 acc_train: 0.9926 loss_val: 0.9933 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00329 loss_train: 0.0824 loss_rec: 0.0824 acc_train: 0.9926 loss_val: 0.9892 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00330 loss_train: 0.2136 loss_rec: 0.2136 acc_train: 0.9926 loss_val: 1.7813 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00331 loss_train: 0.0787 loss_rec: 0.0787 acc_train: 0.9926 loss_val: 1.1152 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00332 loss_train: 0.1461 loss_rec: 0.1461 acc_train: 0.9926 loss_val: 1.0264 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00333 loss_train: 0.0589 loss_rec: 0.0589 acc_train: 0.9926 loss_val: 1.5035 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00334 loss_train: 0.0793 loss_rec: 0.0793 acc_train: 0.9926 loss_val: 1.0626 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00335 loss_train: 0.6287 loss_rec: 0.6287 acc_train: 0.9926 loss_val: 2.1311 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00336 loss_train: 0.0769 loss_rec: 0.0769 acc_train: 0.9926 loss_val: 1.0564 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00337 loss_train: 0.0739 loss_rec: 0.0739 acc_train: 0.9926 loss_val: 1.2202 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00338 loss_train: 0.0717 loss_rec: 0.0717 acc_train: 0.9926 loss_val: 1.4245 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00339 loss_train: 0.0736 loss_rec: 0.0736 acc_train: 0.9926 loss_val: 1.1592 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00340 loss_train: 0.0732 loss_rec: 0.0732 acc_train: 0.9926 loss_val: 1.0703 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00341 loss_train: 0.0729 loss_rec: 0.0729 acc_train: 0.9926 loss_val: 1.0538 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00342 loss_train: 0.0709 loss_rec: 0.0709 acc_train: 0.9926 loss_val: 1.2853 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00343 loss_train: 0.0699 loss_rec: 0.0699 acc_train: 0.9926 loss_val: 1.1276 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00344 loss_train: 0.0677 loss_rec: 0.0677 acc_train: 0.9926 loss_val: 1.3264 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00345 loss_train: 0.0690 loss_rec: 0.0690 acc_train: 0.9926 loss_val: 1.0704 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00346 loss_train: 0.0698 loss_rec: 0.0698 acc_train: 0.9926 loss_val: 1.0679 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00347 loss_train: 0.0695 loss_rec: 0.0695 acc_train: 0.9926 loss_val: 1.0582 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00348 loss_train: 0.0695 loss_rec: 0.0695 acc_train: 0.9926 loss_val: 1.1047 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00349 loss_train: 0.0660 loss_rec: 0.0660 acc_train: 0.9926 loss_val: 1.1366 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00350 loss_train: 0.0659 loss_rec: 0.0659 acc_train: 0.9926 loss_val: 1.1528 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00351 loss_train: 0.0655 loss_rec: 0.0655 acc_train: 0.9926 loss_val: 1.1352 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00352 loss_train: 0.0523 loss_rec: 0.0523 acc_train: 0.9926 loss_val: 1.5368 acc_val: 0.7258 time: 0.0072s\n",
      "Epoch: 00353 loss_train: 0.0660 loss_rec: 0.0660 acc_train: 0.9926 loss_val: 1.1118 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00354 loss_train: 0.0637 loss_rec: 0.0637 acc_train: 0.9926 loss_val: 1.1113 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00355 loss_train: 0.0657 loss_rec: 0.0657 acc_train: 0.9926 loss_val: 1.1630 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00356 loss_train: 0.0662 loss_rec: 0.0662 acc_train: 0.9926 loss_val: 1.1195 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00357 loss_train: 0.0648 loss_rec: 0.0648 acc_train: 0.9926 loss_val: 1.1122 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00358 loss_train: 0.0645 loss_rec: 0.0645 acc_train: 0.9926 loss_val: 1.3238 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00359 loss_train: 0.0649 loss_rec: 0.0649 acc_train: 0.9926 loss_val: 1.2194 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00360 loss_train: 0.0631 loss_rec: 0.0631 acc_train: 0.9926 loss_val: 1.0939 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00361 loss_train: 0.0623 loss_rec: 0.0623 acc_train: 0.9926 loss_val: 1.1158 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00362 loss_train: 0.0609 loss_rec: 0.0609 acc_train: 0.9926 loss_val: 1.2647 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00363 loss_train: 0.0592 loss_rec: 0.0592 acc_train: 0.9926 loss_val: 1.4305 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00364 loss_train: 0.0620 loss_rec: 0.0620 acc_train: 0.9926 loss_val: 1.1253 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00365 loss_train: 0.0637 loss_rec: 0.0637 acc_train: 0.9926 loss_val: 1.1427 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00366 loss_train: 0.0629 loss_rec: 0.0629 acc_train: 0.9926 loss_val: 1.1569 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00367 loss_train: 0.0627 loss_rec: 0.0627 acc_train: 0.9926 loss_val: 1.1659 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00368 loss_train: 0.0619 loss_rec: 0.0619 acc_train: 0.9926 loss_val: 1.1770 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00369 loss_train: 0.0607 loss_rec: 0.0607 acc_train: 0.9926 loss_val: 1.1852 acc_val: 0.7258 time: 0.0015s\n",
      "Epoch: 00370 loss_train: 0.0622 loss_rec: 0.0622 acc_train: 0.9926 loss_val: 1.1054 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00371 loss_train: 0.0618 loss_rec: 0.0618 acc_train: 0.9926 loss_val: 1.1785 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00372 loss_train: 0.0607 loss_rec: 0.0607 acc_train: 0.9926 loss_val: 1.1376 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00373 loss_train: 0.0610 loss_rec: 0.0610 acc_train: 0.9926 loss_val: 1.1547 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00374 loss_train: 0.0593 loss_rec: 0.0593 acc_train: 0.9926 loss_val: 1.2121 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00375 loss_train: 0.0600 loss_rec: 0.0600 acc_train: 0.9926 loss_val: 1.1410 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00376 loss_train: 0.0600 loss_rec: 0.0600 acc_train: 0.9926 loss_val: 1.1217 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00377 loss_train: 0.0576 loss_rec: 0.0576 acc_train: 0.9926 loss_val: 1.5413 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00378 loss_train: 0.0598 loss_rec: 0.0598 acc_train: 0.9926 loss_val: 1.1736 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00379 loss_train: 0.0598 loss_rec: 0.0598 acc_train: 0.9926 loss_val: 1.1585 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00380 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.3407 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00381 loss_train: 0.0570 loss_rec: 0.0570 acc_train: 0.9926 loss_val: 1.1906 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00382 loss_train: 0.0584 loss_rec: 0.0584 acc_train: 0.9926 loss_val: 1.1429 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00383 loss_train: 0.0582 loss_rec: 0.0582 acc_train: 0.9926 loss_val: 1.2169 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00384 loss_train: 0.0595 loss_rec: 0.0595 acc_train: 0.9926 loss_val: 1.1876 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00385 loss_train: 0.0577 loss_rec: 0.0577 acc_train: 0.9926 loss_val: 1.2245 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00386 loss_train: 0.0561 loss_rec: 0.0561 acc_train: 0.9926 loss_val: 1.3299 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00387 loss_train: 0.1163 loss_rec: 0.1163 acc_train: 0.9926 loss_val: 1.1962 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00388 loss_train: 0.0560 loss_rec: 0.0560 acc_train: 0.9926 loss_val: 1.3985 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00389 loss_train: 0.0571 loss_rec: 0.0571 acc_train: 0.9926 loss_val: 1.1811 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00390 loss_train: 0.0571 loss_rec: 0.0571 acc_train: 0.9926 loss_val: 1.2699 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00391 loss_train: 0.0571 loss_rec: 0.0571 acc_train: 0.9926 loss_val: 1.1639 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00392 loss_train: 0.0569 loss_rec: 0.0569 acc_train: 0.9926 loss_val: 1.3213 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00393 loss_train: 0.0561 loss_rec: 0.0561 acc_train: 0.9926 loss_val: 1.4347 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00394 loss_train: 0.0572 loss_rec: 0.0572 acc_train: 0.9926 loss_val: 1.1542 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00395 loss_train: 0.0559 loss_rec: 0.0559 acc_train: 0.9926 loss_val: 2.0603 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00396 loss_train: 0.0561 loss_rec: 0.0561 acc_train: 0.9926 loss_val: 1.1169 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00397 loss_train: 0.0555 loss_rec: 0.0555 acc_train: 0.9926 loss_val: 1.1848 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00398 loss_train: 0.0568 loss_rec: 0.0568 acc_train: 0.9926 loss_val: 1.1826 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00399 loss_train: 0.0557 loss_rec: 0.0557 acc_train: 0.9926 loss_val: 1.2902 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00400 loss_train: 0.0559 loss_rec: 0.0559 acc_train: 0.9926 loss_val: 1.1895 acc_val: 0.7258 time: 0.0023s\n",
      "Epoch: 00401 loss_train: 0.0547 loss_rec: 0.0547 acc_train: 0.9926 loss_val: 1.4217 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00402 loss_train: 0.0553 loss_rec: 0.0553 acc_train: 0.9926 loss_val: 1.1524 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00403 loss_train: 0.0557 loss_rec: 0.0557 acc_train: 0.9926 loss_val: 1.2351 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00404 loss_train: 0.0578 loss_rec: 0.0578 acc_train: 0.9926 loss_val: 1.1635 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00405 loss_train: 0.0556 loss_rec: 0.0556 acc_train: 0.9926 loss_val: 1.1943 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00406 loss_train: 0.0543 loss_rec: 0.0543 acc_train: 0.9926 loss_val: 1.3264 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00407 loss_train: 0.0554 loss_rec: 0.0554 acc_train: 0.9926 loss_val: 1.2191 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00408 loss_train: 0.0555 loss_rec: 0.0555 acc_train: 0.9926 loss_val: 1.2467 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00409 loss_train: 0.0542 loss_rec: 0.0542 acc_train: 0.9926 loss_val: 1.2595 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00410 loss_train: 0.0552 loss_rec: 0.0552 acc_train: 0.9926 loss_val: 1.2046 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00411 loss_train: 0.0547 loss_rec: 0.0547 acc_train: 0.9926 loss_val: 1.2959 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00412 loss_train: 0.0534 loss_rec: 0.0534 acc_train: 0.9926 loss_val: 1.1855 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00413 loss_train: 0.0540 loss_rec: 0.0540 acc_train: 0.9926 loss_val: 1.1966 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00414 loss_train: 0.0536 loss_rec: 0.0536 acc_train: 0.9926 loss_val: 1.3932 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00415 loss_train: 0.0539 loss_rec: 0.0539 acc_train: 0.9926 loss_val: 1.2446 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00416 loss_train: 0.0548 loss_rec: 0.0548 acc_train: 0.9926 loss_val: 1.1945 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00417 loss_train: 0.0531 loss_rec: 0.0531 acc_train: 0.9926 loss_val: 1.2297 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00418 loss_train: 0.0544 loss_rec: 0.0544 acc_train: 0.9926 loss_val: 1.2838 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00419 loss_train: 0.0538 loss_rec: 0.0538 acc_train: 0.9926 loss_val: 1.5790 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00420 loss_train: 0.0515 loss_rec: 0.0515 acc_train: 0.9926 loss_val: 1.2082 acc_val: 0.7258 time: 0.0015s\n",
      "Epoch: 00421 loss_train: 0.0542 loss_rec: 0.0542 acc_train: 0.9926 loss_val: 1.2197 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00422 loss_train: 0.0526 loss_rec: 0.0526 acc_train: 0.9926 loss_val: 1.2976 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00423 loss_train: 0.0513 loss_rec: 0.0513 acc_train: 0.9926 loss_val: 1.9789 acc_val: 0.7258 time: 0.0019s\n",
      "Epoch: 00424 loss_train: 0.0533 loss_rec: 0.0533 acc_train: 0.9926 loss_val: 1.2736 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00425 loss_train: 0.0534 loss_rec: 0.0534 acc_train: 0.9926 loss_val: 1.2600 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00426 loss_train: 0.0529 loss_rec: 0.0529 acc_train: 0.9926 loss_val: 1.3014 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00427 loss_train: 0.0540 loss_rec: 0.0540 acc_train: 0.9926 loss_val: 1.2067 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00428 loss_train: 0.0536 loss_rec: 0.0536 acc_train: 0.9926 loss_val: 1.2061 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00429 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.3472 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00430 loss_train: 0.0525 loss_rec: 0.0525 acc_train: 0.9926 loss_val: 1.2071 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00431 loss_train: 0.0527 loss_rec: 0.0527 acc_train: 0.9926 loss_val: 1.2476 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00432 loss_train: 0.0534 loss_rec: 0.0534 acc_train: 0.9926 loss_val: 1.2088 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00433 loss_train: 0.0523 loss_rec: 0.0523 acc_train: 0.9926 loss_val: 1.2270 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00434 loss_train: 0.0532 loss_rec: 0.0532 acc_train: 0.9926 loss_val: 1.2370 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00435 loss_train: 0.0524 loss_rec: 0.0524 acc_train: 0.9926 loss_val: 1.3140 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00436 loss_train: 0.0521 loss_rec: 0.0521 acc_train: 0.9926 loss_val: 1.2448 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00437 loss_train: 0.0521 loss_rec: 0.0521 acc_train: 0.9926 loss_val: 1.2822 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00438 loss_train: 0.0530 loss_rec: 0.0530 acc_train: 0.9926 loss_val: 1.2328 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00439 loss_train: 0.2046 loss_rec: 0.2046 acc_train: 0.9926 loss_val: 1.6444 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00440 loss_train: 0.0529 loss_rec: 0.0529 acc_train: 0.9926 loss_val: 1.2261 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00441 loss_train: 0.0516 loss_rec: 0.0516 acc_train: 0.9926 loss_val: 1.4048 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00442 loss_train: 0.0523 loss_rec: 0.0523 acc_train: 0.9926 loss_val: 1.3581 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00443 loss_train: 0.0519 loss_rec: 0.0519 acc_train: 0.9926 loss_val: 1.2585 acc_val: 0.7258 time: 0.0023s\n",
      "Epoch: 00444 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.2179 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00445 loss_train: 0.0513 loss_rec: 0.0513 acc_train: 0.9926 loss_val: 1.4669 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00446 loss_train: 0.0503 loss_rec: 0.0503 acc_train: 0.9926 loss_val: 1.2863 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00447 loss_train: 0.0498 loss_rec: 0.0498 acc_train: 0.9926 loss_val: 1.8128 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00448 loss_train: 0.0493 loss_rec: 0.0493 acc_train: 0.9926 loss_val: 1.7002 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00449 loss_train: 0.0518 loss_rec: 0.0518 acc_train: 0.9926 loss_val: 1.3038 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00450 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.2513 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00451 loss_train: 0.0513 loss_rec: 0.0513 acc_train: 0.9926 loss_val: 1.3170 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00452 loss_train: 0.0517 loss_rec: 0.0517 acc_train: 0.9926 loss_val: 1.2652 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00453 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.2686 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00454 loss_train: 0.0516 loss_rec: 0.0516 acc_train: 0.9926 loss_val: 1.2279 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00455 loss_train: 0.0517 loss_rec: 0.0517 acc_train: 0.9926 loss_val: 1.2546 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00456 loss_train: 0.0508 loss_rec: 0.0508 acc_train: 0.9926 loss_val: 1.2797 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00457 loss_train: 0.0508 loss_rec: 0.0508 acc_train: 0.9926 loss_val: 1.2560 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00458 loss_train: 0.0509 loss_rec: 0.0509 acc_train: 0.9926 loss_val: 1.2344 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00459 loss_train: 0.0508 loss_rec: 0.0508 acc_train: 0.9926 loss_val: 1.5077 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00460 loss_train: 0.0509 loss_rec: 0.0509 acc_train: 0.9926 loss_val: 1.2596 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00461 loss_train: 0.0507 loss_rec: 0.0507 acc_train: 0.9926 loss_val: 1.3083 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00462 loss_train: 0.0508 loss_rec: 0.0508 acc_train: 0.9926 loss_val: 1.4168 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00463 loss_train: 0.0550 loss_rec: 0.0550 acc_train: 0.9926 loss_val: 1.3131 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00464 loss_train: 0.0512 loss_rec: 0.0512 acc_train: 0.9926 loss_val: 1.2936 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00465 loss_train: 0.0510 loss_rec: 0.0510 acc_train: 0.9926 loss_val: 1.2330 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00466 loss_train: 0.0502 loss_rec: 0.0502 acc_train: 0.9926 loss_val: 1.2658 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00467 loss_train: 0.0509 loss_rec: 0.0509 acc_train: 0.9926 loss_val: 1.4019 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00468 loss_train: 0.0511 loss_rec: 0.0511 acc_train: 0.9926 loss_val: 1.2430 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00469 loss_train: 0.0510 loss_rec: 0.0510 acc_train: 0.9926 loss_val: 1.2787 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00470 loss_train: 0.0505 loss_rec: 0.0505 acc_train: 0.9926 loss_val: 1.3264 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00471 loss_train: 0.0512 loss_rec: 0.0512 acc_train: 0.9926 loss_val: 1.2537 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00472 loss_train: 0.0505 loss_rec: 0.0505 acc_train: 0.9926 loss_val: 1.3140 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00473 loss_train: 0.0508 loss_rec: 0.0508 acc_train: 0.9926 loss_val: 1.2426 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00474 loss_train: 0.0504 loss_rec: 0.0504 acc_train: 0.9926 loss_val: 1.3030 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00475 loss_train: 0.0500 loss_rec: 0.0500 acc_train: 0.9926 loss_val: 1.3771 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00476 loss_train: 0.0502 loss_rec: 0.0502 acc_train: 0.9926 loss_val: 1.3517 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00477 loss_train: 0.0504 loss_rec: 0.0504 acc_train: 0.9926 loss_val: 1.3767 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00478 loss_train: 0.0507 loss_rec: 0.0507 acc_train: 0.9926 loss_val: 1.2803 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00479 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.3022 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00480 loss_train: 0.0507 loss_rec: 0.0507 acc_train: 0.9926 loss_val: 1.3156 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00481 loss_train: 0.0503 loss_rec: 0.0503 acc_train: 0.9926 loss_val: 1.3024 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00482 loss_train: 0.0504 loss_rec: 0.0504 acc_train: 0.9926 loss_val: 1.2616 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00483 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.3701 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00484 loss_train: 0.0506 loss_rec: 0.0506 acc_train: 0.9926 loss_val: 1.2921 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00485 loss_train: 0.0500 loss_rec: 0.0500 acc_train: 0.9926 loss_val: 1.2919 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00486 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.2609 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00487 loss_train: 0.0501 loss_rec: 0.0501 acc_train: 0.9926 loss_val: 1.2871 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00488 loss_train: 0.0938 loss_rec: 0.0938 acc_train: 0.9926 loss_val: 1.3627 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00489 loss_train: 0.0499 loss_rec: 0.0499 acc_train: 0.9926 loss_val: 1.2836 acc_val: 0.7258 time: 0.0023s\n",
      "Epoch: 00490 loss_train: 0.0650 loss_rec: 0.0650 acc_train: 0.9926 loss_val: 1.2921 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00491 loss_train: 0.0498 loss_rec: 0.0498 acc_train: 0.9926 loss_val: 1.3294 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00492 loss_train: 0.0500 loss_rec: 0.0500 acc_train: 0.9926 loss_val: 1.2658 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00493 loss_train: 0.0498 loss_rec: 0.0498 acc_train: 0.9926 loss_val: 1.3940 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00494 loss_train: 0.4635 loss_rec: 0.4635 acc_train: 0.9926 loss_val: 2.8326 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00495 loss_train: 0.0500 loss_rec: 0.0500 acc_train: 0.9926 loss_val: 1.3040 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00496 loss_train: 0.0498 loss_rec: 0.0498 acc_train: 0.9926 loss_val: 1.2718 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00497 loss_train: 0.0491 loss_rec: 0.0491 acc_train: 0.9926 loss_val: 1.3749 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00498 loss_train: 0.0499 loss_rec: 0.0499 acc_train: 0.9926 loss_val: 1.4510 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00499 loss_train: 0.0500 loss_rec: 0.0500 acc_train: 0.9926 loss_val: 1.2805 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00500 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.3703 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00501 loss_train: 0.0499 loss_rec: 0.0499 acc_train: 0.9926 loss_val: 1.3246 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00502 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.3069 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00503 loss_train: 0.0482 loss_rec: 0.0482 acc_train: 0.9926 loss_val: 1.2908 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00504 loss_train: 0.0631 loss_rec: 0.0631 acc_train: 0.9926 loss_val: 1.2967 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00505 loss_train: 0.0492 loss_rec: 0.0492 acc_train: 0.9926 loss_val: 1.3782 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00506 loss_train: 0.0501 loss_rec: 0.0501 acc_train: 0.9926 loss_val: 1.2687 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00507 loss_train: 0.0491 loss_rec: 0.0491 acc_train: 0.9926 loss_val: 1.3098 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00508 loss_train: 0.0491 loss_rec: 0.0491 acc_train: 0.9926 loss_val: 1.3408 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00509 loss_train: 0.0496 loss_rec: 0.0496 acc_train: 0.9926 loss_val: 1.2784 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00510 loss_train: 0.0493 loss_rec: 0.0493 acc_train: 0.9926 loss_val: 1.3410 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00511 loss_train: 0.0491 loss_rec: 0.0491 acc_train: 0.9926 loss_val: 1.2883 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00512 loss_train: 0.0489 loss_rec: 0.0489 acc_train: 0.9926 loss_val: 1.3778 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00513 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.2829 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00514 loss_train: 0.0490 loss_rec: 0.0490 acc_train: 0.9926 loss_val: 2.0936 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00515 loss_train: 0.0494 loss_rec: 0.0494 acc_train: 0.9926 loss_val: 1.2771 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00516 loss_train: 0.0477 loss_rec: 0.0477 acc_train: 0.9926 loss_val: 1.5203 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00517 loss_train: 0.0518 loss_rec: 0.0518 acc_train: 0.9926 loss_val: 1.2857 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00518 loss_train: 0.0490 loss_rec: 0.0490 acc_train: 0.9926 loss_val: 1.2783 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00519 loss_train: 0.0489 loss_rec: 0.0489 acc_train: 0.9926 loss_val: 1.4208 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00520 loss_train: 0.0492 loss_rec: 0.0492 acc_train: 0.9926 loss_val: 1.3230 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00521 loss_train: 0.0492 loss_rec: 0.0492 acc_train: 0.9926 loss_val: 1.3054 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00522 loss_train: 0.0496 loss_rec: 0.0496 acc_train: 0.9926 loss_val: 1.3759 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00523 loss_train: 0.0486 loss_rec: 0.0486 acc_train: 0.9926 loss_val: 1.2888 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00524 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.2841 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00525 loss_train: 0.0484 loss_rec: 0.0484 acc_train: 0.9926 loss_val: 1.4775 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00526 loss_train: 0.0489 loss_rec: 0.0489 acc_train: 0.9926 loss_val: 1.2808 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00527 loss_train: 0.0385 loss_rec: 0.0385 acc_train: 0.9926 loss_val: 1.2791 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00528 loss_train: 0.0503 loss_rec: 0.0503 acc_train: 0.9926 loss_val: 1.2749 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00529 loss_train: 0.0492 loss_rec: 0.0492 acc_train: 0.9926 loss_val: 1.3074 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00530 loss_train: 0.0487 loss_rec: 0.0487 acc_train: 0.9926 loss_val: 1.4345 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00531 loss_train: 0.0489 loss_rec: 0.0489 acc_train: 0.9926 loss_val: 1.2803 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00532 loss_train: 0.1586 loss_rec: 0.1586 acc_train: 0.9779 loss_val: 1.7967 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00533 loss_train: 0.0488 loss_rec: 0.0488 acc_train: 0.9926 loss_val: 1.4091 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00534 loss_train: 0.0492 loss_rec: 0.0492 acc_train: 0.9926 loss_val: 1.3039 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00535 loss_train: 0.0488 loss_rec: 0.0488 acc_train: 0.9926 loss_val: 1.2922 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00536 loss_train: 0.0488 loss_rec: 0.0488 acc_train: 0.9926 loss_val: 1.3146 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00537 loss_train: 0.0481 loss_rec: 0.0481 acc_train: 0.9926 loss_val: 1.3195 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00538 loss_train: 0.0490 loss_rec: 0.0490 acc_train: 0.9926 loss_val: 1.3045 acc_val: 0.7258 time: 0.0024s\n",
      "Epoch: 00539 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.3239 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00540 loss_train: 0.0484 loss_rec: 0.0484 acc_train: 0.9926 loss_val: 1.3358 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00541 loss_train: 0.0484 loss_rec: 0.0484 acc_train: 0.9926 loss_val: 1.3295 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00542 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.3150 acc_val: 0.7258 time: 0.0023s\n",
      "Epoch: 00543 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3022 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00544 loss_train: 0.0520 loss_rec: 0.0520 acc_train: 0.9926 loss_val: 1.5903 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00545 loss_train: 0.1394 loss_rec: 0.1394 acc_train: 0.9926 loss_val: 1.3270 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00546 loss_train: 0.0484 loss_rec: 0.0484 acc_train: 0.9926 loss_val: 1.3120 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00547 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.4269 acc_val: 0.7258 time: 0.0060s\n",
      "Epoch: 00548 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.3186 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00549 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.7401 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00550 loss_train: 0.2651 loss_rec: 0.2651 acc_train: 0.9926 loss_val: 2.6169 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00551 loss_train: 0.0482 loss_rec: 0.0482 acc_train: 0.9926 loss_val: 1.3926 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00552 loss_train: 0.0482 loss_rec: 0.0482 acc_train: 0.9926 loss_val: 1.3292 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00553 loss_train: 0.0655 loss_rec: 0.0655 acc_train: 0.9926 loss_val: 1.3825 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00554 loss_train: 0.0484 loss_rec: 0.0484 acc_train: 0.9926 loss_val: 1.3168 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00555 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.4318 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00556 loss_train: 0.0483 loss_rec: 0.0483 acc_train: 0.9926 loss_val: 1.3797 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00557 loss_train: 0.0447 loss_rec: 0.0447 acc_train: 0.9926 loss_val: 1.8873 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00558 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.3089 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00559 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3520 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00560 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3251 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00561 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.6169 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00562 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3340 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00563 loss_train: 0.0486 loss_rec: 0.0486 acc_train: 0.9926 loss_val: 1.3253 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00564 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.9062 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00565 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3073 acc_val: 0.7258 time: 0.0014s\n",
      "Epoch: 00566 loss_train: 0.0481 loss_rec: 0.0481 acc_train: 0.9926 loss_val: 1.3146 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00567 loss_train: 0.0480 loss_rec: 0.0480 acc_train: 0.9926 loss_val: 1.6811 acc_val: 0.7258 time: 0.0029s\n",
      "Epoch: 00568 loss_train: 0.0481 loss_rec: 0.0481 acc_train: 0.9926 loss_val: 1.3257 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00569 loss_train: 0.0483 loss_rec: 0.0483 acc_train: 0.9926 loss_val: 1.3514 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00570 loss_train: 0.0483 loss_rec: 0.0483 acc_train: 0.9926 loss_val: 1.3253 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00571 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3131 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00572 loss_train: 0.0485 loss_rec: 0.0485 acc_train: 0.9926 loss_val: 1.3140 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00573 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3110 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00574 loss_train: 0.0483 loss_rec: 0.0483 acc_train: 0.9926 loss_val: 1.3186 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00575 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3937 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00576 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3255 acc_val: 0.7258 time: 0.0016s\n",
      "Epoch: 00577 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3040 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00578 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3656 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00579 loss_train: 0.0482 loss_rec: 0.0482 acc_train: 0.9926 loss_val: 1.3123 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00580 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.3229 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00581 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.5801 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00582 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3584 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00583 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3501 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00584 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.5021 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00585 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3167 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00586 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3063 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00587 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3812 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00588 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3415 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00589 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3449 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00590 loss_train: 0.0477 loss_rec: 0.0477 acc_train: 0.9926 loss_val: 1.3360 acc_val: 0.7258 time: 0.0022s\n",
      "Epoch: 00591 loss_train: 0.0482 loss_rec: 0.0482 acc_train: 0.9926 loss_val: 1.3093 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00592 loss_train: 0.1118 loss_rec: 0.1118 acc_train: 0.9926 loss_val: 1.7434 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00593 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3188 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00594 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3280 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00595 loss_train: 0.0478 loss_rec: 0.0478 acc_train: 0.9926 loss_val: 1.3185 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00596 loss_train: 0.0477 loss_rec: 0.0477 acc_train: 0.9926 loss_val: 1.3306 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00597 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3645 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00598 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3560 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00599 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3276 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00600 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.5107 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00601 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3280 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00602 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3765 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00603 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.3585 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00604 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3502 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00605 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3689 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00606 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3960 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00607 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3365 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00608 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3259 acc_val: 0.7258 time: 0.0015s\n",
      "Epoch: 00609 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3350 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00610 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3197 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00611 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3641 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00612 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3579 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00613 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3219 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00614 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3301 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00615 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3888 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00616 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3237 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00617 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3054 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00618 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3153 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00619 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.9884 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00620 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3411 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00621 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3198 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00622 loss_train: 0.3370 loss_rec: 0.3370 acc_train: 0.9926 loss_val: 1.9786 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00623 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3542 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00624 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3819 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00625 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3461 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00626 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.3321 acc_val: 0.7258 time: 0.0027s\n",
      "Epoch: 00627 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.3586 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00628 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.4058 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00629 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.4826 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00630 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3223 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00631 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3426 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00632 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3576 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00633 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.4230 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00634 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3655 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00635 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.4158 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00636 loss_train: 0.0474 loss_rec: 0.0474 acc_train: 0.9926 loss_val: 1.3511 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00637 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3656 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00638 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3392 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00639 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.4034 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00640 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3405 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00641 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.3265 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00642 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3913 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00643 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3331 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00644 loss_train: 0.0475 loss_rec: 0.0475 acc_train: 0.9926 loss_val: 1.4942 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00645 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.4099 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00646 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3919 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00647 loss_train: 0.0656 loss_rec: 0.0656 acc_train: 0.9926 loss_val: 1.4327 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00648 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3329 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00649 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3279 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00650 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3659 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00651 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3409 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00652 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.4547 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00653 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.4045 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00654 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3211 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00655 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3396 acc_val: 0.7258 time: 0.0021s\n",
      "Epoch: 00656 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3555 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00657 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3432 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00658 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3407 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00659 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3345 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00660 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.6428 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00661 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3228 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00662 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3713 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00663 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3327 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00664 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3330 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00665 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3327 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00666 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3629 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00667 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.4354 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00668 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3892 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00669 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3279 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00670 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.4021 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00671 loss_train: 0.0473 loss_rec: 0.0473 acc_train: 0.9926 loss_val: 1.3661 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00672 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.5094 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00673 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3232 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00674 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.4626 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00675 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3255 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00676 loss_train: 0.0471 loss_rec: 0.0471 acc_train: 0.9926 loss_val: 1.3255 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00677 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.6878 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00678 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3844 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00679 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3752 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00680 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3656 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00681 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.8089 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00682 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3352 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00683 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3390 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00684 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3573 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00685 loss_train: 0.0470 loss_rec: 0.0470 acc_train: 0.9926 loss_val: 1.3631 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00686 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3306 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00687 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.4584 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00688 loss_train: 0.0472 loss_rec: 0.0472 acc_train: 0.9926 loss_val: 1.3362 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00689 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3435 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00690 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3294 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00691 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3490 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00692 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.5605 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00693 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.4244 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00694 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3379 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00695 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3432 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00696 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3949 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00697 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3509 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00698 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3828 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00699 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3506 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00700 loss_train: 0.0476 loss_rec: 0.0476 acc_train: 0.9926 loss_val: 1.3269 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00701 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3441 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00702 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3431 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00703 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3454 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00704 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3409 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00705 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3315 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00706 loss_train: 0.2398 loss_rec: 0.2398 acc_train: 0.9926 loss_val: 1.6222 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00707 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.4243 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00708 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3293 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00709 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3312 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00710 loss_train: 0.0469 loss_rec: 0.0469 acc_train: 0.9926 loss_val: 1.3278 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00711 loss_train: 0.0331 loss_rec: 0.0331 acc_train: 0.9926 loss_val: 1.2658 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00712 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3998 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00713 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3737 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00714 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.5152 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00715 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3305 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00716 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.4041 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00717 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3429 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00718 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3785 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00719 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3818 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00720 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3486 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00721 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3309 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00722 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3356 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00723 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3370 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00724 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3497 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00725 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3430 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00726 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3609 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00727 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3404 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00728 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3680 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00729 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3432 acc_val: 0.7258 time: 0.0026s\n",
      "Epoch: 00730 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.6945 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00731 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3294 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00732 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3533 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00733 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3284 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00734 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3377 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00735 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.4177 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00736 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.4086 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00737 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3335 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00738 loss_train: 0.1622 loss_rec: 0.1622 acc_train: 0.9706 loss_val: 1.4568 acc_val: 0.7097 time: 0.0030s\n",
      "Epoch: 00739 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3944 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00740 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.4084 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00741 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.5088 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00742 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3541 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00743 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3728 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00744 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3650 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00745 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.4886 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00746 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3461 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00747 loss_train: 0.0619 loss_rec: 0.0619 acc_train: 0.9926 loss_val: 1.4825 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00748 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3435 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00749 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3570 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00750 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3555 acc_val: 0.7258 time: 0.0072s\n",
      "Epoch: 00751 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3423 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00752 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3590 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00753 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3610 acc_val: 0.7258 time: 0.0059s\n",
      "Epoch: 00754 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3521 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00755 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3579 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00756 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3654 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00757 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3604 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00758 loss_train: 0.0467 loss_rec: 0.0467 acc_train: 0.9926 loss_val: 1.3977 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00759 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.5137 acc_val: 0.7258 time: 0.0020s\n",
      "Epoch: 00760 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3656 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00761 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3634 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00762 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3583 acc_val: 0.7258 time: 0.0071s\n",
      "Epoch: 00763 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3429 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00764 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3913 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00765 loss_train: 0.0466 loss_rec: 0.0466 acc_train: 0.9926 loss_val: 1.3431 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00766 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3532 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00767 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.4299 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00768 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3549 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00769 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3474 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00770 loss_train: 0.1157 loss_rec: 0.1157 acc_train: 0.9926 loss_val: 1.3704 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00771 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.4768 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00772 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.6595 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00773 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.4220 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00774 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3717 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00775 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3438 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00776 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3820 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00777 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3486 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00778 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3453 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00779 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.7076 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00780 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3368 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00781 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3843 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00782 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3873 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00783 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.4355 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00784 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3738 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00785 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3807 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00786 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3475 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00787 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3723 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00788 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3479 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00789 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3581 acc_val: 0.7258 time: 0.0031s\n",
      "Epoch: 00790 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3437 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00791 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 2.0273 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00792 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3773 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00793 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3606 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00794 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3421 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00795 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3590 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00796 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3799 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00797 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3419 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00798 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3716 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00799 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3770 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00800 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.4934 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00801 loss_train: 0.0465 loss_rec: 0.0465 acc_train: 0.9926 loss_val: 1.3401 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00802 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3515 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00803 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3512 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00804 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3487 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00805 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3990 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00806 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.4847 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00807 loss_train: 0.0450 loss_rec: 0.0450 acc_train: 0.9926 loss_val: 1.5688 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00808 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.4060 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00809 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3778 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00810 loss_train: 0.0464 loss_rec: 0.0464 acc_train: 0.9926 loss_val: 1.3598 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00811 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3589 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00812 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3616 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00813 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3518 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00814 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3818 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00815 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4250 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00816 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.4432 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00817 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3401 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00818 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3415 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00819 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3855 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00820 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.6575 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00821 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3436 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00822 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 2.1460 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00823 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.4123 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00824 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3555 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00825 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.4901 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00826 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.6282 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00827 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.4084 acc_val: 0.7258 time: 0.0042s\n",
      "Epoch: 00828 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3709 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00829 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3649 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00830 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3707 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00831 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3466 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00832 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.5768 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00833 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3647 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00834 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.8219 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00835 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.4558 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00836 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3857 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00837 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3980 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00838 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3505 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00839 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4148 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00840 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3989 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00841 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3603 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00842 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3605 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00843 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3395 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00844 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3478 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00845 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3514 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00846 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3930 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00847 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3461 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00848 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3910 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00849 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.4341 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00850 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3548 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00851 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3391 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00852 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.4106 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00853 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3779 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00854 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3482 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00855 loss_train: 0.0462 loss_rec: 0.0462 acc_train: 0.9926 loss_val: 1.3428 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00856 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3688 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00857 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4505 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00858 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3719 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00859 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3597 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00860 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3726 acc_val: 0.7258 time: 0.0028s\n",
      "Epoch: 00861 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3412 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00862 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3706 acc_val: 0.7258 time: 0.0070s\n",
      "Epoch: 00863 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4096 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00864 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.4075 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00865 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3409 acc_val: 0.7258 time: 0.0064s\n",
      "Epoch: 00866 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3734 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00867 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3613 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00868 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3549 acc_val: 0.7258 time: 0.0057s\n",
      "Epoch: 00869 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3330 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00870 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3519 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00871 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3851 acc_val: 0.7258 time: 0.0077s\n",
      "Epoch: 00872 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3338 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00873 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3437 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00874 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3457 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00875 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.4280 acc_val: 0.7258 time: 0.0060s\n",
      "Epoch: 00876 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3696 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00877 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.4236 acc_val: 0.7258 time: 0.0060s\n",
      "Epoch: 00878 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3293 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00879 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3511 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00880 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4780 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00881 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3386 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00882 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3481 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00883 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3787 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00884 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3397 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00885 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3837 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00886 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3386 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00887 loss_train: 0.0451 loss_rec: 0.0451 acc_train: 0.9926 loss_val: 1.5633 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00888 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3385 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00889 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3384 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00890 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.5971 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00891 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3411 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00892 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3383 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00893 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3682 acc_val: 0.7258 time: 0.0071s\n",
      "Epoch: 00894 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3910 acc_val: 0.7258 time: 0.0053s\n",
      "Epoch: 00895 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3639 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00896 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.4736 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00897 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3457 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00898 loss_train: 0.0450 loss_rec: 0.0450 acc_train: 0.9926 loss_val: 1.6510 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00899 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3457 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00900 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3503 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00901 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.4093 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00902 loss_train: 0.0451 loss_rec: 0.0451 acc_train: 0.9926 loss_val: 1.3782 acc_val: 0.7258 time: 0.0053s\n",
      "Epoch: 00903 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3569 acc_val: 0.7258 time: 0.0056s\n",
      "Epoch: 00904 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3682 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00905 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3899 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00906 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.4055 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00907 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4537 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00908 loss_train: 0.1537 loss_rec: 0.1537 acc_train: 0.9926 loss_val: 1.4118 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00909 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3841 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00910 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.4496 acc_val: 0.7258 time: 0.0054s\n",
      "Epoch: 00911 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3389 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00912 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3599 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00913 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.4521 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00914 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3549 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00915 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3602 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00916 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3820 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00917 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.5321 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00918 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3825 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00919 loss_train: 0.0477 loss_rec: 0.0477 acc_train: 0.9926 loss_val: 1.4463 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00920 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3432 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00921 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3906 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00922 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3719 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00923 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3666 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00924 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3423 acc_val: 0.7258 time: 0.0025s\n",
      "Epoch: 00925 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3616 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00926 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.4979 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00927 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3370 acc_val: 0.7258 time: 0.0039s\n",
      "Epoch: 00928 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3496 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00929 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4959 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00930 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3829 acc_val: 0.7258 time: 0.0049s\n",
      "Epoch: 00931 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3371 acc_val: 0.7258 time: 0.0034s\n",
      "Epoch: 00932 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3393 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00933 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3282 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00934 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.6752 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00935 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3633 acc_val: 0.7258 time: 0.0053s\n",
      "Epoch: 00936 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3370 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00937 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3476 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00938 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3828 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00939 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3430 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00940 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4669 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00941 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3599 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00942 loss_train: 0.0460 loss_rec: 0.0460 acc_train: 0.9926 loss_val: 1.3448 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00943 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3854 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00944 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3581 acc_val: 0.7258 time: 0.0052s\n",
      "Epoch: 00945 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3815 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00946 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3718 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00947 loss_train: 0.0479 loss_rec: 0.0479 acc_train: 0.9926 loss_val: 1.3615 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00948 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3529 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00949 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3451 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00950 loss_train: 0.0646 loss_rec: 0.0646 acc_train: 0.9853 loss_val: 1.3868 acc_val: 0.7258 time: 0.0038s\n",
      "Epoch: 00951 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.4030 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00952 loss_train: 0.0461 loss_rec: 0.0461 acc_train: 0.9926 loss_val: 1.3529 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00953 loss_train: 0.1211 loss_rec: 0.1211 acc_train: 0.9559 loss_val: 1.3956 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00954 loss_train: 0.0495 loss_rec: 0.0495 acc_train: 0.9926 loss_val: 1.8118 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00955 loss_train: 0.0463 loss_rec: 0.0463 acc_train: 0.9926 loss_val: 1.3321 acc_val: 0.7258 time: 0.0053s\n",
      "Epoch: 00956 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3868 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00957 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3537 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00958 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3895 acc_val: 0.7258 time: 0.0046s\n",
      "Epoch: 00959 loss_train: 0.0459 loss_rec: 0.0459 acc_train: 0.9926 loss_val: 1.3488 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00960 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3512 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00961 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.5667 acc_val: 0.7258 time: 0.0032s\n",
      "Epoch: 00962 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3834 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00963 loss_train: 0.0449 loss_rec: 0.0449 acc_train: 0.9926 loss_val: 1.4301 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00964 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3585 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00965 loss_train: 0.0448 loss_rec: 0.0448 acc_train: 0.9926 loss_val: 1.3615 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00966 loss_train: 0.0448 loss_rec: 0.0448 acc_train: 0.9926 loss_val: 1.4729 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00967 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3514 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00968 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.4127 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00969 loss_train: 0.0568 loss_rec: 0.0568 acc_train: 0.9853 loss_val: 1.3849 acc_val: 0.7258 time: 0.0041s\n",
      "Epoch: 00970 loss_train: 0.0468 loss_rec: 0.0468 acc_train: 0.9926 loss_val: 1.3579 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00971 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3525 acc_val: 0.7258 time: 0.0033s\n",
      "Epoch: 00972 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3471 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00973 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3795 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00974 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.4091 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00975 loss_train: 0.1304 loss_rec: 0.1304 acc_train: 0.9926 loss_val: 1.3635 acc_val: 0.7258 time: 0.0047s\n",
      "Epoch: 00976 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3657 acc_val: 0.7258 time: 0.0037s\n",
      "Epoch: 00977 loss_train: 0.0450 loss_rec: 0.0450 acc_train: 0.9926 loss_val: 1.3570 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00978 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3903 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 00979 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3529 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00980 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3590 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00981 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.4076 acc_val: 0.7258 time: 0.0043s\n",
      "Epoch: 00982 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3813 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00983 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3515 acc_val: 0.7258 time: 0.0044s\n",
      "Epoch: 00984 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3498 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00985 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3851 acc_val: 0.7258 time: 0.0030s\n",
      "Epoch: 00986 loss_train: 0.0452 loss_rec: 0.0452 acc_train: 0.9926 loss_val: 1.3803 acc_val: 0.7258 time: 0.0048s\n",
      "Epoch: 00987 loss_train: 0.0458 loss_rec: 0.0458 acc_train: 0.9926 loss_val: 1.3545 acc_val: 0.7258 time: 0.0050s\n",
      "Epoch: 00988 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3862 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00989 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3725 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00990 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3633 acc_val: 0.7258 time: 0.0036s\n",
      "Epoch: 00991 loss_train: 0.0453 loss_rec: 0.0453 acc_train: 0.9926 loss_val: 1.3814 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00992 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3956 acc_val: 0.7258 time: 0.0040s\n",
      "Epoch: 00993 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.3702 acc_val: 0.7258 time: 0.0063s\n",
      "Epoch: 00994 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3509 acc_val: 0.7258 time: 0.0057s\n",
      "Epoch: 00995 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.3711 acc_val: 0.7258 time: 0.0051s\n",
      "Epoch: 00996 loss_train: 0.0454 loss_rec: 0.0454 acc_train: 0.9926 loss_val: 1.4318 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00997 loss_train: 0.0450 loss_rec: 0.0450 acc_train: 0.9926 loss_val: 1.4740 acc_val: 0.7258 time: 0.0045s\n",
      "Epoch: 00998 loss_train: 0.0457 loss_rec: 0.0457 acc_train: 0.9926 loss_val: 1.3554 acc_val: 0.7258 time: 0.0055s\n",
      "Epoch: 00999 loss_train: 0.0456 loss_rec: 0.0456 acc_train: 0.9926 loss_val: 1.3745 acc_val: 0.7258 time: 0.0035s\n",
      "Epoch: 01000 loss_train: 0.0455 loss_rec: 0.0455 acc_train: 0.9926 loss_val: 1.4887 acc_val: 0.7258 time: 0.0043s\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "encoder_s = GCN_Encoder_s(nfeat=features.shape[1],\n",
    "        nhid=args.n_hidden,\n",
    "        nembed=args.n_hidden,\n",
    "        dropout=args.dropout)\n",
    "classifier_s = GCN_Classifier_s(nembed=args.n_hidden, \n",
    "        nhid=args.n_hidden, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=args.dropout, device = device)\n",
    "decoder_s = Decoder_s(nembed=args.n_hidden,\n",
    "        dropout=args.dropout)\n",
    "optimizer_en = optim.Adam(encoder_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer_cls = optim.Adam(classifier_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder_s.train()\n",
    "    classifier_s.train()\n",
    "    decoder_s.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj_mtx)\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon' or args.setting == 'newG_cls':\n",
    "        ori_num = labels.shape[0]\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.recon_upsample(embed, labels, train_idx, adj = adj_mtx.detach().to_dense(), portion = args.up_scale, im_class_num = args.im_class_num)\n",
    "        generated_G = decoder_s(embed)\n",
    "\n",
    "        loss_rec = utils.adj_mse_loss(generated_G[: ori_num, :][:, : ori_num], adj_mtx.detach().to_dense())\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = copy.deepcopy(generated_G.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "\n",
    "            #ipdb.set_trace()\n",
    "            edge_ac = adj_new[: ori_num, : ori_num].eq(adj_mtx.to_dense()).double().sum()/(ori_num**2)\n",
    "        else:\n",
    "            adj_new = generated_G\n",
    "            edge_ac = F.l1_loss(adj_new[: ori_num, : ori_num], adj_mtx.to_dense(), reduction = 'mean')\n",
    "\n",
    "\n",
    "        #calculate generation information\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"after filtering, edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new[:ori_num, :][:, :ori_num] = adj_mtx.detach().to_dense()\n",
    "        #adj_new = adj_new.to_sparse()\n",
    "        #ipdb.set_trace()\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "        if args.setting == 'newG_cls':\n",
    "            idx_train_new = train_idx\n",
    "\n",
    "    elif args.setting == 'embed_up':\n",
    "        #perform SMOTE in embedding space\n",
    "        embed, labels_new, idx_train_new = utils.recon_upsample(embed, labels, train_idx, portion=args.up_scale, im_class_num = args.im_class_num)\n",
    "        adj_new = adj_mtx\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = train_idx\n",
    "        adj_new = adj_mtx\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    output = classifier_s(embed, adj_new)\n",
    "\n",
    "\n",
    "\n",
    "    if args.setting == 'reweight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        weight[-args.im_class_num:] = 1 + args.up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1), weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1))\n",
    "\n",
    "    acc_train = accuracy(output[train_idx], labels_new[train_idx].reshape(-1))\n",
    "    if args.setting == 'recon_newG':\n",
    "        loss = loss_train + loss_rec * args.rec_weight\n",
    "    elif args.setting == 'recon':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "\n",
    "    loss.backward()\n",
    "    if args.setting == 'newG_cls':\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_de.zero_grad()\n",
    "    else:\n",
    "        optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon':\n",
    "        optimizer_de.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "    acc_val = accuracy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "def encode_onehot_torch(labels):\n",
    "    num_classes = int(labels.max() + 1)\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arc_selection-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f122987ad9a59b0c863ec73977cb4833edd644652b774e5b01a9e2fe636c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
