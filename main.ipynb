{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.GraphConvolution import GCN_Encoder_s, GCN_Classifier_s, Decoder_s\n",
    "from utils.GraphConvolution import GraphConvolution, GCN_Encoder3\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w, sigmoid\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_W = 0.01\n",
    "    dropout = 0.5\n",
    "    dropout_W = 0.5\n",
    "    gamma = 1\n",
    "    no_cuda = False\n",
    "    train_ratio=0.6\n",
    "    test_ratio=0.2\n",
    "    n_classes = 2\n",
    "    seed = 1234\n",
    "    torch.manual_seed(seed)\n",
    "    # -----------------------\n",
    "    dataset = \"cora\"\n",
    "    # dataset = \"haberman\"\n",
    "    # dataset = \"diabetes\"\n",
    "    # -----------------------\n",
    "    order = 4\n",
    "    n_features = 0\n",
    "    w_val_size = 10\n",
    "    # imbalance_ratio = None\n",
    "    imbalance_ratio = None\n",
    "    n_hidden = 64\n",
    "    setting = None\n",
    "    im_class_num = 1\n",
    "    setting = \"upsampling\"\n",
    "    opt_new_G = False\n",
    "    up_scale = 1\n",
    "    im_ratio = 0.5\n",
    "    val_size = 10\n",
    "    # -----------------------\n",
    "    # momentum = 0 # For SGD\n",
    "    momentum = 0.9\n",
    "    # -----------------------\n",
    "    optimizer_alg = \"ADAM\"\n",
    "    # optimizer_alg = \"Momentum\"\n",
    "    # optimizer_alg = \"RMSProp\"\n",
    "    # -----------------------\n",
    "    # activation_func = \"ReLU\"\n",
    "    # activation_func = \"Sigmoid\"\n",
    "    activation_func = \"LeakyReLU\"\n",
    "    # activation_func = \"PReLU\"\n",
    "    # activation_func = \"PReLU\"\n",
    "    # -----------------------\n",
    "    initalization = \"Xavier Uniform\"\n",
    "    # initalization = \"Xavier Normal\"\n",
    "    # initalization = \"Kaiming Uniform\"\n",
    "    # initalization = \"Kaiming Normal\"\n",
    "    # initalization = \"Uniform\" # Uniform with 1 over squre root of the fan in \n",
    "    # -----------------------\n",
    "    res_connection = False\n",
    "    res_connection = True\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m data_loader_diabetes, data_loader_haberman, data_loader_cora\n\u001b[0;32m      3\u001b[0m cora_adj_mtx, cora_labels_df, cora_features_df, \\\n\u001b[1;32m----> 4\u001b[0m         cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features \u001b[39m=\u001b[39m data_loader_cora(args)\n\u001b[0;32m      6\u001b[0m diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n\u001b[0;32m      7\u001b[0m         diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features \u001b[39m=\u001b[39m data_loader_diabetes(args)\n\u001b[0;32m      9\u001b[0m haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n\u001b[0;32m     10\u001b[0m         haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features \u001b[39m=\u001b[39m data_loader_haberman(args)\n",
      "File \u001b[1;32md:\\ENSF 619\\ENSF-619-Final-Project\\utils\\data_loader.py:30\u001b[0m, in \u001b[0;36mdata_loader_cora\u001b[1;34m(args, path)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"Load citation network dataset (cora only for now)\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#input: idx_features_labels, adj\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#idx,labels are not required to be processed in advance\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#adj: save in the form of edges. idx1 idx2 \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#output: adj, features, labels are all torch.tensor, in the dense form\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m#-------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# print('Loading {} dataset...'.format(dataset))\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m idx_features_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mgenfromtxt(\u001b[39m\"\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m{}\u001b[39;49;00m\u001b[39m.content\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(path, \u001b[39m\"\u001b[39;49m\u001b[39mcora\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     31\u001b[0m                                     dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mdtype(\u001b[39mstr\u001b[39;49m))\n\u001b[0;32m     32\u001b[0m features \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(idx_features_labels[:, \u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     33\u001b[0m labels \u001b[39m=\u001b[39m idx_features_labels[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\numpy\\lib\\npyio.py:2276\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2275\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2276\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2277\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2279\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2280\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2281\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\numpy\\lib\\npyio.py:2276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2275\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2276\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2277\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2279\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2280\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2281\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\numpy\\lib\\npyio.py:2276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2275\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2276\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39;49m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2277\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2279\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2280\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2281\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\numpy\\lib\\_iotools.py:672\u001b[0m, in \u001b[0;36mStringConverter._loose_call\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_loose_call\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[0;32m    671\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 672\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(value)\n\u001b[0;32m    673\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\numpy\\compat\\py3k.py:34\u001b[0m, in \u001b[0;36masunicode\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     31\u001b[0m unicode \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m\n\u001b[1;32m---> 34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39masunicode\u001b[39m(s):\n\u001b[0;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(s, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.data_loader import data_loader_diabetes, data_loader_haberman, data_loader_cora\n",
    "\n",
    "cora_adj_mtx, cora_labels_df, cora_features_df, \\\n",
    "        cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features = data_loader_cora(args)\n",
    "        \n",
    "diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n",
    "        diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features = data_loader_diabetes(args)\n",
    "\n",
    "haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n",
    "        haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features = data_loader_haberman(args)\n",
    "\n",
    "if args.dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = diabetes_n_features\n",
    "    features = diabetes_features_df\n",
    "    labels = diabetes_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif args.dataset == \"cora\":\n",
    "    adj_mtx = cora_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = cora_n_features\n",
    "    features = cora_features_df\n",
    "    labels = cora_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = cora_train_idx\n",
    "    val_idx = cora_val_idx\n",
    "    test_idx = cora_test_idx\n",
    "elif args.dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = haberman_n_features\n",
    "    features = haberman_features_df\n",
    "    labels = haberman_labels_df\n",
    "    # train_X = haberman_train_X_df\n",
    "    # train_Y = haberman_train_Y_df\n",
    "    # val_X = haberman_val_X_df\n",
    "    # val_Y = haberman_val_Y_df\n",
    "    # test_X = haberman_test_X_df\n",
    "    # test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(pd.DataFrame(labels[train_idx])[0].unique()) == len(pd.DataFrame(labels[val_idx])[0].unique()) == len(pd.DataFrame(labels[test_idx])[0].unique()), \\\n",
    "    # \"There are some classes missing in one the 3 partitiones of the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if False else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to Tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.from_numpy(np.concatenate((train_X, val_X, test_X), axis=0)).to(device)\n",
    "# labels = torch.from_numpy(np.int64(np.concatenate((train_Y, val_Y, test_Y), axis=0))).to(device)\n",
    "train_idx = torch.from_numpy(np.array(train_idx, dtype = np.int64)).to(device)\n",
    "val_idx = torch.from_numpy(np.array(val_idx, dtype = np.int64)).to(device)\n",
    "test_idx = torch.from_numpy(np.array(test_idx, dtype = np.int64)).to(device)\n",
    "features = torch.from_numpy(np.array(features, dtype = np.float64)).to(device)\n",
    "labels = torch.from_numpy(np.array(labels, dtype = np.int64)).to(device)\n",
    "try:\n",
    "    adj_mtx = torch.from_numpy(np.array(adj_mtx, dtype = np.float64)).to(device)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "\n",
    "encoder_n = GCN_Encoder3(nfeat = n_features,\n",
    "        nhid = n_hidden,\n",
    "        nembed = n_hidden[-1],\n",
    "        dropout = args.dropout,\n",
    "        nclass = args.n_classes, \n",
    "        init = args.initalization,\n",
    "        order = 1, \n",
    "        res_connection = args.res_connection)\n",
    "classifier_n = GCN_Classifier(nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, init = args.initalization,\n",
    "        device = device)\n",
    "\n",
    "# encoder = GCN_Encoder_s(nfeat = n_features, nhid = n_hidden[-1], nembed = n_hidden[-1], dropout = args.dropout)\n",
    "# classifier = GCN_Classifier_s(nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# encoder_n = GCN_Encoder_w(nfeat = n_features, nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# optimizer_n = optim.Adam(encoder_n.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "        encoder_n.train()\n",
    "        classifier_n.train()\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        # optimizer_n.zero_grad()\n",
    "        embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "        output = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "        # output = encoder_n(features, adj_mtx)\n",
    "        out = output[train_idx]\n",
    "        gt = labels[train_idx].reshape(-1)\n",
    "        if args.setting == 'reweight':\n",
    "                weight = \"STH\"\n",
    "                loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "        else:\n",
    "                loss_train = F.cross_entropy(out, gt)\n",
    "        acc_train = accuracy(out, gt)\n",
    "        loss_train.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        # encoder_n.step()\n",
    "        gt_v = labels[val_idx].reshape(-1)\n",
    "        out_v = output[val_idx]\n",
    "        loss_val = F.cross_entropy(out_v, gt_v)\n",
    "        acc_val = accuracy(out_v, gt_v)\n",
    "        # print_class_acc(out_v, gt_v)\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "        print_class_acc(output[val_idx], labels[val_idx])\n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_n.eval()\n",
    "    classifier_n.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "    outputs = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.05278420448303223,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ec70d358834299a4ff91f12b049a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 36.9000 acc_train: 0.1607 loss_val: 22.9960 acc_val: 0.1494 time: 0.1395s\n",
      "valid current auc-roc score: 0.536269, current macro_F score: 0.115174\n",
      "Test set results: loss= 11.7497 accuracy= 0.2140\n",
      "test current auc-roc score: 0.576245, current macro_F score: 0.086943\n",
      "Epoch: 00002 loss_train: 41.1954 acc_train: 0.1669 loss_val: 15.7860 acc_val: 0.1956 time: 0.1319s\n",
      "valid current auc-roc score: 0.564339, current macro_F score: 0.118074\n",
      "Epoch: 00003 loss_train: 16.9599 acc_train: 0.2204 loss_val: 7.7318 acc_val: 0.2509 time: 0.1392s\n",
      "valid current auc-roc score: 0.568774, current macro_F score: 0.150518\n",
      "Epoch: 00004 loss_train: 9.7147 acc_train: 0.2796 loss_val: 4.7028 acc_val: 0.2970 time: 0.1232s\n",
      "valid current auc-roc score: 0.580658, current macro_F score: 0.165637\n",
      "Epoch: 00005 loss_train: 5.8111 acc_train: 0.2937 loss_val: 3.1396 acc_val: 0.3432 time: 0.1326s\n",
      "valid current auc-roc score: 0.633549, current macro_F score: 0.199675\n",
      "Epoch: 00006 loss_train: 5.7841 acc_train: 0.2833 loss_val: 3.2041 acc_val: 0.2989 time: 0.1404s\n",
      "valid current auc-roc score: 0.619924, current macro_F score: 0.219585\n",
      "Epoch: 00007 loss_train: 4.3482 acc_train: 0.2950 loss_val: 2.7930 acc_val: 0.3395 time: 0.1302s\n",
      "valid current auc-roc score: 0.620623, current macro_F score: 0.243499\n",
      "Epoch: 00008 loss_train: 4.5549 acc_train: 0.2789 loss_val: 3.0639 acc_val: 0.2657 time: 0.1440s\n",
      "valid current auc-roc score: 0.591565, current macro_F score: 0.179290\n",
      "Epoch: 00009 loss_train: 3.9552 acc_train: 0.3337 loss_val: 2.7353 acc_val: 0.3118 time: 0.1230s\n",
      "valid current auc-roc score: 0.613919, current macro_F score: 0.187845\n",
      "Epoch: 00010 loss_train: 3.0571 acc_train: 0.3916 loss_val: 2.2491 acc_val: 0.3321 time: 0.1447s\n",
      "valid current auc-roc score: 0.627668, current macro_F score: 0.187024\n",
      "Epoch: 00011 loss_train: 3.5814 acc_train: 0.4212 loss_val: 2.6887 acc_val: 0.4151 time: 0.1202s\n",
      "valid current auc-roc score: 0.687270, current macro_F score: 0.258209\n",
      "Test set results: loss= 1.7975 accuracy= 0.4446\n",
      "test current auc-roc score: 0.699461, current macro_F score: 0.247305\n",
      "Epoch: 00012 loss_train: 3.5439 acc_train: 0.4335 loss_val: 2.9003 acc_val: 0.3985 time: 0.1350s\n",
      "valid current auc-roc score: 0.680718, current macro_F score: 0.242269\n",
      "Epoch: 00013 loss_train: 2.7124 acc_train: 0.4150 loss_val: 2.3021 acc_val: 0.3801 time: 0.1273s\n",
      "valid current auc-roc score: 0.664542, current macro_F score: 0.245574\n",
      "Epoch: 00014 loss_train: 3.3180 acc_train: 0.4002 loss_val: 2.5036 acc_val: 0.3967 time: 0.1374s\n",
      "valid current auc-roc score: 0.677470, current macro_F score: 0.257092\n",
      "Epoch: 00015 loss_train: 2.8365 acc_train: 0.4089 loss_val: 1.7151 acc_val: 0.4207 time: 0.1174s\n",
      "valid current auc-roc score: 0.680478, current macro_F score: 0.227626\n",
      "Epoch: 00016 loss_train: 2.7886 acc_train: 0.3571 loss_val: 2.4783 acc_val: 0.3413 time: 0.1151s\n",
      "valid current auc-roc score: 0.610355, current macro_F score: 0.185287\n",
      "Epoch: 00017 loss_train: 2.1540 acc_train: 0.4021 loss_val: 1.8870 acc_val: 0.3653 time: 0.1139s\n",
      "valid current auc-roc score: 0.682320, current macro_F score: 0.211478\n",
      "Epoch: 00018 loss_train: 2.7903 acc_train: 0.3996 loss_val: 2.5624 acc_val: 0.3450 time: 0.1115s\n",
      "valid current auc-roc score: 0.650171, current macro_F score: 0.207270\n",
      "Epoch: 00019 loss_train: 2.0255 acc_train: 0.4255 loss_val: 1.9928 acc_val: 0.3727 time: 0.1155s\n",
      "valid current auc-roc score: 0.695884, current macro_F score: 0.234143\n",
      "Epoch: 00020 loss_train: 1.8588 acc_train: 0.4403 loss_val: 1.8654 acc_val: 0.4059 time: 0.1099s\n",
      "valid current auc-roc score: 0.717315, current macro_F score: 0.288130\n",
      "Epoch: 00021 loss_train: 2.5610 acc_train: 0.3947 loss_val: 2.5937 acc_val: 0.3229 time: 0.1183s\n",
      "valid current auc-roc score: 0.648631, current macro_F score: 0.189222\n",
      "Test set results: loss= 1.6327 accuracy= 0.4760\n",
      "test current auc-roc score: 0.787341, current macro_F score: 0.323931\n",
      "Epoch: 00022 loss_train: 1.8249 acc_train: 0.4353 loss_val: 1.9273 acc_val: 0.3764 time: 0.1064s\n",
      "valid current auc-roc score: 0.690572, current macro_F score: 0.261161\n",
      "Epoch: 00023 loss_train: 1.9356 acc_train: 0.4711 loss_val: 1.7764 acc_val: 0.4280 time: 0.1109s\n",
      "valid current auc-roc score: 0.722626, current macro_F score: 0.288627\n",
      "Epoch: 00024 loss_train: 1.6981 acc_train: 0.4341 loss_val: 1.7058 acc_val: 0.3911 time: 0.1070s\n",
      "valid current auc-roc score: 0.706561, current macro_F score: 0.252606\n",
      "Epoch: 00025 loss_train: 1.7385 acc_train: 0.4169 loss_val: 1.6801 acc_val: 0.4096 time: 0.1221s\n",
      "valid current auc-roc score: 0.701481, current macro_F score: 0.274644\n",
      "Epoch: 00026 loss_train: 1.7020 acc_train: 0.4427 loss_val: 1.6710 acc_val: 0.3856 time: 0.1261s\n",
      "valid current auc-roc score: 0.681097, current macro_F score: 0.229276\n",
      "Epoch: 00027 loss_train: 1.5198 acc_train: 0.4495 loss_val: 1.6332 acc_val: 0.4041 time: 0.1139s\n",
      "valid current auc-roc score: 0.683622, current macro_F score: 0.276919\n",
      "Epoch: 00028 loss_train: 1.6101 acc_train: 0.4150 loss_val: 1.6500 acc_val: 0.4059 time: 0.1381s\n",
      "valid current auc-roc score: 0.700903, current macro_F score: 0.251282\n",
      "Epoch: 00029 loss_train: 1.5481 acc_train: 0.4501 loss_val: 1.6391 acc_val: 0.3948 time: 0.1290s\n",
      "valid current auc-roc score: 0.693508, current macro_F score: 0.243227\n",
      "Epoch: 00030 loss_train: 1.6966 acc_train: 0.4027 loss_val: 1.6312 acc_val: 0.3782 time: 0.1416s\n",
      "valid current auc-roc score: 0.703877, current macro_F score: 0.216864\n",
      "Epoch: 00031 loss_train: 1.8031 acc_train: 0.4594 loss_val: 1.8356 acc_val: 0.3838 time: 0.1227s\n",
      "valid current auc-roc score: 0.696460, current macro_F score: 0.233053\n",
      "Test set results: loss= 1.5617 accuracy= 0.4373\n",
      "test current auc-roc score: 0.760633, current macro_F score: 0.261613\n",
      "Epoch: 00032 loss_train: 1.5715 acc_train: 0.4427 loss_val: 1.6854 acc_val: 0.3782 time: 0.1739s\n",
      "valid current auc-roc score: 0.692035, current macro_F score: 0.211792\n",
      "Epoch: 00033 loss_train: 1.5957 acc_train: 0.4181 loss_val: 1.6527 acc_val: 0.3893 time: 0.1801s\n",
      "valid current auc-roc score: 0.703943, current macro_F score: 0.231469\n",
      "Epoch: 00034 loss_train: 1.4707 acc_train: 0.4458 loss_val: 1.6215 acc_val: 0.3875 time: 0.1658s\n",
      "valid current auc-roc score: 0.715726, current macro_F score: 0.243561\n",
      "Epoch: 00035 loss_train: 1.5318 acc_train: 0.4575 loss_val: 1.6223 acc_val: 0.3801 time: 0.1939s\n",
      "valid current auc-roc score: 0.720888, current macro_F score: 0.242434\n",
      "Epoch: 00036 loss_train: 1.5078 acc_train: 0.4489 loss_val: 1.7033 acc_val: 0.3801 time: 0.1680s\n",
      "valid current auc-roc score: 0.691978, current macro_F score: 0.231594\n",
      "Epoch: 00037 loss_train: 1.4799 acc_train: 0.4581 loss_val: 1.6466 acc_val: 0.3838 time: 0.1699s\n",
      "valid current auc-roc score: 0.687824, current macro_F score: 0.247791\n",
      "Epoch: 00038 loss_train: 1.4209 acc_train: 0.4483 loss_val: 1.5753 acc_val: 0.3819 time: 0.1803s\n",
      "valid current auc-roc score: 0.735387, current macro_F score: 0.229594\n",
      "Epoch: 00039 loss_train: 1.4097 acc_train: 0.4637 loss_val: 1.6225 acc_val: 0.3948 time: 0.1829s\n",
      "valid current auc-roc score: 0.734662, current macro_F score: 0.250897\n",
      "Epoch: 00040 loss_train: 1.4766 acc_train: 0.4895 loss_val: 1.7242 acc_val: 0.4207 time: 0.1882s\n",
      "valid current auc-roc score: 0.754219, current macro_F score: 0.281969\n",
      "Epoch: 00041 loss_train: 1.3823 acc_train: 0.4982 loss_val: 1.5914 acc_val: 0.4022 time: 0.1995s\n",
      "valid current auc-roc score: 0.735764, current macro_F score: 0.281211\n",
      "Test set results: loss= 1.4874 accuracy= 0.4945\n",
      "test current auc-roc score: 0.827022, current macro_F score: 0.323398\n",
      "Epoch: 00042 loss_train: 1.3677 acc_train: 0.5031 loss_val: 1.5592 acc_val: 0.4244 time: 0.1971s\n",
      "valid current auc-roc score: 0.767967, current macro_F score: 0.296000\n",
      "Epoch: 00043 loss_train: 1.4358 acc_train: 0.5179 loss_val: 1.5671 acc_val: 0.4428 time: 0.1875s\n",
      "valid current auc-roc score: 0.774371, current macro_F score: 0.321844\n",
      "Epoch: 00044 loss_train: 1.4863 acc_train: 0.4815 loss_val: 1.6163 acc_val: 0.4170 time: 0.1967s\n",
      "valid current auc-roc score: 0.779526, current macro_F score: 0.313974\n",
      "Epoch: 00045 loss_train: 1.3595 acc_train: 0.5160 loss_val: 1.5428 acc_val: 0.4391 time: 0.1677s\n",
      "valid current auc-roc score: 0.779304, current macro_F score: 0.320214\n",
      "Epoch: 00046 loss_train: 1.2615 acc_train: 0.5530 loss_val: 1.4933 acc_val: 0.4834 time: 0.1668s\n",
      "valid current auc-roc score: 0.788933, current macro_F score: 0.364179\n",
      "Epoch: 00047 loss_train: 1.3359 acc_train: 0.5394 loss_val: 1.4515 acc_val: 0.4410 time: 0.1585s\n",
      "valid current auc-roc score: 0.804408, current macro_F score: 0.321164\n",
      "Epoch: 00048 loss_train: 1.3390 acc_train: 0.5567 loss_val: 1.5279 acc_val: 0.4834 time: 0.1530s\n",
      "valid current auc-roc score: 0.811734, current macro_F score: 0.358062\n",
      "Epoch: 00049 loss_train: 1.2464 acc_train: 0.5665 loss_val: 1.4995 acc_val: 0.4908 time: 0.1596s\n",
      "valid current auc-roc score: 0.802368, current macro_F score: 0.379822\n",
      "Epoch: 00050 loss_train: 1.2475 acc_train: 0.5456 loss_val: 1.4061 acc_val: 0.5037 time: 0.1644s\n",
      "valid current auc-roc score: 0.817293, current macro_F score: 0.387398\n",
      "Epoch: 00051 loss_train: 1.4780 acc_train: 0.5431 loss_val: 1.6831 acc_val: 0.4852 time: 0.1495s\n",
      "valid current auc-roc score: 0.801436, current macro_F score: 0.348967\n",
      "Test set results: loss= 1.3361 accuracy= 0.5627\n",
      "test current auc-roc score: 0.859190, current macro_F score: 0.424603\n",
      "Epoch: 00052 loss_train: 1.2727 acc_train: 0.5782 loss_val: 1.3929 acc_val: 0.5092 time: 0.1481s\n",
      "valid current auc-roc score: 0.819461, current macro_F score: 0.395591\n",
      "Epoch: 00053 loss_train: 1.2144 acc_train: 0.5579 loss_val: 1.3900 acc_val: 0.5018 time: 0.1769s\n",
      "valid current auc-roc score: 0.817353, current macro_F score: 0.392104\n",
      "Epoch: 00054 loss_train: 1.2384 acc_train: 0.5788 loss_val: 1.4042 acc_val: 0.4723 time: 0.1453s\n",
      "valid current auc-roc score: 0.828060, current macro_F score: 0.340363\n",
      "Epoch: 00055 loss_train: 1.2849 acc_train: 0.5844 loss_val: 1.3542 acc_val: 0.4982 time: 0.1394s\n",
      "valid current auc-roc score: 0.835067, current macro_F score: 0.378885\n",
      "Epoch: 00056 loss_train: 1.1989 acc_train: 0.5961 loss_val: 1.3707 acc_val: 0.5092 time: 0.1427s\n",
      "valid current auc-roc score: 0.833017, current macro_F score: 0.391885\n",
      "Epoch: 00057 loss_train: 1.2537 acc_train: 0.5616 loss_val: 1.4850 acc_val: 0.4613 time: 0.1327s\n",
      "valid current auc-roc score: 0.819277, current macro_F score: 0.333265\n",
      "Epoch: 00058 loss_train: 1.2246 acc_train: 0.5924 loss_val: 1.4241 acc_val: 0.4889 time: 0.1333s\n",
      "valid current auc-roc score: 0.827751, current macro_F score: 0.386081\n",
      "Epoch: 00059 loss_train: 1.1984 acc_train: 0.6084 loss_val: 1.3474 acc_val: 0.5055 time: 0.1380s\n",
      "valid current auc-roc score: 0.819500, current macro_F score: 0.381996\n",
      "Epoch: 00060 loss_train: 1.1652 acc_train: 0.5837 loss_val: 1.3453 acc_val: 0.5332 time: 0.1285s\n",
      "valid current auc-roc score: 0.839356, current macro_F score: 0.399050\n",
      "Epoch: 00061 loss_train: 1.1854 acc_train: 0.6084 loss_val: 1.3850 acc_val: 0.5590 time: 0.1285s\n",
      "valid current auc-roc score: 0.835771, current macro_F score: 0.436033\n",
      "Test set results: loss= 1.2451 accuracy= 0.5738\n",
      "test current auc-roc score: 0.873685, current macro_F score: 0.430362\n",
      "Epoch: 00062 loss_train: 1.0900 acc_train: 0.6115 loss_val: 1.3624 acc_val: 0.5443 time: 0.1156s\n",
      "valid current auc-roc score: 0.838251, current macro_F score: 0.437679\n",
      "Epoch: 00063 loss_train: 1.1927 acc_train: 0.5850 loss_val: 1.3660 acc_val: 0.5092 time: 0.1079s\n",
      "valid current auc-roc score: 0.821860, current macro_F score: 0.381127\n",
      "Epoch: 00064 loss_train: 1.2319 acc_train: 0.5690 loss_val: 1.3599 acc_val: 0.5185 time: 0.1084s\n",
      "valid current auc-roc score: 0.833617, current macro_F score: 0.372593\n",
      "Epoch: 00065 loss_train: 1.1263 acc_train: 0.6016 loss_val: 1.3722 acc_val: 0.5185 time: 0.1102s\n",
      "valid current auc-roc score: 0.825637, current macro_F score: 0.409499\n",
      "Epoch: 00066 loss_train: 1.1140 acc_train: 0.6213 loss_val: 1.3423 acc_val: 0.5351 time: 0.1157s\n",
      "valid current auc-roc score: 0.838630, current macro_F score: 0.431945\n",
      "Epoch: 00067 loss_train: 1.1468 acc_train: 0.5973 loss_val: 1.4715 acc_val: 0.4852 time: 0.1159s\n",
      "valid current auc-roc score: 0.834163, current macro_F score: 0.397121\n",
      "Epoch: 00068 loss_train: 1.0784 acc_train: 0.6367 loss_val: 1.2707 acc_val: 0.5535 time: 0.0992s\n",
      "valid current auc-roc score: 0.839402, current macro_F score: 0.450819\n",
      "Epoch: 00069 loss_train: 1.1056 acc_train: 0.6293 loss_val: 1.3740 acc_val: 0.5351 time: 0.1094s\n",
      "valid current auc-roc score: 0.838213, current macro_F score: 0.437864\n",
      "Epoch: 00070 loss_train: 1.1021 acc_train: 0.6275 loss_val: 1.4680 acc_val: 0.5221 time: 0.1140s\n",
      "valid current auc-roc score: 0.832026, current macro_F score: 0.428488\n",
      "Epoch: 00071 loss_train: 1.0542 acc_train: 0.6422 loss_val: 1.3521 acc_val: 0.5664 time: 0.1166s\n",
      "valid current auc-roc score: 0.854013, current macro_F score: 0.466822\n",
      "Test set results: loss= 1.2041 accuracy= 0.5959\n",
      "test current auc-roc score: 0.883007, current macro_F score: 0.474878\n",
      "Epoch: 00072 loss_train: 1.0906 acc_train: 0.6349 loss_val: 1.3853 acc_val: 0.5369 time: 0.1103s\n",
      "valid current auc-roc score: 0.842757, current macro_F score: 0.454823\n",
      "Epoch: 00073 loss_train: 1.0550 acc_train: 0.6447 loss_val: 1.3152 acc_val: 0.5590 time: 0.1181s\n",
      "valid current auc-roc score: 0.856707, current macro_F score: 0.450746\n",
      "Epoch: 00074 loss_train: 1.0095 acc_train: 0.6466 loss_val: 1.2852 acc_val: 0.5369 time: 0.1084s\n",
      "valid current auc-roc score: 0.858151, current macro_F score: 0.438526\n",
      "Epoch: 00075 loss_train: 1.0475 acc_train: 0.6533 loss_val: 1.2415 acc_val: 0.5664 time: 0.1044s\n",
      "valid current auc-roc score: 0.857688, current macro_F score: 0.472024\n",
      "Epoch: 00076 loss_train: 1.0372 acc_train: 0.6509 loss_val: 1.3280 acc_val: 0.5406 time: 0.0996s\n",
      "valid current auc-roc score: 0.851781, current macro_F score: 0.446233\n",
      "Epoch: 00077 loss_train: 1.0452 acc_train: 0.6564 loss_val: 1.3038 acc_val: 0.5572 time: 0.1144s\n",
      "valid current auc-roc score: 0.866132, current macro_F score: 0.472052\n",
      "Epoch: 00078 loss_train: 0.9981 acc_train: 0.6650 loss_val: 1.2656 acc_val: 0.5756 time: 0.1056s\n",
      "valid current auc-roc score: 0.865859, current macro_F score: 0.497456\n",
      "Epoch: 00079 loss_train: 0.9943 acc_train: 0.6484 loss_val: 1.2603 acc_val: 0.5775 time: 0.1030s\n",
      "valid current auc-roc score: 0.863928, current macro_F score: 0.497288\n",
      "Epoch: 00080 loss_train: 0.9903 acc_train: 0.6626 loss_val: 1.2622 acc_val: 0.5775 time: 0.1089s\n",
      "valid current auc-roc score: 0.857466, current macro_F score: 0.490229\n",
      "Epoch: 00081 loss_train: 0.9959 acc_train: 0.6712 loss_val: 1.3146 acc_val: 0.5720 time: 0.1082s\n",
      "valid current auc-roc score: 0.865955, current macro_F score: 0.480893\n",
      "Test set results: loss= 1.1517 accuracy= 0.6218\n",
      "test current auc-roc score: 0.893037, current macro_F score: 0.509790\n",
      "Epoch: 00082 loss_train: 0.9219 acc_train: 0.6878 loss_val: 1.2187 acc_val: 0.5793 time: 0.1037s\n",
      "valid current auc-roc score: 0.863334, current macro_F score: 0.499548\n",
      "Epoch: 00083 loss_train: 0.9421 acc_train: 0.6866 loss_val: 1.3283 acc_val: 0.5923 time: 0.0982s\n",
      "valid current auc-roc score: 0.847512, current macro_F score: 0.507678\n",
      "Epoch: 00084 loss_train: 1.0097 acc_train: 0.6583 loss_val: 1.3608 acc_val: 0.5443 time: 0.0994s\n",
      "valid current auc-roc score: 0.844481, current macro_F score: 0.446677\n",
      "Epoch: 00085 loss_train: 0.9647 acc_train: 0.6798 loss_val: 1.2795 acc_val: 0.5812 time: 0.1053s\n",
      "valid current auc-roc score: 0.862362, current macro_F score: 0.502634\n",
      "Epoch: 00086 loss_train: 0.9401 acc_train: 0.6743 loss_val: 1.2523 acc_val: 0.5738 time: 0.0958s\n",
      "valid current auc-roc score: 0.858582, current macro_F score: 0.463495\n",
      "Epoch: 00087 loss_train: 0.9162 acc_train: 0.7007 loss_val: 1.2206 acc_val: 0.6070 time: 0.1037s\n",
      "valid current auc-roc score: 0.860872, current macro_F score: 0.520139\n",
      "Epoch: 00088 loss_train: 0.9370 acc_train: 0.6946 loss_val: 1.2201 acc_val: 0.6089 time: 0.1119s\n",
      "valid current auc-roc score: 0.872670, current macro_F score: 0.540970\n",
      "Epoch: 00089 loss_train: 0.9248 acc_train: 0.6952 loss_val: 1.1944 acc_val: 0.6218 time: 0.1077s\n",
      "valid current auc-roc score: 0.874393, current macro_F score: 0.560157\n",
      "Epoch: 00090 loss_train: 0.8828 acc_train: 0.7057 loss_val: 1.3141 acc_val: 0.5941 time: 0.1038s\n",
      "valid current auc-roc score: 0.872739, current macro_F score: 0.514255\n",
      "Epoch: 00091 loss_train: 0.9305 acc_train: 0.6866 loss_val: 1.1975 acc_val: 0.6125 time: 0.1108s\n",
      "valid current auc-roc score: 0.875181, current macro_F score: 0.528175\n",
      "Test set results: loss= 1.0719 accuracy= 0.6458\n",
      "test current auc-roc score: 0.908864, current macro_F score: 0.576035\n",
      "Epoch: 00092 loss_train: 0.8809 acc_train: 0.7100 loss_val: 1.1843 acc_val: 0.6162 time: 0.1053s\n",
      "valid current auc-roc score: 0.875047, current macro_F score: 0.534544\n",
      "Epoch: 00093 loss_train: 0.8661 acc_train: 0.7032 loss_val: 1.2009 acc_val: 0.5959 time: 0.1037s\n",
      "valid current auc-roc score: 0.871993, current macro_F score: 0.521975\n",
      "Epoch: 00094 loss_train: 0.9475 acc_train: 0.6897 loss_val: 1.1944 acc_val: 0.5996 time: 0.0994s\n",
      "valid current auc-roc score: 0.877944, current macro_F score: 0.532806\n",
      "Epoch: 00095 loss_train: 0.9078 acc_train: 0.6952 loss_val: 1.2101 acc_val: 0.5812 time: 0.1052s\n",
      "valid current auc-roc score: 0.868833, current macro_F score: 0.512927\n",
      "Epoch: 00096 loss_train: 0.8817 acc_train: 0.7167 loss_val: 1.1978 acc_val: 0.6255 time: 0.1269s\n",
      "valid current auc-roc score: 0.893948, current macro_F score: 0.579114\n",
      "Epoch: 00097 loss_train: 0.9113 acc_train: 0.7124 loss_val: 1.1552 acc_val: 0.6181 time: 0.1324s\n",
      "valid current auc-roc score: 0.882483, current macro_F score: 0.546626\n",
      "Epoch: 00098 loss_train: 0.8342 acc_train: 0.7235 loss_val: 1.2213 acc_val: 0.6181 time: 0.1282s\n",
      "valid current auc-roc score: 0.870726, current macro_F score: 0.558727\n",
      "Epoch: 00099 loss_train: 0.8640 acc_train: 0.7143 loss_val: 1.2057 acc_val: 0.5978 time: 0.1246s\n",
      "valid current auc-roc score: 0.880577, current macro_F score: 0.538573\n",
      "Epoch: 00100 loss_train: 0.8604 acc_train: 0.7217 loss_val: 1.1733 acc_val: 0.6273 time: 0.1337s\n",
      "valid current auc-roc score: 0.885704, current macro_F score: 0.582494\n",
      "Epoch: 00101 loss_train: 0.8266 acc_train: 0.7401 loss_val: 1.1839 acc_val: 0.6421 time: 0.1356s\n",
      "valid current auc-roc score: 0.879518, current macro_F score: 0.596363\n",
      "Test set results: loss= 1.0136 accuracy= 0.6827\n",
      "test current auc-roc score: 0.916934, current macro_F score: 0.638203\n",
      "Epoch: 00102 loss_train: 0.8477 acc_train: 0.7321 loss_val: 1.1754 acc_val: 0.6365 time: 0.1396s\n",
      "valid current auc-roc score: 0.887312, current macro_F score: 0.603106\n",
      "Epoch: 00103 loss_train: 0.8886 acc_train: 0.7149 loss_val: 1.1569 acc_val: 0.6070 time: 0.1538s\n",
      "valid current auc-roc score: 0.882078, current macro_F score: 0.559295\n",
      "Epoch: 00104 loss_train: 0.8356 acc_train: 0.7124 loss_val: 1.1728 acc_val: 0.6236 time: 0.1457s\n",
      "valid current auc-roc score: 0.879093, current macro_F score: 0.563773\n",
      "Epoch: 00105 loss_train: 0.8166 acc_train: 0.7266 loss_val: 1.1129 acc_val: 0.6365 time: 0.1401s\n",
      "valid current auc-roc score: 0.884411, current macro_F score: 0.572497\n",
      "Epoch: 00106 loss_train: 0.8581 acc_train: 0.7235 loss_val: 1.2068 acc_val: 0.6273 time: 0.1368s\n",
      "valid current auc-roc score: 0.884459, current macro_F score: 0.568867\n",
      "Epoch: 00107 loss_train: 0.8362 acc_train: 0.7340 loss_val: 1.1255 acc_val: 0.6421 time: 0.1467s\n",
      "valid current auc-roc score: 0.894636, current macro_F score: 0.580838\n",
      "Epoch: 00108 loss_train: 0.7925 acc_train: 0.7328 loss_val: 1.1790 acc_val: 0.6494 time: 0.1519s\n",
      "valid current auc-roc score: 0.886832, current macro_F score: 0.599786\n",
      "Epoch: 00109 loss_train: 0.8014 acc_train: 0.7309 loss_val: 1.1789 acc_val: 0.6550 time: 0.1566s\n",
      "valid current auc-roc score: 0.890671, current macro_F score: 0.614883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loss_vals \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs)):\n\u001b[1;32m----> 7\u001b[0m         acc_train, acc_val, loss_train, loss_val \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m      8\u001b[0m         acc_trains\u001b[39m.\u001b[39mappend(acc_train)\n\u001b[0;32m      9\u001b[0m         acc_vals\u001b[39m.\u001b[39mappend(acc_val)\n",
      "Cell \u001b[1;32mIn [7], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     51\u001b[0m optimizer_cls\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     52\u001b[0m \u001b[39m# optimizer_n.zero_grad()\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m embed \u001b[39m=\u001b[39m encoder_n(features, adj_mtx, funct \u001b[39m=\u001b[39;49m args\u001b[39m.\u001b[39;49mactivation_func)\n\u001b[0;32m     54\u001b[0m output \u001b[39m=\u001b[39m classifier_n(embed, adj_mtx, funct \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mactivation_func)\n\u001b[0;32m     55\u001b[0m \u001b[39m# output = encoder_n(features, adj_mtx)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\ENSF 619\\ENSF-619-Final-Project\\utils\\GraphConvolution.py:432\u001b[0m, in \u001b[0;36mGCN_Encoder3.forward\u001b[1;34m(self, x, adj, funct)\u001b[0m\n\u001b[0;32m    430\u001b[0m         x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m    431\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgc[i](x, adj)\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m--> 432\u001b[0m         x \u001b[39m=\u001b[39m func(x)\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m    433\u001b[0m         x \u001b[39m=\u001b[39m temp \u001b[39m+\u001b[39m x\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\IGDM_Lab\\anaconda3\\envs\\arc_selection-master\\lib\\site-packages\\torch\\nn\\functional.py:1632\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1630\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mleaky_relu_(\u001b[39minput\u001b[39m, negative_slope)\n\u001b[0;32m   1631\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1632\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mleaky_relu(\u001b[39minput\u001b[39;49m, negative_slope)\n\u001b[0;32m   1633\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained 2 layer classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_n.state_dict()\n",
    "    saved_content['classifier'] = classifier_n.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Normal_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "encoder_s = GCN_Encoder_s(nfeat = features.shape[1],\n",
    "        nhid = args.n_hidden,\n",
    "        nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "classifier_s = GCN_Classifier_s(nembed=args.n_hidden, \n",
    "        nhid = args.n_hidden, \n",
    "        nclass = labels.max().item() + 1, \n",
    "        dropout = args.dropout, device = device, init = args.initalization)\n",
    "decoder_s = Decoder_s(nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "# optimizer_en = optim.Adam(encoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_cls = optim.Adam(classifier_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_de = optim.SGD(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.RMSprop(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.RMSprop(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.RMSprop(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder_s.train()\n",
    "    classifier_s.train()\n",
    "    decoder_s.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon' or args.setting == 'newG_cls':\n",
    "        ori_num = labels.shape[0]\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.recon_upsample(embed, labels, train_idx, adj = adj_mtx.detach().to_dense(), portion = args.up_scale, im_class_num = args.im_class_num)\n",
    "        generated_G = decoder_s(embed)\n",
    "\n",
    "        loss_rec = utils.adj_mse_loss(generated_G[: ori_num, :][:, : ori_num], adj_mtx.detach().to_dense())\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = copy.deepcopy(generated_G.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "\n",
    "            #ipdb.set_trace()\n",
    "            edge_ac = adj_new[: ori_num, : ori_num].eq(adj_mtx.to_dense()).double().sum()/(ori_num**2)\n",
    "        else:\n",
    "            adj_new = generated_G\n",
    "            edge_ac = F.l1_loss(adj_new[: ori_num, : ori_num], adj_mtx.to_dense(), reduction = 'mean')\n",
    "\n",
    "\n",
    "        #calculate generation information\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"after filtering, edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new[:ori_num, :][:, :ori_num] = adj_mtx.detach().to_dense()\n",
    "        #adj_new = adj_new.to_sparse()\n",
    "        #ipdb.set_trace()\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "        if args.setting == 'newG_cls':\n",
    "            idx_train_new = train_idx\n",
    "\n",
    "    elif args.setting == 'embed_up':\n",
    "        #perform SMOTE in embedding space\n",
    "        embed, labels_new, idx_train_new = utils.recon_upsample(embed, labels, train_idx, portion=args.up_scale, im_class_num = args.im_class_num)\n",
    "        adj_new = adj_mtx\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = train_idx\n",
    "        adj_new = adj_mtx\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    output = classifier_s(embed, adj_new)\n",
    "\n",
    "\n",
    "\n",
    "    if args.setting == 'reweight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        weight[-args.im_class_num:] = 1 + args.up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1), weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1))\n",
    "\n",
    "    acc_train = accuracy(output[train_idx], labels_new[train_idx].reshape(-1))\n",
    "    if args.setting == 'recon_newG':\n",
    "        loss = loss_train + loss_rec * args.rec_weight\n",
    "    elif args.setting == 'recon':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "\n",
    "    loss.backward()\n",
    "    if args.setting == 'newG_cls':\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_de.zero_grad()\n",
    "    else:\n",
    "        optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon':\n",
    "        optimizer_de.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "    acc_val = accuracy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_s.eval()\n",
    "    classifier_s.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "    outputs = classifier_s(embed, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(args.epochs)):\n",
    "        train(epoch, features, labels)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained GraphSMOTE's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_s.state_dict()\n",
    "    saved_content['classifier'] = classifier_s.state_dict()\n",
    "    saved_content['decoder'] = decoder_s.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/GraphSMOTE_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "from utils.reweight import next, next2\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_w(nfeat = n_features, \n",
    "        nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, \n",
    "        device = device, init = args.initalization)\n",
    "# optimizer = optim.Adam(encoder.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "def train(epoch, features, labels):\n",
    "        encoder.train()\n",
    "        # classifier.train()\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        with higher.innerloop_ctx(encoder, optimizer) as (meta_model, meta_opt):\n",
    "                meta_train_outputs = meta_model(features, adj_mtx)\n",
    "                # criterion.reduction = 'none'\n",
    "                # new_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_train_outputs[train_idx], labels[train_idx])\n",
    "                # meta_train_loss = criterion(meta_train_outputs[train_idx], new_labels[train_idx])\n",
    "                eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=device)\n",
    "                meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "                meta_opt.step(meta_train_loss)\n",
    "                sampled_val_idx, new_adj_mtx, new_features, new_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                # meta_inputs, meta_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                meta_val_idx, meta_adj_mtx, meta_features, meta_labels = sampled_val_idx.to(device=device, non_blocking=True), \\\n",
    "                        new_adj_mtx.to(device=device, non_blocking=True), \\\n",
    "                        new_features.to(device=device, non_blocking=True), \\\n",
    "                        new_labels.to(device=device, non_blocking=True)\n",
    "                meta_val_outputs = meta_model(meta_features, meta_adj_mtx.double())\n",
    "                # criterion.reduction = 'mean'\n",
    "                # new_meta_labels = F.one_hot(meta_labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_val_outputs, meta_labels)\n",
    "                # meta_val_loss = criterion(meta_val_outputs, new_meta_labels)\n",
    "                eps_grads = torch.autograd.grad(meta_train_loss, eps)[0].detach()\n",
    "        w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "        l1_norm = torch.sum(w_tilde)\n",
    "        if l1_norm != 0:\n",
    "                w = w_tilde / l1_norm\n",
    "        else:\n",
    "                w = w_tilde\n",
    "        outputs = encoder(features, adj_mtx.double())\n",
    "        # criterion.reduction = 'none'\n",
    "        # new_main_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "        loss = F.cross_entropy(outputs[train_idx], labels[train_idx])\n",
    "        # loss = criterion(outputs[train_idx], new_main_labels[train_idx])\n",
    "        loss = torch.sum(w * loss)\n",
    "\n",
    "        loss_train = F.cross_entropy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "        acc_train = accuracy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = F.cross_entropy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "        acc_val = accuracy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        # keep track of epoch loss/accuracy\n",
    "        # train_loss += loss.item() * outputs.shape[0]\n",
    "\n",
    "                # out = output[train_idx]\n",
    "                # gt = labels[train_idx].reshape(-1)\n",
    "                # if args.setting == 'reweight':\n",
    "                #         weight = \"STH\"\n",
    "                #         loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "                # else:\n",
    "                #         loss_train = F.cross_entropy(out, gt)\n",
    "                # acc_train = accuracy(out, gt)\n",
    "                # loss_train.backward()\n",
    "                # optimizer_en.step()\n",
    "                # optimizer_cls.step()\n",
    "                # gt_v = labels[test_idx].reshape(-1)\n",
    "                # out_v = output[test_idx]\n",
    "                # loss_val = F.cross_entropy(out_v, gt_v)\n",
    "                # acc_val = accuracy(out_v, gt_v)\n",
    "                # # print_class_acc(out_v, gt_v)\n",
    "                # print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "                # 'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                # 'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                # 'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                # 'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                # 'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        # return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    outputs = encoder(features, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(args.epochs)):\n",
    "        train(epoch, features, labels)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the reweight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Reweight_{}_{}.pth'.format(args.dataset, args.dataset, args.imbalance_ratio))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arc_selection-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f122987ad9a59b0c863ec73977cb4833edd644652b774e5b01a9e2fe636c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
