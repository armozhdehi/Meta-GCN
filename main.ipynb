{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.GraphConvolution import GCN_Encoder_s, GCN_Classifier_s, Decoder_s\n",
    "from utils.GraphConvolution import GraphConvolution, GCN_Encoder3\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w, sigmoid\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_W = 0.01\n",
    "    dropout = 0.5\n",
    "    dropout_W = 0.5\n",
    "    gamma = 1\n",
    "    no_cuda = False\n",
    "    train_ratio=0.6\n",
    "    test_ratio=0.2\n",
    "    n_classes = 2\n",
    "    seed = 1234\n",
    "    torch.manual_seed(seed)\n",
    "    # -----------------------\n",
    "    dataset = \"cora\"\n",
    "    # dataset = \"haberman\"\n",
    "    # dataset = \"diabetes\"\n",
    "    # -----------------------\n",
    "    order = 4\n",
    "    n_features = 0\n",
    "    w_val_size = 10\n",
    "    # imbalance_ratio = None\n",
    "    imbalance_ratio = 0.05\n",
    "    n_hidden = 64\n",
    "    setting = None\n",
    "    im_class_num = 1\n",
    "    setting = \"upsampling\"\n",
    "    opt_new_G = False\n",
    "    up_scale = 1\n",
    "    im_ratio = 0.5\n",
    "    val_size = 10\n",
    "    # -----------------------\n",
    "    # momentum = 0 # For SGD\n",
    "    momentum = 0\n",
    "    # momentum = 0.5\n",
    "    # momentum = 0.9\n",
    "    # momentum = 0.95\n",
    "    # -----------------------\n",
    "    optimizer_alg = \"ADAM\"\n",
    "    # optimizer_alg = \"Momentum\"\n",
    "    # optimizer_alg = \"RMSProp\"\n",
    "    # -----------------------\n",
    "    # activation_func = \"ReLU\"\n",
    "    # activation_func = \"Sigmoid\"\n",
    "    activation_func = \"LeakyReLU\"\n",
    "    # activation_func = \"PReLU\"\n",
    "    # -----------------------\n",
    "    initalization = \"Xavier Uniform\"\n",
    "    # initalization = \"Xavier Normal\"\n",
    "    # initalization = \"Kaiming Uniform\"\n",
    "    # initalization = \"Kaiming Normal\"\n",
    "    # initalization = \"Uniform\" # Uniform with 1 over squre root of the fan in \n",
    "    # -----------------------\n",
    "    res_connection = False\n",
    "    # res_connection = True\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import data_loader_diabetes, data_loader_haberman, data_loader_cora\n",
    "\n",
    "cora_adj_mtx, cora_labels_df, cora_features_df, \\\n",
    "        cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features = data_loader_cora(args)\n",
    "        \n",
    "diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n",
    "        diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features = data_loader_diabetes(args)\n",
    "\n",
    "haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n",
    "        haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features = data_loader_haberman(args)\n",
    "\n",
    "if args.dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = diabetes_n_features\n",
    "    features = diabetes_features_df\n",
    "    labels = diabetes_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif args.dataset == \"cora\":\n",
    "    adj_mtx = cora_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = cora_n_features\n",
    "    features = cora_features_df\n",
    "    labels = cora_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = cora_train_idx\n",
    "    val_idx = cora_val_idx\n",
    "    test_idx = cora_test_idx\n",
    "elif args.dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = haberman_n_features\n",
    "    features = haberman_features_df\n",
    "    labels = haberman_labels_df\n",
    "    # train_X = haberman_train_X_df\n",
    "    # train_Y = haberman_train_Y_df\n",
    "    # val_X = haberman_val_X_df\n",
    "    # val_Y = haberman_val_Y_df\n",
    "    # test_X = haberman_test_X_df\n",
    "    # test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(pd.DataFrame(labels[train_idx])[0].unique()) == len(pd.DataFrame(labels[val_idx])[0].unique()) == len(pd.DataFrame(labels[test_idx])[0].unique()), \\\n",
    "    # \"There are some classes missing in one the 3 partitiones of the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if False else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to Tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.from_numpy(np.concatenate((train_X, val_X, test_X), axis=0)).to(device)\n",
    "# labels = torch.from_numpy(np.int64(np.concatenate((train_Y, val_Y, test_Y), axis=0))).to(device)\n",
    "train_idx = torch.from_numpy(np.array(train_idx, dtype = np.int64)).to(device)\n",
    "val_idx = torch.from_numpy(np.array(val_idx, dtype = np.int64)).to(device)\n",
    "test_idx = torch.from_numpy(np.array(test_idx, dtype = np.int64)).to(device)\n",
    "features = torch.from_numpy(np.array(features, dtype = np.float64)).to(device)\n",
    "labels = torch.from_numpy(np.array(labels, dtype = np.int64)).to(device)\n",
    "try:\n",
    "    adj_mtx = torch.from_numpy(np.array(adj_mtx, dtype = np.float64)).to(device)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "\n",
    "encoder_n = GCN_Encoder3(nfeat = n_features,\n",
    "        nhid = n_hidden,\n",
    "        nembed = n_hidden[-1],\n",
    "        dropout = args.dropout,\n",
    "        nclass = args.n_classes, \n",
    "        init = args.initalization,\n",
    "        order = 1, \n",
    "        res_connection = args.res_connection)\n",
    "classifier_n = GCN_Classifier(nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, init = args.initalization,\n",
    "        device = device)\n",
    "\n",
    "# encoder = GCN_Encoder_s(nfeat = n_features, nhid = n_hidden[-1], nembed = n_hidden[-1], dropout = args.dropout)\n",
    "# classifier = GCN_Classifier_s(nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# encoder_n = GCN_Encoder_w(nfeat = n_features, nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "# optimizer_n = optim.Adam(encoder_n.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "        encoder_n.train()\n",
    "        classifier_n.train()\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        # optimizer_n.zero_grad()\n",
    "        embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "        output = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "        # output = encoder_n(features, adj_mtx)\n",
    "        out = output[train_idx]\n",
    "        gt = labels[train_idx].reshape(-1)\n",
    "        if args.setting == 'reweight':\n",
    "                weight = \"STH\"\n",
    "                loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "        else:\n",
    "                loss_train = F.cross_entropy(out, gt)\n",
    "        acc_train = accuracy(out, gt)\n",
    "        loss_train.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        # encoder_n.step()\n",
    "        gt_v = labels[val_idx].reshape(-1)\n",
    "        out_v = output[val_idx]\n",
    "        loss_val = F.cross_entropy(out_v, gt_v)\n",
    "        acc_val = accuracy(out_v, gt_v)\n",
    "        # print_class_acc(out_v, gt_v)\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "        print_class_acc(output[val_idx], labels[val_idx])\n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_n.eval()\n",
    "    classifier_n.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_n(features, adj_mtx, funct = args.activation_func)\n",
    "    outputs = classifier_n(embed, adj_mtx, funct = args.activation_func)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')\n",
    "    return loss_test.item(), acc_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 2 layer Classifier training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 1.9151 acc_train: 0.5745 loss_val: 1.6861 acc_val: 0.5092 time: 0.0246s\n",
      "valid current auc-roc score: 0.831708, current macro_F score: 0.467585\n",
      "Test set results: loss= 1.5389 accuracy= 0.6015\n",
      "test current auc-roc score: 0.878088, current macro_F score: 0.568420\n",
      "Epoch: 00002 loss_train: 1.9789 acc_train: 0.5714 loss_val: 1.9671 acc_val: 0.5092 time: 0.0211s\n",
      "valid current auc-roc score: 0.818823, current macro_F score: 0.467538\n",
      "Epoch: 00003 loss_train: 1.7023 acc_train: 0.5862 loss_val: 1.7681 acc_val: 0.5406 time: 0.0208s\n",
      "valid current auc-roc score: 0.839387, current macro_F score: 0.512845\n",
      "Epoch: 00004 loss_train: 1.6070 acc_train: 0.6164 loss_val: 1.6322 acc_val: 0.5461 time: 0.0208s\n",
      "valid current auc-roc score: 0.845915, current macro_F score: 0.508518\n",
      "Epoch: 00005 loss_train: 1.4767 acc_train: 0.6127 loss_val: 1.8431 acc_val: 0.5646 time: 0.0186s\n",
      "valid current auc-roc score: 0.852035, current macro_F score: 0.504993\n",
      "Epoch: 00006 loss_train: 1.7013 acc_train: 0.6466 loss_val: 1.8989 acc_val: 0.5517 time: 0.0172s\n",
      "valid current auc-roc score: 0.854216, current macro_F score: 0.520589\n",
      "Epoch: 00007 loss_train: 1.1458 acc_train: 0.6619 loss_val: 1.5982 acc_val: 0.5646 time: 0.0183s\n",
      "valid current auc-roc score: 0.850715, current macro_F score: 0.518937\n",
      "Epoch: 00008 loss_train: 1.5989 acc_train: 0.6755 loss_val: 1.8208 acc_val: 0.5849 time: 0.0188s\n",
      "valid current auc-roc score: 0.864984, current macro_F score: 0.527486\n",
      "Epoch: 00009 loss_train: 1.2731 acc_train: 0.6558 loss_val: 1.4817 acc_val: 0.5978 time: 0.0190s\n",
      "valid current auc-roc score: 0.853298, current macro_F score: 0.554065\n",
      "Epoch: 00010 loss_train: 1.1589 acc_train: 0.6927 loss_val: 1.3219 acc_val: 0.6162 time: 0.0177s\n",
      "valid current auc-roc score: 0.866473, current macro_F score: 0.578856\n",
      "Epoch: 00011 loss_train: 1.1788 acc_train: 0.6755 loss_val: 1.6855 acc_val: 0.5683 time: 0.0180s\n",
      "valid current auc-roc score: 0.855446, current macro_F score: 0.524744\n",
      "Test set results: loss= 1.3000 accuracy= 0.6273\n",
      "test current auc-roc score: 0.891556, current macro_F score: 0.601589\n",
      "Epoch: 00012 loss_train: 1.1768 acc_train: 0.6527 loss_val: 1.3881 acc_val: 0.5756 time: 0.0204s\n",
      "valid current auc-roc score: 0.862362, current macro_F score: 0.538193\n",
      "Epoch: 00013 loss_train: 1.1551 acc_train: 0.6613 loss_val: 1.3986 acc_val: 0.6125 time: 0.0182s\n",
      "valid current auc-roc score: 0.871169, current macro_F score: 0.563344\n",
      "Epoch: 00014 loss_train: 1.2153 acc_train: 0.6293 loss_val: 1.8260 acc_val: 0.5351 time: 0.0178s\n",
      "valid current auc-roc score: 0.849794, current macro_F score: 0.478880\n",
      "Epoch: 00015 loss_train: 1.0420 acc_train: 0.6872 loss_val: 1.4061 acc_val: 0.5959 time: 0.0183s\n",
      "valid current auc-roc score: 0.869381, current macro_F score: 0.564712\n",
      "Epoch: 00016 loss_train: 0.9864 acc_train: 0.6866 loss_val: 1.8328 acc_val: 0.5738 time: 0.0184s\n",
      "valid current auc-roc score: 0.860574, current macro_F score: 0.544324\n",
      "Epoch: 00017 loss_train: 1.0813 acc_train: 0.6410 loss_val: 1.6922 acc_val: 0.5480 time: 0.0184s\n",
      "valid current auc-roc score: 0.860149, current macro_F score: 0.505036\n",
      "Epoch: 00018 loss_train: 1.5468 acc_train: 0.6293 loss_val: 1.9478 acc_val: 0.5517 time: 0.0182s\n",
      "valid current auc-roc score: 0.854347, current macro_F score: 0.512112\n",
      "Epoch: 00019 loss_train: 1.0175 acc_train: 0.6718 loss_val: 1.6158 acc_val: 0.5664 time: 0.0181s\n",
      "valid current auc-roc score: 0.860034, current macro_F score: 0.521361\n",
      "Epoch: 00020 loss_train: 1.2933 acc_train: 0.6570 loss_val: 2.1905 acc_val: 0.5830 time: 0.0210s\n",
      "valid current auc-roc score: 0.843501, current macro_F score: 0.507421\n",
      "Epoch: 00021 loss_train: 1.0216 acc_train: 0.6792 loss_val: 1.4968 acc_val: 0.5996 time: 0.0211s\n",
      "valid current auc-roc score: 0.883679, current macro_F score: 0.549311\n",
      "Test set results: loss= 1.5926 accuracy= 0.6402\n",
      "test current auc-roc score: 0.903287, current macro_F score: 0.568395\n",
      "Epoch: 00022 loss_train: 1.0665 acc_train: 0.6595 loss_val: 1.8829 acc_val: 0.5756 time: 0.0185s\n",
      "valid current auc-roc score: 0.873920, current macro_F score: 0.530403\n",
      "Epoch: 00023 loss_train: 1.1648 acc_train: 0.6466 loss_val: 1.4980 acc_val: 0.5849 time: 0.0174s\n",
      "valid current auc-roc score: 0.869659, current macro_F score: 0.494981\n",
      "Epoch: 00024 loss_train: 1.1982 acc_train: 0.6921 loss_val: 1.6691 acc_val: 0.6033 time: 0.0193s\n",
      "valid current auc-roc score: 0.884432, current macro_F score: 0.559977\n",
      "Epoch: 00025 loss_train: 1.1186 acc_train: 0.6595 loss_val: 1.5245 acc_val: 0.5867 time: 0.0187s\n",
      "valid current auc-roc score: 0.869937, current macro_F score: 0.524705\n",
      "Epoch: 00026 loss_train: 2.4654 acc_train: 0.6767 loss_val: 1.3646 acc_val: 0.6255 time: 0.0186s\n",
      "valid current auc-roc score: 0.880579, current macro_F score: 0.547236\n",
      "Epoch: 00027 loss_train: 1.0901 acc_train: 0.7198 loss_val: 1.4729 acc_val: 0.6494 time: 0.0182s\n",
      "valid current auc-roc score: 0.883548, current macro_F score: 0.572455\n",
      "Epoch: 00028 loss_train: 1.0193 acc_train: 0.6687 loss_val: 1.3291 acc_val: 0.5812 time: 0.0185s\n",
      "valid current auc-roc score: 0.879083, current macro_F score: 0.526846\n",
      "Epoch: 00029 loss_train: 1.0242 acc_train: 0.7100 loss_val: 1.3990 acc_val: 0.6347 time: 0.0177s\n",
      "valid current auc-roc score: 0.876590, current macro_F score: 0.593739\n",
      "Epoch: 00030 loss_train: 0.9713 acc_train: 0.7149 loss_val: 1.3088 acc_val: 0.6421 time: 0.0205s\n",
      "valid current auc-roc score: 0.894831, current macro_F score: 0.612054\n",
      "Epoch: 00031 loss_train: 0.9954 acc_train: 0.7180 loss_val: 1.3007 acc_val: 0.6107 time: 0.0185s\n",
      "valid current auc-roc score: 0.883896, current macro_F score: 0.574224\n",
      "Test set results: loss= 16.1704 accuracy= 0.5904\n",
      "test current auc-roc score: 0.852076, current macro_F score: 0.523850\n",
      "Epoch: 00032 loss_train: 3.7408 acc_train: 0.6490 loss_val: 3.3371 acc_val: 0.5720 time: 0.0208s\n",
      "valid current auc-roc score: 0.838839, current macro_F score: 0.533267\n",
      "Epoch: 00033 loss_train: 1.1354 acc_train: 0.6656 loss_val: 1.2133 acc_val: 0.6199 time: 0.0212s\n",
      "valid current auc-roc score: 0.880110, current macro_F score: 0.589279\n",
      "Epoch: 00034 loss_train: 1.1544 acc_train: 0.6262 loss_val: 1.2966 acc_val: 0.5554 time: 0.0207s\n",
      "valid current auc-roc score: 0.872486, current macro_F score: 0.533424\n",
      "Epoch: 00035 loss_train: 1.2819 acc_train: 0.5936 loss_val: 1.6820 acc_val: 0.5258 time: 0.0207s\n",
      "valid current auc-roc score: 0.825878, current macro_F score: 0.508591\n",
      "Epoch: 00036 loss_train: 1.2543 acc_train: 0.6059 loss_val: 1.5518 acc_val: 0.5424 time: 0.0209s\n",
      "valid current auc-roc score: 0.837880, current macro_F score: 0.514889\n",
      "Epoch: 00037 loss_train: 1.2795 acc_train: 0.6305 loss_val: 1.5796 acc_val: 0.5498 time: 0.0185s\n",
      "valid current auc-roc score: 0.856751, current macro_F score: 0.524486\n",
      "Epoch: 00038 loss_train: 1.8131 acc_train: 0.6527 loss_val: 2.2559 acc_val: 0.5683 time: 0.0208s\n",
      "valid current auc-roc score: 0.857661, current macro_F score: 0.536166\n",
      "Epoch: 00039 loss_train: 1.2278 acc_train: 0.6675 loss_val: 1.4272 acc_val: 0.5996 time: 0.0184s\n",
      "valid current auc-roc score: 0.874009, current macro_F score: 0.565223\n",
      "Epoch: 00040 loss_train: 1.1988 acc_train: 0.6564 loss_val: 1.6140 acc_val: 0.5904 time: 0.0188s\n",
      "valid current auc-roc score: 0.864839, current macro_F score: 0.524747\n",
      "Epoch: 00041 loss_train: 1.2293 acc_train: 0.6767 loss_val: 1.5786 acc_val: 0.6199 time: 0.0193s\n",
      "valid current auc-roc score: 0.865537, current macro_F score: 0.610473\n",
      "Test set results: loss= 1.3695 accuracy= 0.6624\n",
      "test current auc-roc score: 0.896810, current macro_F score: 0.622876\n",
      "Epoch: 00042 loss_train: 1.2795 acc_train: 0.6884 loss_val: 1.6844 acc_val: 0.5959 time: 0.0205s\n",
      "valid current auc-roc score: 0.855899, current macro_F score: 0.550591\n",
      "Epoch: 00043 loss_train: 1.1776 acc_train: 0.6872 loss_val: 1.7754 acc_val: 0.5996 time: 0.0199s\n",
      "valid current auc-roc score: 0.879971, current macro_F score: 0.557415\n",
      "Epoch: 00044 loss_train: 1.1849 acc_train: 0.6712 loss_val: 1.5976 acc_val: 0.5886 time: 0.0178s\n",
      "valid current auc-roc score: 0.853746, current macro_F score: 0.558026\n",
      "Epoch: 00045 loss_train: 1.1304 acc_train: 0.6730 loss_val: 1.3398 acc_val: 0.6199 time: 0.0185s\n",
      "valid current auc-roc score: 0.875975, current macro_F score: 0.586172\n",
      "Epoch: 00046 loss_train: 1.3866 acc_train: 0.6743 loss_val: 1.9900 acc_val: 0.5959 time: 0.0183s\n",
      "valid current auc-roc score: 0.858751, current macro_F score: 0.571529\n",
      "Epoch: 00047 loss_train: 1.1195 acc_train: 0.7050 loss_val: 1.5684 acc_val: 0.5978 time: 0.0198s\n",
      "valid current auc-roc score: 0.881849, current macro_F score: 0.576355\n",
      "Epoch: 00048 loss_train: 1.0078 acc_train: 0.7192 loss_val: 1.7054 acc_val: 0.6384 time: 0.0201s\n",
      "valid current auc-roc score: 0.885221, current macro_F score: 0.612573\n",
      "Epoch: 00049 loss_train: 1.1010 acc_train: 0.7284 loss_val: 1.5976 acc_val: 0.6476 time: 0.0191s\n",
      "valid current auc-roc score: 0.893074, current macro_F score: 0.615905\n",
      "Epoch: 00050 loss_train: 1.0681 acc_train: 0.7149 loss_val: 1.7277 acc_val: 0.6292 time: 0.0183s\n",
      "valid current auc-roc score: 0.881483, current macro_F score: 0.576217\n",
      "Epoch: 00051 loss_train: 0.9606 acc_train: 0.7278 loss_val: 1.9452 acc_val: 0.6218 time: 0.0204s\n",
      "valid current auc-roc score: 0.879400, current macro_F score: 0.562253\n",
      "Test set results: loss= 1.5778 accuracy= 0.7066\n",
      "test current auc-roc score: 0.909897, current macro_F score: 0.661238\n",
      "Epoch: 00052 loss_train: 1.0861 acc_train: 0.7007 loss_val: 1.5679 acc_val: 0.6273 time: 0.0203s\n",
      "valid current auc-roc score: 0.875988, current macro_F score: 0.562085\n",
      "Epoch: 00053 loss_train: 1.0327 acc_train: 0.7198 loss_val: 1.8855 acc_val: 0.6347 time: 0.0205s\n",
      "valid current auc-roc score: 0.884670, current macro_F score: 0.576364\n",
      "Epoch: 00054 loss_train: 1.1313 acc_train: 0.7137 loss_val: 1.8909 acc_val: 0.6605 time: 0.0219s\n",
      "valid current auc-roc score: 0.894694, current macro_F score: 0.620169\n",
      "Epoch: 00055 loss_train: 1.0043 acc_train: 0.7198 loss_val: 1.4564 acc_val: 0.6458 time: 0.0211s\n",
      "valid current auc-roc score: 0.879190, current macro_F score: 0.583620\n",
      "Epoch: 00056 loss_train: 1.0319 acc_train: 0.6995 loss_val: 1.4479 acc_val: 0.6587 time: 0.0209s\n",
      "valid current auc-roc score: 0.881250, current macro_F score: 0.620755\n",
      "Epoch: 00057 loss_train: 0.9534 acc_train: 0.7334 loss_val: 1.1807 acc_val: 0.6365 time: 0.0186s\n",
      "valid current auc-roc score: 0.891843, current macro_F score: 0.586316\n",
      "Epoch: 00058 loss_train: 0.9729 acc_train: 0.7087 loss_val: 1.5034 acc_val: 0.6162 time: 0.0177s\n",
      "valid current auc-roc score: 0.870603, current macro_F score: 0.574682\n",
      "Epoch: 00059 loss_train: 1.0299 acc_train: 0.6977 loss_val: 2.1085 acc_val: 0.6089 time: 0.0174s\n",
      "valid current auc-roc score: 0.875601, current macro_F score: 0.535505\n",
      "Epoch: 00060 loss_train: 1.0560 acc_train: 0.6823 loss_val: 1.4637 acc_val: 0.6089 time: 0.0187s\n",
      "valid current auc-roc score: 0.879156, current macro_F score: 0.559662\n",
      "Epoch: 00061 loss_train: 0.9596 acc_train: 0.7155 loss_val: 1.4332 acc_val: 0.6236 time: 0.0204s\n",
      "valid current auc-roc score: 0.883266, current macro_F score: 0.576240\n",
      "Test set results: loss= 1.4307 accuracy= 0.6753\n",
      "test current auc-roc score: 0.908710, current macro_F score: 0.641167\n",
      "Epoch: 00062 loss_train: 0.9689 acc_train: 0.6995 loss_val: 1.8128 acc_val: 0.6162 time: 0.0192s\n",
      "valid current auc-roc score: 0.877584, current macro_F score: 0.558912\n",
      "Epoch: 00063 loss_train: 0.8868 acc_train: 0.6970 loss_val: 1.3699 acc_val: 0.5830 time: 0.0188s\n",
      "valid current auc-roc score: 0.881468, current macro_F score: 0.531033\n",
      "Epoch: 00064 loss_train: 0.9726 acc_train: 0.7075 loss_val: 1.2120 acc_val: 0.6328 time: 0.0190s\n",
      "valid current auc-roc score: 0.888131, current macro_F score: 0.601525\n",
      "Epoch: 00065 loss_train: 0.8310 acc_train: 0.7217 loss_val: 1.3853 acc_val: 0.6365 time: 0.0199s\n",
      "valid current auc-roc score: 0.883999, current macro_F score: 0.583435\n",
      "Epoch: 00066 loss_train: 0.8660 acc_train: 0.7278 loss_val: 1.4898 acc_val: 0.6292 time: 0.0186s\n",
      "valid current auc-roc score: 0.880649, current macro_F score: 0.594119\n",
      "Epoch: 00067 loss_train: 0.9095 acc_train: 0.7180 loss_val: 1.3932 acc_val: 0.6089 time: 0.0202s\n",
      "valid current auc-roc score: 0.884751, current macro_F score: 0.555875\n",
      "Epoch: 00068 loss_train: 0.9759 acc_train: 0.6829 loss_val: 1.6918 acc_val: 0.5609 time: 0.0207s\n",
      "valid current auc-roc score: 0.863870, current macro_F score: 0.507244\n",
      "Epoch: 00069 loss_train: 0.8787 acc_train: 0.6989 loss_val: 1.7646 acc_val: 0.6033 time: 0.0188s\n",
      "valid current auc-roc score: 0.879057, current macro_F score: 0.584536\n",
      "Epoch: 00070 loss_train: 0.8582 acc_train: 0.7358 loss_val: 1.4534 acc_val: 0.6199 time: 0.0184s\n",
      "valid current auc-roc score: 0.884135, current macro_F score: 0.575715\n",
      "Epoch: 00071 loss_train: 0.9288 acc_train: 0.7118 loss_val: 1.5550 acc_val: 0.6125 time: 0.0185s\n",
      "valid current auc-roc score: 0.890326, current macro_F score: 0.593539\n",
      "Test set results: loss= 1.5521 accuracy= 0.6753\n",
      "test current auc-roc score: 0.910265, current macro_F score: 0.643572\n",
      "Epoch: 00072 loss_train: 0.8898 acc_train: 0.7241 loss_val: 1.6322 acc_val: 0.5941 time: 0.0215s\n",
      "valid current auc-roc score: 0.876817, current macro_F score: 0.555390\n",
      "Epoch: 00073 loss_train: 0.8165 acc_train: 0.7340 loss_val: 1.4674 acc_val: 0.6347 time: 0.0209s\n",
      "valid current auc-roc score: 0.887023, current macro_F score: 0.599456\n",
      "Epoch: 00074 loss_train: 0.7952 acc_train: 0.7494 loss_val: 1.5095 acc_val: 0.6439 time: 0.0211s\n",
      "valid current auc-roc score: 0.887304, current macro_F score: 0.613909\n",
      "Epoch: 00075 loss_train: 1.4750 acc_train: 0.7087 loss_val: 2.0714 acc_val: 0.5867 time: 0.0207s\n",
      "valid current auc-roc score: 0.867493, current macro_F score: 0.548155\n",
      "Epoch: 00076 loss_train: 0.8277 acc_train: 0.7568 loss_val: 1.8427 acc_val: 0.6624 time: 0.0207s\n",
      "valid current auc-roc score: 0.898151, current macro_F score: 0.638050\n",
      "Epoch: 00077 loss_train: 0.8223 acc_train: 0.7451 loss_val: 1.9396 acc_val: 0.6494 time: 0.0207s\n",
      "valid current auc-roc score: 0.889092, current macro_F score: 0.616504\n",
      "Epoch: 00078 loss_train: 0.8196 acc_train: 0.7463 loss_val: 1.7650 acc_val: 0.6402 time: 0.0210s\n",
      "valid current auc-roc score: 0.888925, current macro_F score: 0.614825\n",
      "Epoch: 00079 loss_train: 0.8033 acc_train: 0.7592 loss_val: 1.6651 acc_val: 0.6531 time: 0.0202s\n",
      "valid current auc-roc score: 0.892296, current macro_F score: 0.622749\n",
      "Epoch: 00080 loss_train: 0.8113 acc_train: 0.7685 loss_val: 1.6252 acc_val: 0.6458 time: 0.0202s\n",
      "valid current auc-roc score: 0.899245, current macro_F score: 0.606608\n",
      "Epoch: 00081 loss_train: 0.8575 acc_train: 0.7777 loss_val: 1.2792 acc_val: 0.6900 time: 0.0205s\n",
      "valid current auc-roc score: 0.902447, current macro_F score: 0.664277\n",
      "Test set results: loss= 1.7882 accuracy= 0.7103\n",
      "test current auc-roc score: 0.915129, current macro_F score: 0.667733\n",
      "Epoch: 00082 loss_train: 0.8634 acc_train: 0.7734 loss_val: 1.4986 acc_val: 0.7030 time: 0.0184s\n",
      "valid current auc-roc score: 0.899730, current macro_F score: 0.650399\n",
      "Epoch: 00083 loss_train: 0.7859 acc_train: 0.7734 loss_val: 1.2107 acc_val: 0.6863 time: 0.0204s\n",
      "valid current auc-roc score: 0.912796, current macro_F score: 0.662691\n",
      "Epoch: 00084 loss_train: 0.9389 acc_train: 0.7549 loss_val: 1.6008 acc_val: 0.6255 time: 0.0184s\n",
      "valid current auc-roc score: 0.892320, current macro_F score: 0.597586\n",
      "Epoch: 00085 loss_train: 0.7605 acc_train: 0.7703 loss_val: 1.3316 acc_val: 0.6642 time: 0.0191s\n",
      "valid current auc-roc score: 0.900331, current macro_F score: 0.645677\n",
      "Epoch: 00086 loss_train: 0.7035 acc_train: 0.7728 loss_val: 1.1494 acc_val: 0.6697 time: 0.0182s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_accs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         acc_train, acc_val, loss_train, loss_val \u001b[39m=\u001b[39m train(epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         acc_trains\u001b[39m.\u001b[39mappend(acc_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         acc_vals\u001b[39m.\u001b[39mappend(acc_val)\n",
      "\u001b[1;32m/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# print_class_acc(out_v, gt_v)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{:05d}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mloss_train: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(loss_train\u001b[39m.\u001b[39mitem()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39macc_train: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(acc_train\u001b[39m.\u001b[39mitem()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mloss_val: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(loss_val\u001b[39m.\u001b[39mitem()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39macc_val: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(acc_val\u001b[39m.\u001b[39mitem()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mtime: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m print_class_acc(output[val_idx], labels[val_idx])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahdi/Desktop/Projects/619-project/ENSF-619-Final-Project/main.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mreturn\u001b[39;00m acc_train\u001b[39m.\u001b[39mitem(), acc_val\u001b[39m.\u001b[39mitem(), loss_train\u001b[39m.\u001b[39mitem(), loss_val\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/Projects/619-project/ENSF-619-Final-Project/utils/evaluation.py:30\u001b[0m, in \u001b[0;36mprint_class_acc\u001b[0;34m(output, labels, pre)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m#ipdb.set_trace()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m labels\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     auc_score \u001b[39m=\u001b[39m roc_auc_score(labels\u001b[39m.\u001b[39;49mdetach(), F\u001b[39m.\u001b[39;49msoftmax(output, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mdetach(), average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmacro\u001b[39;49m\u001b[39m'\u001b[39;49m, multi_class\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39movr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     auc_score \u001b[39m=\u001b[39m roc_auc_score(labels\u001b[39m.\u001b[39mdetach(), F\u001b[39m.\u001b[39msoftmax(output, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach(), average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:550\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    548\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    549\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 550\u001b[0m y_score \u001b[39m=\u001b[39m check_array(y_score, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    553\u001b[0m     y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    554\u001b[0m ):\n\u001b[1;32m    555\u001b[0m     \u001b[39m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m max_fpr \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/sklearn/utils/validation.py:108\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# false positives from overflow in sum method. The sum is also calculated\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# safely to reduce dtype induced overflows.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m is_float \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[39mif\u001b[39;00m is_float \u001b[39mand\u001b[39;00m (np\u001b[39m.\u001b[39misfinite(_safe_accumulator_op(np\u001b[39m.\u001b[39;49msum, X))):\n\u001b[1;32m    109\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39melif\u001b[39;00m is_float:\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/sklearn/utils/extmath.py:897\u001b[0m, in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m     result \u001b[39m=\u001b[39m op(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    896\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 897\u001b[0m     result \u001b[39m=\u001b[39m op(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    898\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "loss_test = []\n",
    "test_acc = []\n",
    "loss_tests = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)\n",
    "        if epoch % 10 == 0:\n",
    "                loss_test, test_acc = test(epoch)\n",
    "                loss_tests.append(loss_test)\n",
    "                test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save / Load Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dic(path, dictionary):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "def load_dic(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        dic = pickle.load(f)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"acc_trains\": acc_trains,\n",
    "    \"acc_vals\": acc_vals,\n",
    "    \"loss_trains\": loss_trains,\n",
    "    \"loss_vals\": loss_vals,\n",
    "    \"loss_test\": loss_test,\n",
    "    \"test_acc\": test_acc,\n",
    "    \"loss_tests\": loss_tests,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "save_dic('result.pkl', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained 2 layer classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_n.state_dict()\n",
    "    saved_content['classifier'] = classifier_n.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Normal_opt{}_m{}_act{}_init{}_ir{}_res{}_l{}_h{}.pth'.format(args.dataset, \n",
    "    args.optimizer_alg, \n",
    "    args.momentum, \n",
    "    args.activation_func, \n",
    "    args.initalization,\n",
    "    args.imbalance_ratio,\n",
    "    args.res_connection,\n",
    "    len(n_hidden),\n",
    "    n_hidden[-1]))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "encoder_s = GCN_Encoder_s(nfeat = features.shape[1],\n",
    "        nhid = args.n_hidden,\n",
    "        nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "classifier_s = GCN_Classifier_s(nembed=args.n_hidden, \n",
    "        nhid = args.n_hidden, \n",
    "        nclass = labels.max().item() + 1, \n",
    "        dropout = args.dropout, device = device, init = args.initalization)\n",
    "decoder_s = Decoder_s(nembed = args.n_hidden,\n",
    "        dropout = args.dropout, init = args.initalization)\n",
    "# optimizer_en = optim.Adam(encoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_cls = optim.Adam(classifier_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "#                        lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer_en = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_cls = optim.SGD(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "        optimizer_de = optim.SGD(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer_en = optim.RMSprop(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.RMSprop(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.RMSprop(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer_en = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_cls = optim.Adam(classifier_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "        optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder_s.train()\n",
    "    classifier_s.train()\n",
    "    decoder_s.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon' or args.setting == 'newG_cls':\n",
    "        ori_num = labels.shape[0]\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.recon_upsample(embed, labels, train_idx, adj = adj_mtx.detach().to_dense(), portion = args.up_scale, im_class_num = args.im_class_num)\n",
    "        generated_G = decoder_s(embed)\n",
    "\n",
    "        loss_rec = utils.adj_mse_loss(generated_G[: ori_num, :][:, : ori_num], adj_mtx.detach().to_dense())\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = copy.deepcopy(generated_G.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "\n",
    "            #ipdb.set_trace()\n",
    "            edge_ac = adj_new[: ori_num, : ori_num].eq(adj_mtx.to_dense()).double().sum()/(ori_num**2)\n",
    "        else:\n",
    "            adj_new = generated_G\n",
    "            edge_ac = F.l1_loss(adj_new[: ori_num, : ori_num], adj_mtx.to_dense(), reduction = 'mean')\n",
    "\n",
    "\n",
    "        #calculate generation information\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"after filtering, edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new[:ori_num, :][:, :ori_num] = adj_mtx.detach().to_dense()\n",
    "        #adj_new = adj_new.to_sparse()\n",
    "        #ipdb.set_trace()\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "        if args.setting == 'newG_cls':\n",
    "            idx_train_new = train_idx\n",
    "\n",
    "    elif args.setting == 'embed_up':\n",
    "        #perform SMOTE in embedding space\n",
    "        embed, labels_new, idx_train_new = utils.recon_upsample(embed, labels, train_idx, portion=args.up_scale, im_class_num = args.im_class_num)\n",
    "        adj_new = adj_mtx\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = train_idx\n",
    "        adj_new = adj_mtx\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    output = classifier_s(embed, adj_new)\n",
    "\n",
    "\n",
    "\n",
    "    if args.setting == 'reweight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        weight[-args.im_class_num:] = 1 + args.up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1), weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1))\n",
    "\n",
    "    acc_train = accuracy(output[train_idx], labels_new[train_idx].reshape(-1))\n",
    "    if args.setting == 'recon_newG':\n",
    "        loss = loss_train + loss_rec * args.rec_weight\n",
    "    elif args.setting == 'recon':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "\n",
    "    loss.backward()\n",
    "    if args.setting == 'newG_cls':\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_de.zero_grad()\n",
    "    else:\n",
    "        optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon':\n",
    "        optimizer_de.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "    acc_val = accuracy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder_s.eval()\n",
    "    classifier_s.eval()\n",
    "#     outputs = encoder(features, adj_mtx)\n",
    "    embed = encoder_s(features, adj_mtx)\n",
    "    outputs = classifier_s(embed, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')\n",
    "    return loss_test.item(), acc_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSOMTE's training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "loss_test = []\n",
    "test_acc = []\n",
    "loss_tests = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch, features, labels)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)\n",
    "        if epoch % 10 == 0:\n",
    "                test(epoch)\n",
    "                loss_test, test_acc = test(epoch)\n",
    "                loss_tests.append(loss_test)\n",
    "                test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained GraphSMOTE's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder_s.state_dict()\n",
    "    saved_content['classifier'] = classifier_s.state_dict()\n",
    "    saved_content['decoder'] = decoder_s.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/GraphSMOTE_opt{}_m{}_act{}_init{}_ir{}_res{}_l{}_h{}.pth'.format(args.dataset, \n",
    "        args.optimizer_alg, \n",
    "        args.momentum, \n",
    "        args.activation_func, \n",
    "        args.initalization,\n",
    "        args.imbalance_ratio,\n",
    "        args.res_connection,\n",
    "        len(n_hidden),\n",
    "        n_hidden[-1]))\n",
    "        return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "from utils.reweight import next, next2\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_w(nfeat = n_features, \n",
    "        nembed = n_hidden[-1], \n",
    "        nhid = n_hidden[-1], \n",
    "        nclass = int(labels.max().item()) + 1, \n",
    "        dropout = args.dropout, \n",
    "        device = device, init = args.initalization)\n",
    "# optimizer = optim.Adam(encoder.parameters(),\n",
    "#                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "if args.optimizer_alg == \"ADAM\":\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "elif args.optimizer_alg == \"Momentum\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, momentum = args.momentum)\n",
    "elif args.optimizer_alg == \"RMSProp\":\n",
    "        optimizer = optim.SGD(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "else:\n",
    "        optimizer = optim.Adam(encoder_n.parameters(),\n",
    "                        lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "def train(epoch, features, labels):\n",
    "        encoder.train()\n",
    "        # classifier.train()\n",
    "        t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        with higher.innerloop_ctx(encoder, optimizer) as (meta_model, meta_opt):\n",
    "                meta_train_outputs = meta_model(features, adj_mtx)\n",
    "                # criterion.reduction = 'none'\n",
    "                # new_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_train_outputs[train_idx], labels[train_idx])\n",
    "                # meta_train_loss = criterion(meta_train_outputs[train_idx], new_labels[train_idx])\n",
    "                eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=device)\n",
    "                meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "                meta_opt.step(meta_train_loss)\n",
    "                sampled_val_idx, new_adj_mtx, new_features, new_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                # meta_inputs, meta_labels = next(args, features, labels, val_idx, adj_mtx)\n",
    "                meta_val_idx, meta_adj_mtx, meta_features, meta_labels = sampled_val_idx.to(device=device, non_blocking=True), \\\n",
    "                        new_adj_mtx.to(device=device, non_blocking=True), \\\n",
    "                        new_features.to(device=device, non_blocking=True), \\\n",
    "                        new_labels.to(device=device, non_blocking=True)\n",
    "                meta_val_outputs = meta_model(meta_features, meta_adj_mtx.double())\n",
    "                # criterion.reduction = 'mean'\n",
    "                # new_meta_labels = F.one_hot(meta_labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "                meta_train_loss = F.cross_entropy(meta_val_outputs, meta_labels)\n",
    "                # meta_val_loss = criterion(meta_val_outputs, new_meta_labels)\n",
    "                eps_grads = torch.autograd.grad(meta_train_loss, eps)[0].detach()\n",
    "        w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "        l1_norm = torch.sum(w_tilde)\n",
    "        if l1_norm != 0:\n",
    "                w = w_tilde / l1_norm\n",
    "        else:\n",
    "                w = w_tilde\n",
    "        outputs = encoder(features, adj_mtx.double())\n",
    "        # criterion.reduction = 'none'\n",
    "        # new_main_labels = F.one_hot(labels, num_classes=int(labels.max().item()) + 1).double()\n",
    "        loss = F.cross_entropy(outputs[train_idx], labels[train_idx])\n",
    "        # loss = criterion(outputs[train_idx], new_main_labels[train_idx])\n",
    "        loss = torch.sum(w * loss)\n",
    "\n",
    "        loss_train = F.cross_entropy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "        acc_train = accuracy(outputs[train_idx], labels[train_idx].reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = F.cross_entropy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "        acc_val = accuracy(outputs[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "        print('Epoch: {:05d}'.format(epoch + 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n",
    "\n",
    "        # keep track of epoch loss/accuracy\n",
    "        # train_loss += loss.item() * outputs.shape[0]\n",
    "\n",
    "                # out = output[train_idx]\n",
    "                # gt = labels[train_idx].reshape(-1)\n",
    "                # if args.setting == 'reweight':\n",
    "                #         weight = \"STH\"\n",
    "                #         loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "                # else:\n",
    "                #         loss_train = F.cross_entropy(out, gt)\n",
    "                # acc_train = accuracy(out, gt)\n",
    "                # loss_train.backward()\n",
    "                # optimizer_en.step()\n",
    "                # optimizer_cls.step()\n",
    "                # gt_v = labels[test_idx].reshape(-1)\n",
    "                # out_v = output[test_idx]\n",
    "                # loss_val = F.cross_entropy(out_v, gt_v)\n",
    "                # acc_val = accuracy(out_v, gt_v)\n",
    "                # # print_class_acc(out_v, gt_v)\n",
    "                # print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "                # 'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                # 'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                # 'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                # 'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                # 'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "        # return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    outputs = encoder(features, adj_mtx)\n",
    "    loss_test = F.cross_entropy(outputs[test_idx], labels[test_idx])\n",
    "    acc_test = accuracy(outputs[test_idx], labels[test_idx])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    print_class_acc(outputs[test_idx], labels[test_idx], pre='test')\n",
    "    return loss_test.item(), acc_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight model's train and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "loss_test = []\n",
    "test_acc = []\n",
    "loss_tests = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch, features, labels)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)\n",
    "        if epoch % 10 == 0:\n",
    "                loss_test, test_acc = test(epoch)\n",
    "                loss_tests.append(loss_test)\n",
    "                test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the reweight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    saved_content = {}\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    torch.save(saved_content, 'checkpoint/{}/Reweight_opt{}_m{}_act{}_init{}_ir{}_res{}_l{}_h{}.pth'.format(args.dataset, \n",
    "        args.optimizer_alg, \n",
    "        args.momentum, \n",
    "        args.activation_func, \n",
    "        args.initalization,\n",
    "        args.imbalance_ratio,\n",
    "        args.res_connection,\n",
    "        len(n_hidden),\n",
    "        n_hidden[-1]))\n",
    "    return\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24532637d9bf4dd13a0e19ab2bab35781a2363c69a0b4abc73099a34c5034bdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
