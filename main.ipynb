{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.GraphConvolution import GCN_Encoder_s, GCN_Classifier_s, Decoder_s\n",
    "from utils.GraphConvolution import GraphConvolution, GCN_Encoder3\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_W = 0.01\n",
    "    dropout = 0.5\n",
    "    dropout_W = 0.5\n",
    "    gamma = 1\n",
    "    no_cuda = False\n",
    "    train_ratio=0.6\n",
    "    test_ratio=0.2\n",
    "    n_classes = 2\n",
    "    seed = 12345\n",
    "    torch.manual_seed(seed)\n",
    "    # dataset = \"cora\"\n",
    "    # dataset = \"haberman\"\n",
    "    dataset = \"diabetes\"\n",
    "    order = 4\n",
    "    n_features = 0\n",
    "    w_val_size = 10\n",
    "    imbalance_ratio = None\n",
    "    n_hidden = 64\n",
    "    setting = None\n",
    "    im_class_num = 1\n",
    "    setting = \"upsampling\"\n",
    "    opt_new_G = False\n",
    "    up_scale = 1\n",
    "    im_ratio = 0.5\n",
    "    val_size = 10\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import data_loader_diabetes, data_loader_haberman, data_loader_cora\n",
    "\n",
    "cora_adj_mtx, cora_labels_df, cora_features_df, \\\n",
    "        cora_train_idx, cora_val_idx, cora_test_idx, cora_n_features = data_loader_cora(args)\n",
    "        \n",
    "diabetes_adj_mtx, diabetes_labels_df, diabetes_features_df, \\\n",
    "        diabetes_train_idx, diabetes_val_idx, diabetes_test_idx, diabetes_n_features = data_loader_diabetes(args)\n",
    "\n",
    "haberman_adj_mtx, haberman_labels_df, haberman_features_df, \\\n",
    "        haberman_train_idx, haberman_val_idx, haberman_test_idx, haberman_n_features = data_loader_haberman(args)\n",
    "\n",
    "if args.dataset == \"diabetes\":\n",
    "    adj_mtx = diabetes_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = diabetes_n_features\n",
    "    features = diabetes_features_df\n",
    "    labels = diabetes_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = diabetes_train_idx\n",
    "    val_idx = diabetes_val_idx\n",
    "    test_idx = diabetes_test_idx\n",
    "elif args.dataset == \"cora\":\n",
    "    adj_mtx = cora_adj_mtx\n",
    "    n_hidden = [64, 64, 64]\n",
    "    n_features = cora_n_features\n",
    "    features = cora_features_df\n",
    "    labels = cora_labels_df\n",
    "    # train_X = diabetes_train_X_df\n",
    "    # train_Y = diabetes_train_Y_df\n",
    "    # val_X = diabetes_val_X_df\n",
    "    # val_Y = diabetes_val_Y_df\n",
    "    # test_X = diabetes_test_X_df\n",
    "    # test_Y = diabetes_test_Y_df\n",
    "    train_idx = cora_train_idx\n",
    "    val_idx = cora_val_idx\n",
    "    test_idx = cora_test_idx\n",
    "elif args.dataset == \"haberman\":\n",
    "    adj_mtx = haberman_adj_mtx\n",
    "    n_hidden = [64]\n",
    "    n_features = haberman_n_features\n",
    "    features = haberman_features_df\n",
    "    labels = haberman_labels_df\n",
    "    # train_X = haberman_train_X_df\n",
    "    # train_Y = haberman_train_Y_df\n",
    "    # val_X = haberman_val_X_df\n",
    "    # val_Y = haberman_val_Y_df\n",
    "    # test_X = haberman_test_X_df\n",
    "    # test_Y = haberman_test_Y_df\n",
    "    train_idx = haberman_train_idx\n",
    "    val_idx = haberman_val_idx\n",
    "    test_idx = haberman_test_idx\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(pd.DataFrame(labels[train_idx])[0].unique()) == len(pd.DataFrame(labels[val_idx])[0].unique()) == len(pd.DataFrame(labels[test_idx])[0].unique()), \\\n",
    "#     \"There are some classes missing in one the 3 partitiones of the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if False else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe to Tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.from_numpy(np.concatenate((train_X, val_X, test_X), axis=0)).to(device)\n",
    "# labels = torch.from_numpy(np.int64(np.concatenate((train_Y, val_Y, test_Y), axis=0))).to(device)\n",
    "train_idx = torch.from_numpy(np.array(train_idx, dtype = np.int64)).to(device)\n",
    "val_idx = torch.from_numpy(np.array(val_idx, dtype = np.int64)).to(device)\n",
    "test_idx = torch.from_numpy(np.array(test_idx, dtype = np.int64)).to(device)\n",
    "features = torch.from_numpy(np.array(features, dtype = np.float64)).to(device)\n",
    "labels = torch.from_numpy(np.array(labels, dtype = np.int64)).to(device)\n",
    "try:\n",
    "    adj_mtx = torch.from_numpy(np.array(adj_mtx, dtype = np.float64)).to(device)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_s(nfeat = n_features, nhid = n_hidden[-1], nembed = n_hidden[-1], dropout = args.dropout)\n",
    "classifier = GCN_Classifier_s(nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "optimizer_en = optim.Adam(encoder.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "optimizer_cls = optim.Adam(classifier.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "def train(epoch):\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        embed = encoder(features, adj_mtx)\n",
    "        output = classifier(embed, adj_mtx)\n",
    "        out = output[train_idx]\n",
    "        gt = labels[train_idx].reshape(-1)\n",
    "        if args.setting == 'reweight':\n",
    "                weight = \"STH\"\n",
    "                loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "        else:\n",
    "                loss_train = F.cross_entropy(out, gt)\n",
    "        acc_train = accuracy(out, gt)\n",
    "        loss_train.backward()\n",
    "        optimizer_en.step()\n",
    "        optimizer_cls.step()\n",
    "        gt_v = labels[test_idx].reshape(-1)\n",
    "        out_v = output[test_idx]\n",
    "        loss_val = F.cross_entropy(out_v, gt_v)\n",
    "        acc_val = accuracy(out_v, gt_v)\n",
    "        # print_class_acc(out_v, gt_v)\n",
    "        print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "        return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02110457420349121,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09864b3b62548b88c4a2c2b46b42a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 0.5879 acc_train: 0.7065 loss_val: 0.6244 acc_val: 0.7078 time: 0.0104s\n",
      "Epoch: 00002 loss_train: 0.5806 acc_train: 0.7087 loss_val: 0.5971 acc_val: 0.6883 time: 0.0080s\n",
      "Epoch: 00003 loss_train: 0.5855 acc_train: 0.7043 loss_val: 0.6017 acc_val: 0.6883 time: 0.0085s\n",
      "Epoch: 00004 loss_train: 0.5808 acc_train: 0.7000 loss_val: 0.5973 acc_val: 0.7013 time: 0.0097s\n",
      "Epoch: 00005 loss_train: 0.5766 acc_train: 0.7087 loss_val: 0.6170 acc_val: 0.6948 time: 0.0065s\n",
      "Epoch: 00006 loss_train: 0.5704 acc_train: 0.7283 loss_val: 0.6127 acc_val: 0.6948 time: 0.0080s\n",
      "Epoch: 00007 loss_train: 0.5683 acc_train: 0.7130 loss_val: 0.5762 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00008 loss_train: 0.5780 acc_train: 0.7109 loss_val: 0.5687 acc_val: 0.7013 time: 0.0081s\n",
      "Epoch: 00009 loss_train: 0.5980 acc_train: 0.7109 loss_val: 0.6085 acc_val: 0.6948 time: 0.0096s\n",
      "Epoch: 00010 loss_train: 0.5975 acc_train: 0.7152 loss_val: 0.6119 acc_val: 0.7143 time: 0.0115s\n",
      "Epoch: 00011 loss_train: 0.5827 acc_train: 0.7000 loss_val: 0.6079 acc_val: 0.7078 time: 0.0086s\n",
      "Epoch: 00012 loss_train: 0.5967 acc_train: 0.7065 loss_val: 0.6029 acc_val: 0.6818 time: 0.0100s\n",
      "Epoch: 00013 loss_train: 0.5821 acc_train: 0.7196 loss_val: 0.5822 acc_val: 0.7208 time: 0.0096s\n",
      "Epoch: 00014 loss_train: 0.5874 acc_train: 0.6848 loss_val: 0.5955 acc_val: 0.7078 time: 0.0093s\n",
      "Epoch: 00015 loss_train: 0.5900 acc_train: 0.7152 loss_val: 0.6002 acc_val: 0.6883 time: 0.0077s\n",
      "Epoch: 00016 loss_train: 0.5749 acc_train: 0.7022 loss_val: 0.6156 acc_val: 0.6818 time: 0.0110s\n",
      "Epoch: 00017 loss_train: 0.5663 acc_train: 0.7109 loss_val: 0.6098 acc_val: 0.6883 time: 0.0082s\n",
      "Epoch: 00018 loss_train: 0.5718 acc_train: 0.7022 loss_val: 0.6005 acc_val: 0.6948 time: 0.0078s\n",
      "Epoch: 00019 loss_train: 0.6021 acc_train: 0.6957 loss_val: 0.6246 acc_val: 0.6818 time: 0.0075s\n",
      "Epoch: 00020 loss_train: 0.6005 acc_train: 0.7043 loss_val: 0.5839 acc_val: 0.7403 time: 0.0076s\n",
      "Epoch: 00021 loss_train: 0.5617 acc_train: 0.7196 loss_val: 0.5862 acc_val: 0.7208 time: 0.0124s\n",
      "Epoch: 00022 loss_train: 0.5951 acc_train: 0.7043 loss_val: 0.6160 acc_val: 0.6948 time: 0.0083s\n",
      "Epoch: 00023 loss_train: 0.6373 acc_train: 0.7022 loss_val: 0.6426 acc_val: 0.7013 time: 0.0111s\n",
      "Epoch: 00024 loss_train: 0.5703 acc_train: 0.7022 loss_val: 0.6027 acc_val: 0.7078 time: 0.0093s\n",
      "Epoch: 00025 loss_train: 0.5933 acc_train: 0.6957 loss_val: 0.5828 acc_val: 0.7078 time: 0.0080s\n",
      "Epoch: 00026 loss_train: 0.5835 acc_train: 0.7152 loss_val: 0.5907 acc_val: 0.6948 time: 0.0095s\n",
      "Epoch: 00027 loss_train: 0.5929 acc_train: 0.7043 loss_val: 0.5960 acc_val: 0.7208 time: 0.0095s\n",
      "Epoch: 00028 loss_train: 0.5719 acc_train: 0.7130 loss_val: 0.5783 acc_val: 0.7013 time: 0.0112s\n",
      "Epoch: 00029 loss_train: 0.5739 acc_train: 0.7109 loss_val: 0.5809 acc_val: 0.6818 time: 0.0091s\n",
      "Epoch: 00030 loss_train: 0.6176 acc_train: 0.6978 loss_val: 0.6134 acc_val: 0.6818 time: 0.0091s\n",
      "Epoch: 00031 loss_train: 0.5934 acc_train: 0.7043 loss_val: 0.6047 acc_val: 0.6883 time: 0.0101s\n",
      "Epoch: 00032 loss_train: 0.5791 acc_train: 0.7174 loss_val: 0.6126 acc_val: 0.6883 time: 0.0110s\n",
      "Epoch: 00033 loss_train: 0.5986 acc_train: 0.7109 loss_val: 0.6155 acc_val: 0.6753 time: 0.0103s\n",
      "Epoch: 00034 loss_train: 0.6042 acc_train: 0.7109 loss_val: 0.5875 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00035 loss_train: 0.6067 acc_train: 0.6957 loss_val: 0.6346 acc_val: 0.6753 time: 0.0080s\n",
      "Epoch: 00036 loss_train: 0.5977 acc_train: 0.7152 loss_val: 0.5863 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00037 loss_train: 0.6097 acc_train: 0.6913 loss_val: 0.6056 acc_val: 0.6948 time: 0.0072s\n",
      "Epoch: 00038 loss_train: 0.6014 acc_train: 0.7109 loss_val: 0.5971 acc_val: 0.7013 time: 0.0085s\n",
      "Epoch: 00039 loss_train: 0.6020 acc_train: 0.6978 loss_val: 0.6025 acc_val: 0.7013 time: 0.0095s\n",
      "Epoch: 00040 loss_train: 0.6043 acc_train: 0.7130 loss_val: 0.6056 acc_val: 0.7013 time: 0.0080s\n",
      "Epoch: 00041 loss_train: 0.6025 acc_train: 0.6978 loss_val: 0.6287 acc_val: 0.6948 time: 0.0093s\n",
      "Epoch: 00042 loss_train: 0.6111 acc_train: 0.7130 loss_val: 0.6099 acc_val: 0.6948 time: 0.0111s\n",
      "Epoch: 00043 loss_train: 0.6033 acc_train: 0.7043 loss_val: 0.6065 acc_val: 0.7013 time: 0.0123s\n",
      "Epoch: 00044 loss_train: 0.6003 acc_train: 0.6935 loss_val: 0.6153 acc_val: 0.6818 time: 0.0095s\n",
      "Epoch: 00045 loss_train: 0.6018 acc_train: 0.6978 loss_val: 0.5991 acc_val: 0.7143 time: 0.0109s\n",
      "Epoch: 00046 loss_train: 0.5933 acc_train: 0.7000 loss_val: 0.6119 acc_val: 0.6948 time: 0.0097s\n",
      "Epoch: 00047 loss_train: 0.5969 acc_train: 0.7000 loss_val: 0.6257 acc_val: 0.6753 time: 0.0096s\n",
      "Epoch: 00048 loss_train: 0.5827 acc_train: 0.7130 loss_val: 0.6150 acc_val: 0.6753 time: 0.0130s\n",
      "Epoch: 00049 loss_train: 0.5817 acc_train: 0.7196 loss_val: 0.6197 acc_val: 0.7013 time: 0.0111s\n",
      "Epoch: 00050 loss_train: 0.5822 acc_train: 0.7109 loss_val: 0.5850 acc_val: 0.6883 time: 0.0102s\n",
      "Epoch: 00051 loss_train: 0.6255 acc_train: 0.7109 loss_val: 0.6503 acc_val: 0.7143 time: 0.0150s\n",
      "Epoch: 00052 loss_train: 0.6271 acc_train: 0.6978 loss_val: 0.5442 acc_val: 0.7013 time: 0.0120s\n",
      "Epoch: 00053 loss_train: 0.5868 acc_train: 0.7174 loss_val: 0.5932 acc_val: 0.7013 time: 0.0112s\n",
      "Epoch: 00054 loss_train: 0.5899 acc_train: 0.7087 loss_val: 0.6133 acc_val: 0.7013 time: 0.0106s\n",
      "Epoch: 00055 loss_train: 0.5904 acc_train: 0.7261 loss_val: 0.5728 acc_val: 0.7273 time: 0.0116s\n",
      "Epoch: 00056 loss_train: 0.6008 acc_train: 0.7043 loss_val: 0.6083 acc_val: 0.7013 time: 0.0099s\n",
      "Epoch: 00057 loss_train: 0.5915 acc_train: 0.7152 loss_val: 0.6209 acc_val: 0.6818 time: 0.0101s\n",
      "Epoch: 00058 loss_train: 0.6059 acc_train: 0.6978 loss_val: 0.5918 acc_val: 0.7078 time: 0.0098s\n",
      "Epoch: 00059 loss_train: 0.5868 acc_train: 0.7109 loss_val: 0.6060 acc_val: 0.7078 time: 0.0102s\n",
      "Epoch: 00060 loss_train: 0.6043 acc_train: 0.7022 loss_val: 0.6176 acc_val: 0.6883 time: 0.0119s\n",
      "Epoch: 00061 loss_train: 0.5997 acc_train: 0.7130 loss_val: 0.5852 acc_val: 0.7078 time: 0.0108s\n",
      "Epoch: 00062 loss_train: 0.6011 acc_train: 0.7043 loss_val: 0.6124 acc_val: 0.7078 time: 0.0089s\n",
      "Epoch: 00063 loss_train: 0.5856 acc_train: 0.7304 loss_val: 0.5899 acc_val: 0.7338 time: 0.0130s\n",
      "Epoch: 00064 loss_train: 0.6083 acc_train: 0.7022 loss_val: 0.6090 acc_val: 0.6818 time: 0.0119s\n",
      "Epoch: 00065 loss_train: 0.5986 acc_train: 0.7174 loss_val: 0.5737 acc_val: 0.7208 time: 0.0108s\n",
      "Epoch: 00066 loss_train: 0.5907 acc_train: 0.7152 loss_val: 0.6067 acc_val: 0.6948 time: 0.0085s\n",
      "Epoch: 00067 loss_train: 0.5703 acc_train: 0.7326 loss_val: 0.5891 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00068 loss_train: 0.5998 acc_train: 0.7043 loss_val: 0.5855 acc_val: 0.7078 time: 0.0104s\n",
      "Epoch: 00069 loss_train: 0.5908 acc_train: 0.7130 loss_val: 0.6223 acc_val: 0.6883 time: 0.0226s\n",
      "Epoch: 00070 loss_train: 0.5912 acc_train: 0.7152 loss_val: 0.5707 acc_val: 0.7208 time: 0.0165s\n",
      "Epoch: 00071 loss_train: 0.5886 acc_train: 0.7196 loss_val: 0.6006 acc_val: 0.7013 time: 0.0166s\n",
      "Epoch: 00072 loss_train: 0.5936 acc_train: 0.7109 loss_val: 0.5727 acc_val: 0.7208 time: 0.0172s\n",
      "Epoch: 00073 loss_train: 0.5971 acc_train: 0.7065 loss_val: 0.5915 acc_val: 0.7143 time: 0.0220s\n",
      "Epoch: 00074 loss_train: 0.6035 acc_train: 0.6978 loss_val: 0.5963 acc_val: 0.7013 time: 0.0188s\n",
      "Epoch: 00075 loss_train: 0.5935 acc_train: 0.7130 loss_val: 0.5729 acc_val: 0.7338 time: 0.0164s\n",
      "Epoch: 00076 loss_train: 0.6124 acc_train: 0.6870 loss_val: 0.5769 acc_val: 0.7143 time: 0.0117s\n",
      "Epoch: 00077 loss_train: 0.6001 acc_train: 0.7022 loss_val: 0.6021 acc_val: 0.7013 time: 0.0116s\n",
      "Epoch: 00078 loss_train: 0.5787 acc_train: 0.7217 loss_val: 0.6300 acc_val: 0.6818 time: 0.0184s\n",
      "Epoch: 00079 loss_train: 0.5784 acc_train: 0.7130 loss_val: 0.6148 acc_val: 0.6818 time: 0.0243s\n",
      "Epoch: 00080 loss_train: 0.5953 acc_train: 0.7109 loss_val: 0.6033 acc_val: 0.6883 time: 0.0234s\n",
      "Epoch: 00081 loss_train: 0.5974 acc_train: 0.7022 loss_val: 0.6201 acc_val: 0.6753 time: 0.0181s\n",
      "Epoch: 00082 loss_train: 0.5926 acc_train: 0.7109 loss_val: 0.6283 acc_val: 0.6948 time: 0.0202s\n",
      "Epoch: 00083 loss_train: 0.5884 acc_train: 0.7109 loss_val: 0.5960 acc_val: 0.7013 time: 0.0687s\n",
      "Epoch: 00084 loss_train: 0.5962 acc_train: 0.7022 loss_val: 0.5833 acc_val: 0.7143 time: 0.0166s\n",
      "Epoch: 00085 loss_train: 0.6045 acc_train: 0.6957 loss_val: 0.6006 acc_val: 0.7078 time: 0.0181s\n",
      "Epoch: 00086 loss_train: 0.5824 acc_train: 0.7043 loss_val: 0.5972 acc_val: 0.7013 time: 0.0174s\n",
      "Epoch: 00087 loss_train: 0.5884 acc_train: 0.7087 loss_val: 0.5887 acc_val: 0.7143 time: 0.0167s\n",
      "Epoch: 00088 loss_train: 0.5816 acc_train: 0.7130 loss_val: 0.6389 acc_val: 0.6688 time: 0.0175s\n",
      "Epoch: 00089 loss_train: 0.5942 acc_train: 0.7087 loss_val: 0.6623 acc_val: 0.6753 time: 0.0182s\n",
      "Epoch: 00090 loss_train: 0.5782 acc_train: 0.7174 loss_val: 0.5916 acc_val: 0.6883 time: 0.0150s\n",
      "Epoch: 00091 loss_train: 0.5737 acc_train: 0.7152 loss_val: 0.5957 acc_val: 0.7143 time: 0.0150s\n",
      "Epoch: 00092 loss_train: 0.5838 acc_train: 0.7130 loss_val: 0.6050 acc_val: 0.7273 time: 0.0134s\n",
      "Epoch: 00093 loss_train: 0.6002 acc_train: 0.6935 loss_val: 0.6044 acc_val: 0.7013 time: 0.0136s\n",
      "Epoch: 00094 loss_train: 0.5759 acc_train: 0.7152 loss_val: 0.6124 acc_val: 0.7078 time: 0.0167s\n",
      "Epoch: 00095 loss_train: 0.5696 acc_train: 0.7130 loss_val: 0.6346 acc_val: 0.6623 time: 0.0128s\n",
      "Epoch: 00096 loss_train: 0.5867 acc_train: 0.6935 loss_val: 0.5956 acc_val: 0.7013 time: 0.0126s\n",
      "Epoch: 00097 loss_train: 0.5913 acc_train: 0.7000 loss_val: 0.5778 acc_val: 0.7078 time: 0.0135s\n",
      "Epoch: 00098 loss_train: 0.5787 acc_train: 0.7043 loss_val: 0.5990 acc_val: 0.6948 time: 0.0162s\n",
      "Epoch: 00099 loss_train: 0.5775 acc_train: 0.7022 loss_val: 0.6252 acc_val: 0.6818 time: 0.0135s\n",
      "Epoch: 00100 loss_train: 0.5845 acc_train: 0.7109 loss_val: 0.5915 acc_val: 0.7078 time: 0.0141s\n",
      "Epoch: 00101 loss_train: 0.5811 acc_train: 0.7022 loss_val: 0.5890 acc_val: 0.7143 time: 0.0130s\n",
      "Epoch: 00102 loss_train: 0.5790 acc_train: 0.7217 loss_val: 0.5799 acc_val: 0.6688 time: 0.0124s\n",
      "Epoch: 00103 loss_train: 0.5673 acc_train: 0.7174 loss_val: 0.5969 acc_val: 0.7078 time: 0.0137s\n",
      "Epoch: 00104 loss_train: 0.5774 acc_train: 0.7217 loss_val: 0.6007 acc_val: 0.6753 time: 0.0113s\n",
      "Epoch: 00105 loss_train: 0.5716 acc_train: 0.7130 loss_val: 0.5955 acc_val: 0.6948 time: 0.0118s\n",
      "Epoch: 00106 loss_train: 0.5512 acc_train: 0.7261 loss_val: 0.6144 acc_val: 0.6818 time: 0.0113s\n",
      "Epoch: 00107 loss_train: 0.5672 acc_train: 0.7261 loss_val: 0.5783 acc_val: 0.7143 time: 0.0109s\n",
      "Epoch: 00108 loss_train: 0.5775 acc_train: 0.7043 loss_val: 0.5828 acc_val: 0.6883 time: 0.0129s\n",
      "Epoch: 00109 loss_train: 0.5719 acc_train: 0.7022 loss_val: 0.5984 acc_val: 0.6623 time: 0.0116s\n",
      "Epoch: 00110 loss_train: 0.5746 acc_train: 0.7130 loss_val: 0.5764 acc_val: 0.7273 time: 0.0152s\n",
      "Epoch: 00111 loss_train: 0.5794 acc_train: 0.7043 loss_val: 0.6273 acc_val: 0.7078 time: 0.0135s\n",
      "Epoch: 00112 loss_train: 0.5646 acc_train: 0.7130 loss_val: 0.6067 acc_val: 0.7208 time: 0.0120s\n",
      "Epoch: 00113 loss_train: 0.5566 acc_train: 0.7196 loss_val: 0.5522 acc_val: 0.7143 time: 0.0173s\n",
      "Epoch: 00114 loss_train: 0.5699 acc_train: 0.7109 loss_val: 0.5952 acc_val: 0.6883 time: 0.0193s\n",
      "Epoch: 00115 loss_train: 0.5627 acc_train: 0.7130 loss_val: 0.5810 acc_val: 0.7273 time: 0.0144s\n",
      "Epoch: 00116 loss_train: 0.5657 acc_train: 0.7261 loss_val: 0.6133 acc_val: 0.6818 time: 0.0122s\n",
      "Epoch: 00117 loss_train: 0.5794 acc_train: 0.7043 loss_val: 0.5617 acc_val: 0.7078 time: 0.0134s\n",
      "Epoch: 00118 loss_train: 0.5766 acc_train: 0.7174 loss_val: 0.5768 acc_val: 0.7273 time: 0.0113s\n",
      "Epoch: 00119 loss_train: 0.5657 acc_train: 0.7261 loss_val: 0.5603 acc_val: 0.6948 time: 0.0106s\n",
      "Epoch: 00120 loss_train: 0.5682 acc_train: 0.7152 loss_val: 0.5727 acc_val: 0.7078 time: 0.0088s\n",
      "Epoch: 00121 loss_train: 0.5804 acc_train: 0.7087 loss_val: 0.5973 acc_val: 0.7013 time: 0.0100s\n",
      "Epoch: 00122 loss_train: 0.5825 acc_train: 0.7109 loss_val: 0.6013 acc_val: 0.6883 time: 0.0140s\n",
      "Epoch: 00123 loss_train: 0.5827 acc_train: 0.7152 loss_val: 0.5922 acc_val: 0.7143 time: 0.0114s\n",
      "Epoch: 00124 loss_train: 0.5763 acc_train: 0.7130 loss_val: 0.6052 acc_val: 0.6688 time: 0.0115s\n",
      "Epoch: 00125 loss_train: 0.5573 acc_train: 0.7239 loss_val: 0.5680 acc_val: 0.7208 time: 0.0121s\n",
      "Epoch: 00126 loss_train: 0.5633 acc_train: 0.7174 loss_val: 0.5677 acc_val: 0.7078 time: 0.0109s\n",
      "Epoch: 00127 loss_train: 0.5939 acc_train: 0.6957 loss_val: 0.5661 acc_val: 0.7078 time: 0.0092s\n",
      "Epoch: 00128 loss_train: 0.5580 acc_train: 0.7196 loss_val: 0.6212 acc_val: 0.7078 time: 0.0084s\n",
      "Epoch: 00129 loss_train: 0.5595 acc_train: 0.7283 loss_val: 0.5843 acc_val: 0.7143 time: 0.0110s\n",
      "Epoch: 00130 loss_train: 0.5699 acc_train: 0.7261 loss_val: 0.5713 acc_val: 0.7078 time: 0.0123s\n",
      "Epoch: 00131 loss_train: 0.5741 acc_train: 0.7261 loss_val: 0.6092 acc_val: 0.7078 time: 0.0136s\n",
      "Epoch: 00132 loss_train: 0.5688 acc_train: 0.7152 loss_val: 0.5912 acc_val: 0.7013 time: 0.0190s\n",
      "Epoch: 00133 loss_train: 0.5668 acc_train: 0.7217 loss_val: 0.5804 acc_val: 0.7143 time: 0.0109s\n",
      "Epoch: 00134 loss_train: 0.5690 acc_train: 0.7196 loss_val: 0.5830 acc_val: 0.6948 time: 0.0117s\n",
      "Epoch: 00135 loss_train: 0.5567 acc_train: 0.7196 loss_val: 0.5890 acc_val: 0.6948 time: 0.0101s\n",
      "Epoch: 00136 loss_train: 0.5737 acc_train: 0.7087 loss_val: 0.5731 acc_val: 0.7078 time: 0.0101s\n",
      "Epoch: 00137 loss_train: 0.5618 acc_train: 0.7174 loss_val: 0.5914 acc_val: 0.7143 time: 0.0105s\n",
      "Epoch: 00138 loss_train: 0.5673 acc_train: 0.7174 loss_val: 0.5868 acc_val: 0.6883 time: 0.0129s\n",
      "Epoch: 00139 loss_train: 0.5631 acc_train: 0.7130 loss_val: 0.5662 acc_val: 0.7013 time: 0.0107s\n",
      "Epoch: 00140 loss_train: 0.5730 acc_train: 0.7065 loss_val: 0.5739 acc_val: 0.7143 time: 0.0121s\n",
      "Epoch: 00141 loss_train: 0.5601 acc_train: 0.7152 loss_val: 0.5786 acc_val: 0.7013 time: 0.0095s\n",
      "Epoch: 00142 loss_train: 0.5624 acc_train: 0.7174 loss_val: 0.5738 acc_val: 0.7143 time: 0.0102s\n",
      "Epoch: 00143 loss_train: 0.5518 acc_train: 0.7217 loss_val: 0.5706 acc_val: 0.7078 time: 0.0131s\n",
      "Epoch: 00144 loss_train: 0.5651 acc_train: 0.7000 loss_val: 0.5779 acc_val: 0.6948 time: 0.0107s\n",
      "Epoch: 00145 loss_train: 0.5531 acc_train: 0.7130 loss_val: 0.5693 acc_val: 0.7078 time: 0.0065s\n",
      "Epoch: 00146 loss_train: 0.5750 acc_train: 0.7043 loss_val: 0.5924 acc_val: 0.7013 time: 0.0111s\n",
      "Epoch: 00147 loss_train: 0.5709 acc_train: 0.7109 loss_val: 0.5783 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00148 loss_train: 0.5547 acc_train: 0.7152 loss_val: 0.5741 acc_val: 0.7078 time: 0.0109s\n",
      "Epoch: 00149 loss_train: 0.5519 acc_train: 0.7087 loss_val: 0.5724 acc_val: 0.7078 time: 0.0104s\n",
      "Epoch: 00150 loss_train: 0.5365 acc_train: 0.7304 loss_val: 0.5984 acc_val: 0.6688 time: 0.0102s\n",
      "Epoch: 00151 loss_train: 0.5583 acc_train: 0.7109 loss_val: 0.5754 acc_val: 0.6883 time: 0.0102s\n",
      "Epoch: 00152 loss_train: 0.5488 acc_train: 0.7109 loss_val: 0.6494 acc_val: 0.6234 time: 0.0104s\n",
      "Epoch: 00153 loss_train: 0.5520 acc_train: 0.7239 loss_val: 0.5606 acc_val: 0.7078 time: 0.0090s\n",
      "Epoch: 00154 loss_train: 0.5416 acc_train: 0.7326 loss_val: 0.5779 acc_val: 0.7078 time: 0.0103s\n",
      "Epoch: 00155 loss_train: 0.5481 acc_train: 0.7239 loss_val: 0.5594 acc_val: 0.6818 time: 0.0082s\n",
      "Epoch: 00156 loss_train: 0.5522 acc_train: 0.7304 loss_val: 0.5705 acc_val: 0.7078 time: 0.0105s\n",
      "Epoch: 00157 loss_train: 0.5636 acc_train: 0.7217 loss_val: 0.5845 acc_val: 0.7013 time: 0.0107s\n",
      "Epoch: 00158 loss_train: 0.5479 acc_train: 0.7174 loss_val: 0.6007 acc_val: 0.7013 time: 0.0107s\n",
      "Epoch: 00159 loss_train: 0.5514 acc_train: 0.7196 loss_val: 0.5550 acc_val: 0.7013 time: 0.0128s\n",
      "Epoch: 00160 loss_train: 0.5796 acc_train: 0.7022 loss_val: 0.6058 acc_val: 0.6948 time: 0.0112s\n",
      "Epoch: 00161 loss_train: 0.5543 acc_train: 0.7174 loss_val: 0.5632 acc_val: 0.7143 time: 0.0087s\n",
      "Epoch: 00162 loss_train: 0.5687 acc_train: 0.7261 loss_val: 0.5809 acc_val: 0.6948 time: 0.0108s\n",
      "Epoch: 00163 loss_train: 0.5563 acc_train: 0.7043 loss_val: 0.6023 acc_val: 0.7078 time: 0.0102s\n",
      "Epoch: 00164 loss_train: 0.5623 acc_train: 0.7065 loss_val: 0.5893 acc_val: 0.6883 time: 0.0100s\n",
      "Epoch: 00165 loss_train: 0.5575 acc_train: 0.7174 loss_val: 0.5710 acc_val: 0.6948 time: 0.0097s\n",
      "Epoch: 00166 loss_train: 0.5439 acc_train: 0.7196 loss_val: 0.5649 acc_val: 0.7013 time: 0.0095s\n",
      "Epoch: 00167 loss_train: 0.5407 acc_train: 0.7130 loss_val: 0.5673 acc_val: 0.6948 time: 0.0101s\n",
      "Epoch: 00168 loss_train: 0.5522 acc_train: 0.7217 loss_val: 0.5872 acc_val: 0.6883 time: 0.0113s\n",
      "Epoch: 00169 loss_train: 0.5749 acc_train: 0.7109 loss_val: 0.5502 acc_val: 0.7013 time: 0.0090s\n",
      "Epoch: 00170 loss_train: 0.5434 acc_train: 0.7065 loss_val: 0.5936 acc_val: 0.7078 time: 0.0109s\n",
      "Epoch: 00171 loss_train: 0.5392 acc_train: 0.7261 loss_val: 0.5748 acc_val: 0.7078 time: 0.0100s\n",
      "Epoch: 00172 loss_train: 0.5525 acc_train: 0.7174 loss_val: 0.6336 acc_val: 0.6948 time: 0.0100s\n",
      "Epoch: 00173 loss_train: 0.5574 acc_train: 0.7109 loss_val: 0.5764 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00174 loss_train: 0.5471 acc_train: 0.7304 loss_val: 0.5344 acc_val: 0.7338 time: 0.0099s\n",
      "Epoch: 00175 loss_train: 0.5769 acc_train: 0.7043 loss_val: 0.5800 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00176 loss_train: 0.5375 acc_train: 0.7109 loss_val: 0.5821 acc_val: 0.7013 time: 0.0106s\n",
      "Epoch: 00177 loss_train: 0.5460 acc_train: 0.7109 loss_val: 0.5461 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00178 loss_train: 0.5517 acc_train: 0.7043 loss_val: 0.5924 acc_val: 0.6753 time: 0.0117s\n",
      "Epoch: 00179 loss_train: 0.5638 acc_train: 0.7087 loss_val: 0.5742 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00180 loss_train: 0.5313 acc_train: 0.7152 loss_val: 0.6016 acc_val: 0.7078 time: 0.0103s\n",
      "Epoch: 00181 loss_train: 0.5614 acc_train: 0.7065 loss_val: 0.5678 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00182 loss_train: 0.5580 acc_train: 0.7109 loss_val: 0.5466 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00183 loss_train: 0.5315 acc_train: 0.7217 loss_val: 0.5612 acc_val: 0.6948 time: 0.0096s\n",
      "Epoch: 00184 loss_train: 0.5387 acc_train: 0.7326 loss_val: 0.5760 acc_val: 0.6948 time: 0.0095s\n",
      "Epoch: 00185 loss_train: 0.5335 acc_train: 0.7283 loss_val: 0.5455 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00186 loss_train: 0.5472 acc_train: 0.7109 loss_val: 0.5707 acc_val: 0.7273 time: 0.0092s\n",
      "Epoch: 00187 loss_train: 0.5374 acc_train: 0.7304 loss_val: 0.5528 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00188 loss_train: 0.5362 acc_train: 0.7304 loss_val: 0.5813 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00189 loss_train: 0.5463 acc_train: 0.7326 loss_val: 0.6091 acc_val: 0.7078 time: 0.0100s\n",
      "Epoch: 00190 loss_train: 0.5466 acc_train: 0.7130 loss_val: 0.5568 acc_val: 0.7013 time: 0.0100s\n",
      "Epoch: 00191 loss_train: 0.5450 acc_train: 0.7217 loss_val: 0.5590 acc_val: 0.6883 time: 0.0100s\n",
      "Epoch: 00192 loss_train: 0.5442 acc_train: 0.7261 loss_val: 0.5716 acc_val: 0.6818 time: 0.0097s\n",
      "Epoch: 00193 loss_train: 0.5547 acc_train: 0.7217 loss_val: 0.5527 acc_val: 0.7078 time: 0.0101s\n",
      "Epoch: 00194 loss_train: 0.5491 acc_train: 0.7304 loss_val: 0.5596 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00195 loss_train: 0.5508 acc_train: 0.7109 loss_val: 0.5536 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00196 loss_train: 0.5212 acc_train: 0.7413 loss_val: 0.5976 acc_val: 0.7078 time: 0.0097s\n",
      "Epoch: 00197 loss_train: 0.5473 acc_train: 0.7326 loss_val: 0.5890 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00198 loss_train: 0.5437 acc_train: 0.7239 loss_val: 0.5922 acc_val: 0.7208 time: 0.0135s\n",
      "Epoch: 00199 loss_train: 0.5484 acc_train: 0.7304 loss_val: 0.6195 acc_val: 0.6883 time: 0.0122s\n",
      "Epoch: 00200 loss_train: 0.5403 acc_train: 0.7304 loss_val: 0.5778 acc_val: 0.7078 time: 0.0116s\n",
      "Epoch: 00201 loss_train: 0.5561 acc_train: 0.7239 loss_val: 0.5729 acc_val: 0.7078 time: 0.0113s\n",
      "Epoch: 00202 loss_train: 0.5335 acc_train: 0.7348 loss_val: 0.5735 acc_val: 0.7208 time: 0.0107s\n",
      "Epoch: 00203 loss_train: 0.5376 acc_train: 0.7370 loss_val: 0.5556 acc_val: 0.7143 time: 0.0103s\n",
      "Epoch: 00204 loss_train: 0.5617 acc_train: 0.7370 loss_val: 0.5760 acc_val: 0.7273 time: 0.0104s\n",
      "Epoch: 00205 loss_train: 0.5413 acc_train: 0.7283 loss_val: 0.5873 acc_val: 0.7143 time: 0.0112s\n",
      "Epoch: 00206 loss_train: 0.5578 acc_train: 0.7196 loss_val: 0.5816 acc_val: 0.7078 time: 0.0100s\n",
      "Epoch: 00207 loss_train: 0.5320 acc_train: 0.7326 loss_val: 0.5644 acc_val: 0.7143 time: 0.0096s\n",
      "Epoch: 00208 loss_train: 0.5578 acc_train: 0.7152 loss_val: 0.5587 acc_val: 0.7078 time: 0.0111s\n",
      "Epoch: 00209 loss_train: 0.5384 acc_train: 0.7174 loss_val: 0.5610 acc_val: 0.7013 time: 0.0118s\n",
      "Epoch: 00210 loss_train: 0.5402 acc_train: 0.7370 loss_val: 0.5913 acc_val: 0.7143 time: 0.0111s\n",
      "Epoch: 00211 loss_train: 0.5372 acc_train: 0.7370 loss_val: 0.5641 acc_val: 0.7208 time: 0.0086s\n",
      "Epoch: 00212 loss_train: 0.5439 acc_train: 0.7391 loss_val: 0.5524 acc_val: 0.6948 time: 0.0105s\n",
      "Epoch: 00213 loss_train: 0.5658 acc_train: 0.7239 loss_val: 0.5420 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00214 loss_train: 0.5464 acc_train: 0.7239 loss_val: 0.5565 acc_val: 0.7143 time: 0.0097s\n",
      "Epoch: 00215 loss_train: 0.5469 acc_train: 0.7413 loss_val: 0.5494 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00216 loss_train: 0.5347 acc_train: 0.7348 loss_val: 0.5763 acc_val: 0.7013 time: 0.0093s\n",
      "Epoch: 00217 loss_train: 0.5566 acc_train: 0.7109 loss_val: 0.5872 acc_val: 0.7013 time: 0.0108s\n",
      "Epoch: 00218 loss_train: 0.5350 acc_train: 0.7370 loss_val: 0.5499 acc_val: 0.7143 time: 0.0126s\n",
      "Epoch: 00219 loss_train: 0.5447 acc_train: 0.7152 loss_val: 0.5662 acc_val: 0.7403 time: 0.0109s\n",
      "Epoch: 00220 loss_train: 0.5464 acc_train: 0.7304 loss_val: 0.5802 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00221 loss_train: 0.5377 acc_train: 0.7304 loss_val: 0.5855 acc_val: 0.7143 time: 0.0105s\n",
      "Epoch: 00222 loss_train: 0.5320 acc_train: 0.7391 loss_val: 0.5575 acc_val: 0.7273 time: 0.0097s\n",
      "Epoch: 00223 loss_train: 0.5635 acc_train: 0.7043 loss_val: 0.6020 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00224 loss_train: 0.5323 acc_train: 0.7326 loss_val: 0.5662 acc_val: 0.7143 time: 0.0109s\n",
      "Epoch: 00225 loss_train: 0.5420 acc_train: 0.7413 loss_val: 0.6185 acc_val: 0.7208 time: 0.0116s\n",
      "Epoch: 00226 loss_train: 0.5330 acc_train: 0.7413 loss_val: 0.5347 acc_val: 0.7532 time: 0.0063s\n",
      "Epoch: 00227 loss_train: 0.5425 acc_train: 0.7326 loss_val: 0.6156 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00228 loss_train: 0.5330 acc_train: 0.7304 loss_val: 0.5410 acc_val: 0.7403 time: 0.0114s\n",
      "Epoch: 00229 loss_train: 0.5456 acc_train: 0.7304 loss_val: 0.5409 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00230 loss_train: 0.5355 acc_train: 0.7348 loss_val: 0.5954 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00231 loss_train: 0.5639 acc_train: 0.7196 loss_val: 0.5679 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00232 loss_train: 0.5611 acc_train: 0.7304 loss_val: 0.5704 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00233 loss_train: 0.5547 acc_train: 0.7130 loss_val: 0.5654 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00234 loss_train: 0.5443 acc_train: 0.7152 loss_val: 0.5822 acc_val: 0.7143 time: 0.0089s\n",
      "Epoch: 00235 loss_train: 0.5499 acc_train: 0.7196 loss_val: 0.5865 acc_val: 0.7143 time: 0.0099s\n",
      "Epoch: 00236 loss_train: 0.5501 acc_train: 0.7261 loss_val: 0.5501 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00237 loss_train: 0.5280 acc_train: 0.7326 loss_val: 0.5660 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00238 loss_train: 0.5264 acc_train: 0.7413 loss_val: 0.5652 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00239 loss_train: 0.5519 acc_train: 0.7304 loss_val: 0.5561 acc_val: 0.7208 time: 0.0120s\n",
      "Epoch: 00240 loss_train: 0.5385 acc_train: 0.7348 loss_val: 0.5171 acc_val: 0.7338 time: 0.0106s\n",
      "Epoch: 00241 loss_train: 0.5269 acc_train: 0.7457 loss_val: 0.5692 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00242 loss_train: 0.5559 acc_train: 0.7196 loss_val: 0.5610 acc_val: 0.7143 time: 0.0064s\n",
      "Epoch: 00243 loss_train: 0.5365 acc_train: 0.7283 loss_val: 0.5758 acc_val: 0.7013 time: 0.0105s\n",
      "Epoch: 00244 loss_train: 0.5308 acc_train: 0.7261 loss_val: 0.5976 acc_val: 0.7013 time: 0.0103s\n",
      "Epoch: 00245 loss_train: 0.5466 acc_train: 0.7152 loss_val: 0.5762 acc_val: 0.7013 time: 0.0100s\n",
      "Epoch: 00246 loss_train: 0.5612 acc_train: 0.7109 loss_val: 0.5844 acc_val: 0.6883 time: 0.0100s\n",
      "Epoch: 00247 loss_train: 0.5270 acc_train: 0.7261 loss_val: 0.6027 acc_val: 0.7143 time: 0.0079s\n",
      "Epoch: 00248 loss_train: 0.5463 acc_train: 0.7261 loss_val: 0.5757 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00249 loss_train: 0.5344 acc_train: 0.7326 loss_val: 0.5631 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00250 loss_train: 0.5404 acc_train: 0.7370 loss_val: 0.5762 acc_val: 0.7338 time: 0.0116s\n",
      "Epoch: 00251 loss_train: 0.5357 acc_train: 0.7348 loss_val: 0.5904 acc_val: 0.7013 time: 0.0090s\n",
      "Epoch: 00252 loss_train: 0.5315 acc_train: 0.7478 loss_val: 0.5705 acc_val: 0.7013 time: 0.0095s\n",
      "Epoch: 00253 loss_train: 0.5406 acc_train: 0.7261 loss_val: 0.5452 acc_val: 0.7403 time: 0.0106s\n",
      "Epoch: 00254 loss_train: 0.5494 acc_train: 0.7348 loss_val: 0.5692 acc_val: 0.7078 time: 0.0094s\n",
      "Epoch: 00255 loss_train: 0.5568 acc_train: 0.7196 loss_val: 0.5718 acc_val: 0.6948 time: 0.0095s\n",
      "Epoch: 00256 loss_train: 0.5548 acc_train: 0.7283 loss_val: 0.5730 acc_val: 0.6883 time: 0.0087s\n",
      "Epoch: 00257 loss_train: 0.5466 acc_train: 0.7239 loss_val: 0.5663 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00258 loss_train: 0.5475 acc_train: 0.7370 loss_val: 0.5709 acc_val: 0.7273 time: 0.0097s\n",
      "Epoch: 00259 loss_train: 0.5363 acc_train: 0.7283 loss_val: 0.5579 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00260 loss_train: 0.5406 acc_train: 0.7348 loss_val: 0.5650 acc_val: 0.7078 time: 0.0112s\n",
      "Epoch: 00261 loss_train: 0.5612 acc_train: 0.7370 loss_val: 0.5771 acc_val: 0.7532 time: 0.0084s\n",
      "Epoch: 00262 loss_train: 0.5554 acc_train: 0.7239 loss_val: 0.5708 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00263 loss_train: 0.5383 acc_train: 0.7217 loss_val: 0.5585 acc_val: 0.7208 time: 0.0094s\n",
      "Epoch: 00264 loss_train: 0.5427 acc_train: 0.7326 loss_val: 0.5656 acc_val: 0.7273 time: 0.0107s\n",
      "Epoch: 00265 loss_train: 0.5368 acc_train: 0.7326 loss_val: 0.5717 acc_val: 0.6948 time: 0.0108s\n",
      "Epoch: 00266 loss_train: 0.5411 acc_train: 0.7239 loss_val: 0.5684 acc_val: 0.7143 time: 0.0095s\n",
      "Epoch: 00267 loss_train: 0.5402 acc_train: 0.7283 loss_val: 0.5742 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00268 loss_train: 0.5395 acc_train: 0.7174 loss_val: 0.5411 acc_val: 0.7208 time: 0.0096s\n",
      "Epoch: 00269 loss_train: 0.5441 acc_train: 0.7370 loss_val: 0.5746 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00270 loss_train: 0.5438 acc_train: 0.7348 loss_val: 0.5717 acc_val: 0.7143 time: 0.0131s\n",
      "Epoch: 00271 loss_train: 0.5297 acc_train: 0.7413 loss_val: 0.5487 acc_val: 0.7208 time: 0.0121s\n",
      "Epoch: 00272 loss_train: 0.5376 acc_train: 0.7283 loss_val: 0.5786 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00273 loss_train: 0.5504 acc_train: 0.7239 loss_val: 0.5521 acc_val: 0.7143 time: 0.0116s\n",
      "Epoch: 00274 loss_train: 0.5461 acc_train: 0.7087 loss_val: 0.5686 acc_val: 0.7273 time: 0.0109s\n",
      "Epoch: 00275 loss_train: 0.5312 acc_train: 0.7370 loss_val: 0.5429 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00276 loss_train: 0.5313 acc_train: 0.7217 loss_val: 0.5699 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00277 loss_train: 0.5557 acc_train: 0.7370 loss_val: 0.5429 acc_val: 0.7273 time: 0.0107s\n",
      "Epoch: 00278 loss_train: 0.5403 acc_train: 0.7478 loss_val: 0.5611 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00279 loss_train: 0.5526 acc_train: 0.7348 loss_val: 0.5681 acc_val: 0.7143 time: 0.0114s\n",
      "Epoch: 00280 loss_train: 0.5270 acc_train: 0.7413 loss_val: 0.5486 acc_val: 0.7208 time: 0.0187s\n",
      "Epoch: 00281 loss_train: 0.5364 acc_train: 0.7326 loss_val: 0.5691 acc_val: 0.7013 time: 0.0127s\n",
      "Epoch: 00282 loss_train: 0.5489 acc_train: 0.7326 loss_val: 0.5462 acc_val: 0.7208 time: 0.0112s\n",
      "Epoch: 00283 loss_train: 0.5356 acc_train: 0.7217 loss_val: 0.5894 acc_val: 0.6948 time: 0.0105s\n",
      "Epoch: 00284 loss_train: 0.5478 acc_train: 0.7174 loss_val: 0.5333 acc_val: 0.7143 time: 0.0122s\n",
      "Epoch: 00285 loss_train: 0.5422 acc_train: 0.7283 loss_val: 0.5336 acc_val: 0.7403 time: 0.0111s\n",
      "Epoch: 00286 loss_train: 0.5331 acc_train: 0.7196 loss_val: 0.5720 acc_val: 0.7143 time: 0.0110s\n",
      "Epoch: 00287 loss_train: 0.5473 acc_train: 0.7196 loss_val: 0.5662 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00288 loss_train: 0.5412 acc_train: 0.7283 loss_val: 0.5711 acc_val: 0.7468 time: 0.0103s\n",
      "Epoch: 00289 loss_train: 0.5344 acc_train: 0.7304 loss_val: 0.5563 acc_val: 0.7273 time: 0.0122s\n",
      "Epoch: 00290 loss_train: 0.5167 acc_train: 0.7565 loss_val: 0.6105 acc_val: 0.7273 time: 0.0121s\n",
      "Epoch: 00291 loss_train: 0.5433 acc_train: 0.7261 loss_val: 0.5782 acc_val: 0.7273 time: 0.0130s\n",
      "Epoch: 00292 loss_train: 0.5508 acc_train: 0.7283 loss_val: 0.5558 acc_val: 0.7532 time: 0.0121s\n",
      "Epoch: 00293 loss_train: 0.5566 acc_train: 0.7304 loss_val: 0.5519 acc_val: 0.7403 time: 0.0125s\n",
      "Epoch: 00294 loss_train: 0.5398 acc_train: 0.7370 loss_val: 0.5527 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00295 loss_train: 0.5523 acc_train: 0.7370 loss_val: 0.5548 acc_val: 0.7403 time: 0.0127s\n",
      "Epoch: 00296 loss_train: 0.5423 acc_train: 0.7283 loss_val: 0.5758 acc_val: 0.7208 time: 0.0117s\n",
      "Epoch: 00297 loss_train: 0.5427 acc_train: 0.7283 loss_val: 0.5809 acc_val: 0.7208 time: 0.0124s\n",
      "Epoch: 00298 loss_train: 0.5232 acc_train: 0.7413 loss_val: 0.5541 acc_val: 0.7273 time: 0.0118s\n",
      "Epoch: 00299 loss_train: 0.5373 acc_train: 0.7413 loss_val: 0.5483 acc_val: 0.7078 time: 0.0131s\n",
      "Epoch: 00300 loss_train: 0.5411 acc_train: 0.7261 loss_val: 0.5790 acc_val: 0.7338 time: 0.0140s\n",
      "Epoch: 00301 loss_train: 0.5388 acc_train: 0.7391 loss_val: 0.5878 acc_val: 0.7208 time: 0.0106s\n",
      "Epoch: 00302 loss_train: 0.5413 acc_train: 0.7261 loss_val: 0.5662 acc_val: 0.7208 time: 0.0109s\n",
      "Epoch: 00303 loss_train: 0.5478 acc_train: 0.7283 loss_val: 0.6063 acc_val: 0.7078 time: 0.0120s\n",
      "Epoch: 00304 loss_train: 0.5451 acc_train: 0.7370 loss_val: 0.5690 acc_val: 0.7338 time: 0.0133s\n",
      "Epoch: 00305 loss_train: 0.5336 acc_train: 0.7348 loss_val: 0.5860 acc_val: 0.7208 time: 0.0121s\n",
      "Epoch: 00306 loss_train: 0.5319 acc_train: 0.7413 loss_val: 0.5408 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00307 loss_train: 0.5303 acc_train: 0.7326 loss_val: 0.5800 acc_val: 0.7143 time: 0.0104s\n",
      "Epoch: 00308 loss_train: 0.5296 acc_train: 0.7391 loss_val: 0.5449 acc_val: 0.7208 time: 0.0121s\n",
      "Epoch: 00309 loss_train: 0.5350 acc_train: 0.7326 loss_val: 0.5324 acc_val: 0.7403 time: 0.0125s\n",
      "Epoch: 00310 loss_train: 0.5297 acc_train: 0.7391 loss_val: 0.5560 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 00311 loss_train: 0.5326 acc_train: 0.7283 loss_val: 0.5448 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00312 loss_train: 0.5554 acc_train: 0.7304 loss_val: 0.5584 acc_val: 0.7403 time: 0.0088s\n",
      "Epoch: 00313 loss_train: 0.5511 acc_train: 0.7261 loss_val: 0.5628 acc_val: 0.7143 time: 0.0110s\n",
      "Epoch: 00314 loss_train: 0.5415 acc_train: 0.7304 loss_val: 0.5593 acc_val: 0.7143 time: 0.0118s\n",
      "Epoch: 00315 loss_train: 0.5356 acc_train: 0.7217 loss_val: 0.5387 acc_val: 0.7403 time: 0.0166s\n",
      "Epoch: 00316 loss_train: 0.5367 acc_train: 0.7261 loss_val: 0.5729 acc_val: 0.7208 time: 0.0183s\n",
      "Epoch: 00317 loss_train: 0.5393 acc_train: 0.7217 loss_val: 0.5757 acc_val: 0.6948 time: 0.0116s\n",
      "Epoch: 00318 loss_train: 0.5347 acc_train: 0.7435 loss_val: 0.5567 acc_val: 0.7338 time: 0.0663s\n",
      "Epoch: 00319 loss_train: 0.5301 acc_train: 0.7348 loss_val: 0.5605 acc_val: 0.7273 time: 0.0163s\n",
      "Epoch: 00320 loss_train: 0.5503 acc_train: 0.7283 loss_val: 0.5815 acc_val: 0.7143 time: 0.0158s\n",
      "Epoch: 00321 loss_train: 0.5365 acc_train: 0.7217 loss_val: 0.5263 acc_val: 0.7532 time: 0.0108s\n",
      "Epoch: 00322 loss_train: 0.5310 acc_train: 0.7435 loss_val: 0.5802 acc_val: 0.7143 time: 0.0119s\n",
      "Epoch: 00323 loss_train: 0.5413 acc_train: 0.7326 loss_val: 0.5700 acc_val: 0.7403 time: 0.0110s\n",
      "Epoch: 00324 loss_train: 0.5299 acc_train: 0.7391 loss_val: 0.5583 acc_val: 0.7338 time: 0.0089s\n",
      "Epoch: 00325 loss_train: 0.5350 acc_train: 0.7304 loss_val: 0.5353 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00326 loss_train: 0.5448 acc_train: 0.7217 loss_val: 0.5665 acc_val: 0.7208 time: 0.0107s\n",
      "Epoch: 00327 loss_train: 0.5402 acc_train: 0.7304 loss_val: 0.5566 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00328 loss_train: 0.5370 acc_train: 0.7457 loss_val: 0.5526 acc_val: 0.7273 time: 0.0113s\n",
      "Epoch: 00329 loss_train: 0.5535 acc_train: 0.7174 loss_val: 0.5546 acc_val: 0.7208 time: 0.0112s\n",
      "Epoch: 00330 loss_train: 0.5336 acc_train: 0.7348 loss_val: 0.5686 acc_val: 0.7013 time: 0.0100s\n",
      "Epoch: 00331 loss_train: 0.5366 acc_train: 0.7391 loss_val: 0.5590 acc_val: 0.7143 time: 0.0112s\n",
      "Epoch: 00332 loss_train: 0.5472 acc_train: 0.7261 loss_val: 0.5475 acc_val: 0.7338 time: 0.0120s\n",
      "Epoch: 00333 loss_train: 0.5261 acc_train: 0.7435 loss_val: 0.5612 acc_val: 0.7143 time: 0.0082s\n",
      "Epoch: 00334 loss_train: 0.5291 acc_train: 0.7283 loss_val: 0.6134 acc_val: 0.7013 time: 0.0122s\n",
      "Epoch: 00335 loss_train: 0.5295 acc_train: 0.7413 loss_val: 0.5703 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00336 loss_train: 0.5499 acc_train: 0.7261 loss_val: 0.5914 acc_val: 0.7013 time: 0.0144s\n",
      "Epoch: 00337 loss_train: 0.5431 acc_train: 0.7239 loss_val: 0.5379 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00338 loss_train: 0.5401 acc_train: 0.7196 loss_val: 0.5725 acc_val: 0.7013 time: 0.0119s\n",
      "Epoch: 00339 loss_train: 0.5368 acc_train: 0.7348 loss_val: 0.5639 acc_val: 0.7273 time: 0.0110s\n",
      "Epoch: 00340 loss_train: 0.5529 acc_train: 0.7283 loss_val: 0.5653 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00341 loss_train: 0.5348 acc_train: 0.7239 loss_val: 0.5614 acc_val: 0.7078 time: 0.0116s\n",
      "Epoch: 00342 loss_train: 0.5344 acc_train: 0.7283 loss_val: 0.5497 acc_val: 0.7208 time: 0.0109s\n",
      "Epoch: 00343 loss_train: 0.5395 acc_train: 0.7326 loss_val: 0.5791 acc_val: 0.7208 time: 0.0127s\n",
      "Epoch: 00344 loss_train: 0.5554 acc_train: 0.7348 loss_val: 0.5458 acc_val: 0.7143 time: 0.0111s\n",
      "Epoch: 00345 loss_train: 0.5466 acc_train: 0.7239 loss_val: 0.6116 acc_val: 0.7273 time: 0.0166s\n",
      "Epoch: 00346 loss_train: 0.5362 acc_train: 0.7283 loss_val: 0.5315 acc_val: 0.7468 time: 0.0131s\n",
      "Epoch: 00347 loss_train: 0.5372 acc_train: 0.7304 loss_val: 0.5760 acc_val: 0.7143 time: 0.0126s\n",
      "Epoch: 00348 loss_train: 0.5438 acc_train: 0.7348 loss_val: 0.5783 acc_val: 0.7078 time: 0.0119s\n",
      "Epoch: 00349 loss_train: 0.5435 acc_train: 0.7370 loss_val: 0.5663 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00350 loss_train: 0.5387 acc_train: 0.7283 loss_val: 0.5359 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00351 loss_train: 0.5377 acc_train: 0.7239 loss_val: 0.5548 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00352 loss_train: 0.5426 acc_train: 0.7348 loss_val: 0.5578 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00353 loss_train: 0.5402 acc_train: 0.7348 loss_val: 0.5648 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00354 loss_train: 0.5361 acc_train: 0.7326 loss_val: 0.5633 acc_val: 0.7078 time: 0.0125s\n",
      "Epoch: 00355 loss_train: 0.5333 acc_train: 0.7217 loss_val: 0.5758 acc_val: 0.7013 time: 0.0110s\n",
      "Epoch: 00356 loss_train: 0.5311 acc_train: 0.7261 loss_val: 0.5528 acc_val: 0.7208 time: 0.0118s\n",
      "Epoch: 00357 loss_train: 0.5752 acc_train: 0.7261 loss_val: 0.5676 acc_val: 0.7078 time: 0.0116s\n",
      "Epoch: 00358 loss_train: 0.5276 acc_train: 0.7413 loss_val: 0.5898 acc_val: 0.7143 time: 0.0120s\n",
      "Epoch: 00359 loss_train: 0.5464 acc_train: 0.7391 loss_val: 0.5785 acc_val: 0.7338 time: 0.0104s\n",
      "Epoch: 00360 loss_train: 0.5457 acc_train: 0.7391 loss_val: 0.5804 acc_val: 0.7468 time: 0.0128s\n",
      "Epoch: 00361 loss_train: 0.5615 acc_train: 0.7130 loss_val: 0.5820 acc_val: 0.7273 time: 0.0125s\n",
      "Epoch: 00362 loss_train: 0.5615 acc_train: 0.7435 loss_val: 0.5627 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00363 loss_train: 0.5484 acc_train: 0.7217 loss_val: 0.5875 acc_val: 0.7338 time: 0.0135s\n",
      "Epoch: 00364 loss_train: 0.5519 acc_train: 0.7413 loss_val: 0.5880 acc_val: 0.7078 time: 0.0102s\n",
      "Epoch: 00365 loss_train: 0.5513 acc_train: 0.7283 loss_val: 0.5816 acc_val: 0.7143 time: 0.0121s\n",
      "Epoch: 00366 loss_train: 0.5433 acc_train: 0.7261 loss_val: 0.5893 acc_val: 0.7143 time: 0.0120s\n",
      "Epoch: 00367 loss_train: 0.5438 acc_train: 0.7348 loss_val: 0.5450 acc_val: 0.7468 time: 0.0111s\n",
      "Epoch: 00368 loss_train: 0.5322 acc_train: 0.7391 loss_val: 0.5623 acc_val: 0.7273 time: 0.0131s\n",
      "Epoch: 00369 loss_train: 0.5455 acc_train: 0.7283 loss_val: 0.5685 acc_val: 0.7143 time: 0.0130s\n",
      "Epoch: 00370 loss_train: 0.5250 acc_train: 0.7326 loss_val: 0.5607 acc_val: 0.7273 time: 0.0107s\n",
      "Epoch: 00371 loss_train: 0.5455 acc_train: 0.7217 loss_val: 0.6035 acc_val: 0.7403 time: 0.0110s\n",
      "Epoch: 00372 loss_train: 0.5476 acc_train: 0.7217 loss_val: 0.5440 acc_val: 0.7208 time: 0.0121s\n",
      "Epoch: 00373 loss_train: 0.5518 acc_train: 0.7152 loss_val: 0.5774 acc_val: 0.7273 time: 0.0119s\n",
      "Epoch: 00374 loss_train: 0.5450 acc_train: 0.7261 loss_val: 0.5550 acc_val: 0.7273 time: 0.0114s\n",
      "Epoch: 00375 loss_train: 0.5376 acc_train: 0.7283 loss_val: 0.5580 acc_val: 0.7208 time: 0.0107s\n",
      "Epoch: 00376 loss_train: 0.5508 acc_train: 0.7261 loss_val: 0.5461 acc_val: 0.6948 time: 0.0107s\n",
      "Epoch: 00377 loss_train: 0.5501 acc_train: 0.7196 loss_val: 0.5483 acc_val: 0.7468 time: 0.0105s\n",
      "Epoch: 00378 loss_train: 0.5494 acc_train: 0.7370 loss_val: 0.5412 acc_val: 0.7143 time: 0.0111s\n",
      "Epoch: 00379 loss_train: 0.5376 acc_train: 0.7239 loss_val: 0.5547 acc_val: 0.6948 time: 0.0108s\n",
      "Epoch: 00380 loss_train: 0.5389 acc_train: 0.7413 loss_val: 0.5475 acc_val: 0.7078 time: 0.0096s\n",
      "Epoch: 00381 loss_train: 0.5379 acc_train: 0.7196 loss_val: 0.5510 acc_val: 0.7208 time: 0.0117s\n",
      "Epoch: 00382 loss_train: 0.5363 acc_train: 0.7239 loss_val: 0.5839 acc_val: 0.6883 time: 0.0109s\n",
      "Epoch: 00383 loss_train: 0.5360 acc_train: 0.7391 loss_val: 0.5742 acc_val: 0.7403 time: 0.0101s\n",
      "Epoch: 00384 loss_train: 0.5343 acc_train: 0.7457 loss_val: 0.5527 acc_val: 0.7143 time: 0.0111s\n",
      "Epoch: 00385 loss_train: 0.5285 acc_train: 0.7435 loss_val: 0.5959 acc_val: 0.7143 time: 0.0110s\n",
      "Epoch: 00386 loss_train: 0.5301 acc_train: 0.7435 loss_val: 0.5730 acc_val: 0.7078 time: 0.0092s\n",
      "Epoch: 00387 loss_train: 0.5355 acc_train: 0.7261 loss_val: 0.5766 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00388 loss_train: 0.5355 acc_train: 0.7217 loss_val: 0.5835 acc_val: 0.7078 time: 0.0080s\n",
      "Epoch: 00389 loss_train: 0.5457 acc_train: 0.7304 loss_val: 0.5787 acc_val: 0.7208 time: 0.0113s\n",
      "Epoch: 00390 loss_train: 0.5255 acc_train: 0.7478 loss_val: 0.5568 acc_val: 0.7403 time: 0.0106s\n",
      "Epoch: 00391 loss_train: 0.5344 acc_train: 0.7370 loss_val: 0.6042 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00392 loss_train: 0.5251 acc_train: 0.7304 loss_val: 0.5623 acc_val: 0.7403 time: 0.0104s\n",
      "Epoch: 00393 loss_train: 0.5349 acc_train: 0.7348 loss_val: 0.5488 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00394 loss_train: 0.5385 acc_train: 0.7348 loss_val: 0.5538 acc_val: 0.7208 time: 0.0128s\n",
      "Epoch: 00395 loss_train: 0.5425 acc_train: 0.7283 loss_val: 0.5416 acc_val: 0.7143 time: 0.0153s\n",
      "Epoch: 00396 loss_train: 0.5567 acc_train: 0.7370 loss_val: 0.5655 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00397 loss_train: 0.5327 acc_train: 0.7348 loss_val: 0.5636 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00398 loss_train: 0.5317 acc_train: 0.7565 loss_val: 0.5943 acc_val: 0.7013 time: 0.0110s\n",
      "Epoch: 00399 loss_train: 0.5586 acc_train: 0.7283 loss_val: 0.5929 acc_val: 0.6818 time: 0.0084s\n",
      "Epoch: 00400 loss_train: 0.5396 acc_train: 0.7348 loss_val: 0.5764 acc_val: 0.7208 time: 0.0114s\n",
      "Epoch: 00401 loss_train: 0.5431 acc_train: 0.7283 loss_val: 0.5584 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00402 loss_train: 0.5315 acc_train: 0.7435 loss_val: 0.5743 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00403 loss_train: 0.5466 acc_train: 0.7261 loss_val: 0.5633 acc_val: 0.7078 time: 0.0102s\n",
      "Epoch: 00404 loss_train: 0.5473 acc_train: 0.7304 loss_val: 0.5736 acc_val: 0.7273 time: 0.0106s\n",
      "Epoch: 00405 loss_train: 0.5352 acc_train: 0.7283 loss_val: 0.5353 acc_val: 0.7403 time: 0.0126s\n",
      "Epoch: 00406 loss_train: 0.5266 acc_train: 0.7304 loss_val: 0.5861 acc_val: 0.7143 time: 0.0104s\n",
      "Epoch: 00407 loss_train: 0.5343 acc_train: 0.7435 loss_val: 0.5699 acc_val: 0.7013 time: 0.0100s\n",
      "Epoch: 00408 loss_train: 0.5347 acc_train: 0.7196 loss_val: 0.5469 acc_val: 0.7338 time: 0.0113s\n",
      "Epoch: 00409 loss_train: 0.5257 acc_train: 0.7478 loss_val: 0.5990 acc_val: 0.7273 time: 0.0126s\n",
      "Epoch: 00410 loss_train: 0.5302 acc_train: 0.7457 loss_val: 0.5524 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00411 loss_train: 0.5350 acc_train: 0.7348 loss_val: 0.5974 acc_val: 0.7143 time: 0.0098s\n",
      "Epoch: 00412 loss_train: 0.5200 acc_train: 0.7413 loss_val: 0.5917 acc_val: 0.7013 time: 0.0110s\n",
      "Epoch: 00413 loss_train: 0.5393 acc_train: 0.7239 loss_val: 0.5651 acc_val: 0.7273 time: 0.0096s\n",
      "Epoch: 00414 loss_train: 0.5624 acc_train: 0.7152 loss_val: 0.5683 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00415 loss_train: 0.5359 acc_train: 0.7413 loss_val: 0.5438 acc_val: 0.7338 time: 0.0104s\n",
      "Epoch: 00416 loss_train: 0.5205 acc_train: 0.7261 loss_val: 0.5760 acc_val: 0.7273 time: 0.0105s\n",
      "Epoch: 00417 loss_train: 0.5516 acc_train: 0.7304 loss_val: 0.5881 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00418 loss_train: 0.5715 acc_train: 0.7261 loss_val: 0.5807 acc_val: 0.6948 time: 0.0113s\n",
      "Epoch: 00419 loss_train: 0.5409 acc_train: 0.7152 loss_val: 0.5537 acc_val: 0.7338 time: 0.0103s\n",
      "Epoch: 00420 loss_train: 0.5374 acc_train: 0.7435 loss_val: 0.5591 acc_val: 0.7403 time: 0.0130s\n",
      "Epoch: 00421 loss_train: 0.5401 acc_train: 0.7304 loss_val: 0.5393 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00422 loss_train: 0.5413 acc_train: 0.7391 loss_val: 0.5936 acc_val: 0.7013 time: 0.0109s\n",
      "Epoch: 00423 loss_train: 0.5325 acc_train: 0.7326 loss_val: 0.5809 acc_val: 0.6948 time: 0.0098s\n",
      "Epoch: 00424 loss_train: 0.5194 acc_train: 0.7413 loss_val: 0.5325 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00425 loss_train: 0.5568 acc_train: 0.7152 loss_val: 0.5731 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00426 loss_train: 0.5563 acc_train: 0.7304 loss_val: 0.5648 acc_val: 0.7208 time: 0.0095s\n",
      "Epoch: 00427 loss_train: 0.5255 acc_train: 0.7370 loss_val: 0.5512 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00428 loss_train: 0.5356 acc_train: 0.7261 loss_val: 0.5724 acc_val: 0.7143 time: 0.0111s\n",
      "Epoch: 00429 loss_train: 0.5358 acc_train: 0.7326 loss_val: 0.5536 acc_val: 0.7078 time: 0.0127s\n",
      "Epoch: 00430 loss_train: 0.5400 acc_train: 0.7261 loss_val: 0.5450 acc_val: 0.7143 time: 0.0129s\n",
      "Epoch: 00431 loss_train: 0.5443 acc_train: 0.7174 loss_val: 0.5718 acc_val: 0.7273 time: 0.0146s\n",
      "Epoch: 00432 loss_train: 0.5412 acc_train: 0.7304 loss_val: 0.5630 acc_val: 0.7208 time: 0.0189s\n",
      "Epoch: 00433 loss_train: 0.5347 acc_train: 0.7326 loss_val: 0.5990 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00434 loss_train: 0.5345 acc_train: 0.7413 loss_val: 0.5622 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00435 loss_train: 0.5300 acc_train: 0.7261 loss_val: 0.5730 acc_val: 0.6883 time: 0.0111s\n",
      "Epoch: 00436 loss_train: 0.5442 acc_train: 0.7348 loss_val: 0.5622 acc_val: 0.7468 time: 0.0108s\n",
      "Epoch: 00437 loss_train: 0.5229 acc_train: 0.7457 loss_val: 0.5702 acc_val: 0.7338 time: 0.0126s\n",
      "Epoch: 00438 loss_train: 0.5196 acc_train: 0.7435 loss_val: 0.5420 acc_val: 0.7338 time: 0.0120s\n",
      "Epoch: 00439 loss_train: 0.5361 acc_train: 0.7304 loss_val: 0.5352 acc_val: 0.7273 time: 0.0157s\n",
      "Epoch: 00440 loss_train: 0.5503 acc_train: 0.7174 loss_val: 0.5866 acc_val: 0.7078 time: 0.0150s\n",
      "Epoch: 00441 loss_train: 0.5318 acc_train: 0.7413 loss_val: 0.5525 acc_val: 0.7338 time: 0.0131s\n",
      "Epoch: 00442 loss_train: 0.5272 acc_train: 0.7304 loss_val: 0.5825 acc_val: 0.7208 time: 0.0116s\n",
      "Epoch: 00443 loss_train: 0.5296 acc_train: 0.7348 loss_val: 0.5437 acc_val: 0.7468 time: 0.0109s\n",
      "Epoch: 00444 loss_train: 0.5326 acc_train: 0.7391 loss_val: 0.5781 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00445 loss_train: 0.5205 acc_train: 0.7435 loss_val: 0.6278 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00446 loss_train: 0.5436 acc_train: 0.7391 loss_val: 0.6040 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00447 loss_train: 0.5227 acc_train: 0.7435 loss_val: 0.5654 acc_val: 0.7273 time: 0.0115s\n",
      "Epoch: 00448 loss_train: 0.5187 acc_train: 0.7391 loss_val: 0.5445 acc_val: 0.7208 time: 0.0143s\n",
      "Epoch: 00449 loss_train: 0.5319 acc_train: 0.7326 loss_val: 0.5506 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00450 loss_train: 0.5225 acc_train: 0.7370 loss_val: 0.5513 acc_val: 0.7338 time: 0.0116s\n",
      "Epoch: 00451 loss_train: 0.5473 acc_train: 0.7283 loss_val: 0.5518 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00452 loss_train: 0.5392 acc_train: 0.7326 loss_val: 0.5299 acc_val: 0.7338 time: 0.0113s\n",
      "Epoch: 00453 loss_train: 0.5511 acc_train: 0.7109 loss_val: 0.5636 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00454 loss_train: 0.5330 acc_train: 0.7326 loss_val: 0.5599 acc_val: 0.7273 time: 0.0191s\n",
      "Epoch: 00455 loss_train: 0.5334 acc_train: 0.7370 loss_val: 0.5464 acc_val: 0.7273 time: 0.0187s\n",
      "Epoch: 00456 loss_train: 0.5369 acc_train: 0.7478 loss_val: 0.5701 acc_val: 0.7208 time: 0.0160s\n",
      "Epoch: 00457 loss_train: 0.5274 acc_train: 0.7413 loss_val: 0.5805 acc_val: 0.7208 time: 0.0163s\n",
      "Epoch: 00458 loss_train: 0.5362 acc_train: 0.7348 loss_val: 0.5591 acc_val: 0.7208 time: 0.0149s\n",
      "Epoch: 00459 loss_train: 0.5329 acc_train: 0.7391 loss_val: 0.5485 acc_val: 0.7078 time: 0.0118s\n",
      "Epoch: 00460 loss_train: 0.5355 acc_train: 0.7283 loss_val: 0.5678 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00461 loss_train: 0.5265 acc_train: 0.7391 loss_val: 0.5726 acc_val: 0.7078 time: 0.0129s\n",
      "Epoch: 00462 loss_train: 0.5453 acc_train: 0.7435 loss_val: 0.5844 acc_val: 0.7143 time: 0.0155s\n",
      "Epoch: 00463 loss_train: 0.5135 acc_train: 0.7391 loss_val: 0.5457 acc_val: 0.7468 time: 0.0121s\n",
      "Epoch: 00464 loss_train: 0.5407 acc_train: 0.7370 loss_val: 0.5647 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00465 loss_train: 0.5344 acc_train: 0.7457 loss_val: 0.5801 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00466 loss_train: 0.5306 acc_train: 0.7304 loss_val: 0.5479 acc_val: 0.7273 time: 0.0123s\n",
      "Epoch: 00467 loss_train: 0.5275 acc_train: 0.7370 loss_val: 0.5544 acc_val: 0.7403 time: 0.0117s\n",
      "Epoch: 00468 loss_train: 0.5316 acc_train: 0.7348 loss_val: 0.5354 acc_val: 0.7208 time: 0.0151s\n",
      "Epoch: 00469 loss_train: 0.5474 acc_train: 0.7217 loss_val: 0.5692 acc_val: 0.7078 time: 0.0133s\n",
      "Epoch: 00470 loss_train: 0.5282 acc_train: 0.7348 loss_val: 0.5705 acc_val: 0.7403 time: 0.0124s\n",
      "Epoch: 00471 loss_train: 0.5416 acc_train: 0.7283 loss_val: 0.5691 acc_val: 0.7078 time: 0.0140s\n",
      "Epoch: 00472 loss_train: 0.5483 acc_train: 0.7348 loss_val: 0.5365 acc_val: 0.7468 time: 0.0148s\n",
      "Epoch: 00473 loss_train: 0.5254 acc_train: 0.7391 loss_val: 0.5641 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00474 loss_train: 0.5294 acc_train: 0.7391 loss_val: 0.5323 acc_val: 0.7273 time: 0.0131s\n",
      "Epoch: 00475 loss_train: 0.5226 acc_train: 0.7304 loss_val: 0.5732 acc_val: 0.7338 time: 0.0201s\n",
      "Epoch: 00476 loss_train: 0.5370 acc_train: 0.7413 loss_val: 0.5607 acc_val: 0.7273 time: 0.0138s\n",
      "Epoch: 00477 loss_train: 0.5388 acc_train: 0.7283 loss_val: 0.6238 acc_val: 0.6883 time: 0.0124s\n",
      "Epoch: 00478 loss_train: 0.5450 acc_train: 0.7370 loss_val: 0.5544 acc_val: 0.7208 time: 0.0128s\n",
      "Epoch: 00479 loss_train: 0.5329 acc_train: 0.7283 loss_val: 0.5555 acc_val: 0.7273 time: 0.0135s\n",
      "Epoch: 00480 loss_train: 0.5376 acc_train: 0.7435 loss_val: 0.5715 acc_val: 0.7208 time: 0.0125s\n",
      "Epoch: 00481 loss_train: 0.5284 acc_train: 0.7326 loss_val: 0.5815 acc_val: 0.7013 time: 0.0115s\n",
      "Epoch: 00482 loss_train: 0.5444 acc_train: 0.7370 loss_val: 0.5992 acc_val: 0.7078 time: 0.0161s\n",
      "Epoch: 00483 loss_train: 0.5501 acc_train: 0.7391 loss_val: 0.5291 acc_val: 0.7468 time: 0.0115s\n",
      "Epoch: 00484 loss_train: 0.5295 acc_train: 0.7435 loss_val: 0.5651 acc_val: 0.7273 time: 0.0131s\n",
      "Epoch: 00485 loss_train: 0.5295 acc_train: 0.7348 loss_val: 0.5395 acc_val: 0.7403 time: 0.0118s\n",
      "Epoch: 00486 loss_train: 0.5489 acc_train: 0.7239 loss_val: 0.5810 acc_val: 0.7143 time: 0.0120s\n",
      "Epoch: 00487 loss_train: 0.5489 acc_train: 0.7261 loss_val: 0.5650 acc_val: 0.7338 time: 0.0127s\n",
      "Epoch: 00488 loss_train: 0.5324 acc_train: 0.7326 loss_val: 0.5657 acc_val: 0.7143 time: 0.0154s\n",
      "Epoch: 00489 loss_train: 0.5284 acc_train: 0.7413 loss_val: 0.5869 acc_val: 0.7338 time: 0.0124s\n",
      "Epoch: 00490 loss_train: 0.5319 acc_train: 0.7413 loss_val: 0.5620 acc_val: 0.7143 time: 0.0165s\n",
      "Epoch: 00491 loss_train: 0.5531 acc_train: 0.7326 loss_val: 0.5939 acc_val: 0.7143 time: 0.0164s\n",
      "Epoch: 00492 loss_train: 0.5364 acc_train: 0.7217 loss_val: 0.5586 acc_val: 0.7273 time: 0.0118s\n",
      "Epoch: 00493 loss_train: 0.5506 acc_train: 0.7457 loss_val: 0.5747 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00494 loss_train: 0.5210 acc_train: 0.7435 loss_val: 0.5623 acc_val: 0.7273 time: 0.0126s\n",
      "Epoch: 00495 loss_train: 0.5344 acc_train: 0.7326 loss_val: 0.5571 acc_val: 0.7273 time: 0.0125s\n",
      "Epoch: 00496 loss_train: 0.5300 acc_train: 0.7304 loss_val: 0.5626 acc_val: 0.7338 time: 0.0119s\n",
      "Epoch: 00497 loss_train: 0.5417 acc_train: 0.7196 loss_val: 0.5683 acc_val: 0.6948 time: 0.0106s\n",
      "Epoch: 00498 loss_train: 0.5418 acc_train: 0.7370 loss_val: 0.5738 acc_val: 0.7208 time: 0.0131s\n",
      "Epoch: 00499 loss_train: 0.5348 acc_train: 0.7283 loss_val: 0.5828 acc_val: 0.7208 time: 0.0134s\n",
      "Epoch: 00500 loss_train: 0.5347 acc_train: 0.7413 loss_val: 0.5517 acc_val: 0.7208 time: 0.0136s\n",
      "Epoch: 00501 loss_train: 0.5284 acc_train: 0.7413 loss_val: 0.5701 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00502 loss_train: 0.5327 acc_train: 0.7435 loss_val: 0.5536 acc_val: 0.7338 time: 0.0131s\n",
      "Epoch: 00503 loss_train: 0.5296 acc_train: 0.7348 loss_val: 0.5646 acc_val: 0.7208 time: 0.0126s\n",
      "Epoch: 00504 loss_train: 0.5440 acc_train: 0.7435 loss_val: 0.5976 acc_val: 0.7208 time: 0.0141s\n",
      "Epoch: 00505 loss_train: 0.5254 acc_train: 0.7370 loss_val: 0.5624 acc_val: 0.7403 time: 0.0131s\n",
      "Epoch: 00506 loss_train: 0.5327 acc_train: 0.7348 loss_val: 0.5673 acc_val: 0.7208 time: 0.0131s\n",
      "Epoch: 00507 loss_train: 0.5403 acc_train: 0.7326 loss_val: 0.5851 acc_val: 0.7013 time: 0.0110s\n",
      "Epoch: 00508 loss_train: 0.5263 acc_train: 0.7370 loss_val: 0.5746 acc_val: 0.7143 time: 0.0136s\n",
      "Epoch: 00509 loss_train: 0.5417 acc_train: 0.7391 loss_val: 0.5515 acc_val: 0.7078 time: 0.0144s\n",
      "Epoch: 00510 loss_train: 0.5310 acc_train: 0.7348 loss_val: 0.5560 acc_val: 0.7143 time: 0.0131s\n",
      "Epoch: 00511 loss_train: 0.5247 acc_train: 0.7370 loss_val: 0.5674 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00512 loss_train: 0.5204 acc_train: 0.7413 loss_val: 0.5612 acc_val: 0.7403 time: 0.0115s\n",
      "Epoch: 00513 loss_train: 0.5442 acc_train: 0.7391 loss_val: 0.5987 acc_val: 0.7208 time: 0.0130s\n",
      "Epoch: 00514 loss_train: 0.5239 acc_train: 0.7435 loss_val: 0.5899 acc_val: 0.7143 time: 0.0139s\n",
      "Epoch: 00515 loss_train: 0.5311 acc_train: 0.7413 loss_val: 0.5793 acc_val: 0.7338 time: 0.0114s\n",
      "Epoch: 00516 loss_train: 0.5309 acc_train: 0.7348 loss_val: 0.5634 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00517 loss_train: 0.5284 acc_train: 0.7283 loss_val: 0.5655 acc_val: 0.7078 time: 0.0131s\n",
      "Epoch: 00518 loss_train: 0.5333 acc_train: 0.7326 loss_val: 0.5827 acc_val: 0.7208 time: 0.0120s\n",
      "Epoch: 00519 loss_train: 0.5286 acc_train: 0.7370 loss_val: 0.5541 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00520 loss_train: 0.5232 acc_train: 0.7457 loss_val: 0.5740 acc_val: 0.7273 time: 0.0122s\n",
      "Epoch: 00521 loss_train: 0.5348 acc_train: 0.7348 loss_val: 0.5418 acc_val: 0.7338 time: 0.0109s\n",
      "Epoch: 00522 loss_train: 0.5231 acc_train: 0.7391 loss_val: 0.5527 acc_val: 0.7338 time: 0.0124s\n",
      "Epoch: 00523 loss_train: 0.5462 acc_train: 0.7326 loss_val: 0.5289 acc_val: 0.7403 time: 0.0146s\n",
      "Epoch: 00524 loss_train: 0.5383 acc_train: 0.7413 loss_val: 0.5847 acc_val: 0.7208 time: 0.0103s\n",
      "Epoch: 00525 loss_train: 0.5330 acc_train: 0.7435 loss_val: 0.5510 acc_val: 0.7273 time: 0.0109s\n",
      "Epoch: 00526 loss_train: 0.5249 acc_train: 0.7391 loss_val: 0.5295 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00527 loss_train: 0.5325 acc_train: 0.7217 loss_val: 0.5682 acc_val: 0.7078 time: 0.0106s\n",
      "Epoch: 00528 loss_train: 0.5208 acc_train: 0.7391 loss_val: 0.5627 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00529 loss_train: 0.5316 acc_train: 0.7326 loss_val: 0.5592 acc_val: 0.7273 time: 0.0121s\n",
      "Epoch: 00530 loss_train: 0.5407 acc_train: 0.7413 loss_val: 0.5907 acc_val: 0.7338 time: 0.0150s\n",
      "Epoch: 00531 loss_train: 0.5511 acc_train: 0.7196 loss_val: 0.6053 acc_val: 0.7013 time: 0.0131s\n",
      "Epoch: 00532 loss_train: 0.5364 acc_train: 0.7391 loss_val: 0.5652 acc_val: 0.7208 time: 0.0119s\n",
      "Epoch: 00533 loss_train: 0.5234 acc_train: 0.7413 loss_val: 0.5624 acc_val: 0.7273 time: 0.0128s\n",
      "Epoch: 00534 loss_train: 0.5413 acc_train: 0.7152 loss_val: 0.5600 acc_val: 0.7208 time: 0.0127s\n",
      "Epoch: 00535 loss_train: 0.5246 acc_train: 0.7217 loss_val: 0.5717 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00536 loss_train: 0.5361 acc_train: 0.7326 loss_val: 0.5608 acc_val: 0.7078 time: 0.0108s\n",
      "Epoch: 00537 loss_train: 0.5340 acc_train: 0.7522 loss_val: 0.5647 acc_val: 0.7338 time: 0.0139s\n",
      "Epoch: 00538 loss_train: 0.5312 acc_train: 0.7413 loss_val: 0.5821 acc_val: 0.7403 time: 0.0133s\n",
      "Epoch: 00539 loss_train: 0.5586 acc_train: 0.7304 loss_val: 0.5534 acc_val: 0.7338 time: 0.0160s\n",
      "Epoch: 00540 loss_train: 0.5496 acc_train: 0.7391 loss_val: 0.6139 acc_val: 0.6948 time: 0.0127s\n",
      "Epoch: 00541 loss_train: 0.5320 acc_train: 0.7478 loss_val: 0.5833 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00542 loss_train: 0.5388 acc_train: 0.7391 loss_val: 0.5907 acc_val: 0.7208 time: 0.0125s\n",
      "Epoch: 00543 loss_train: 0.5307 acc_train: 0.7391 loss_val: 0.5515 acc_val: 0.7338 time: 0.0143s\n",
      "Epoch: 00544 loss_train: 0.5478 acc_train: 0.7239 loss_val: 0.5872 acc_val: 0.6948 time: 0.0120s\n",
      "Epoch: 00545 loss_train: 0.5426 acc_train: 0.7370 loss_val: 0.5519 acc_val: 0.6883 time: 0.0095s\n",
      "Epoch: 00546 loss_train: 0.5338 acc_train: 0.7413 loss_val: 0.5659 acc_val: 0.7273 time: 0.0115s\n",
      "Epoch: 00547 loss_train: 0.5308 acc_train: 0.7413 loss_val: 0.5676 acc_val: 0.7208 time: 0.0137s\n",
      "Epoch: 00548 loss_train: 0.5324 acc_train: 0.7413 loss_val: 0.5673 acc_val: 0.7273 time: 0.0143s\n",
      "Epoch: 00549 loss_train: 0.5336 acc_train: 0.7522 loss_val: 0.5571 acc_val: 0.7403 time: 0.0153s\n",
      "Epoch: 00550 loss_train: 0.5250 acc_train: 0.7239 loss_val: 0.5726 acc_val: 0.7338 time: 0.0127s\n",
      "Epoch: 00551 loss_train: 0.5393 acc_train: 0.7283 loss_val: 0.5538 acc_val: 0.7273 time: 0.0158s\n",
      "Epoch: 00552 loss_train: 0.5279 acc_train: 0.7413 loss_val: 0.5921 acc_val: 0.7078 time: 0.0154s\n",
      "Epoch: 00553 loss_train: 0.5305 acc_train: 0.7391 loss_val: 0.5556 acc_val: 0.7078 time: 0.0126s\n",
      "Epoch: 00554 loss_train: 0.5423 acc_train: 0.7217 loss_val: 0.5666 acc_val: 0.7338 time: 0.0122s\n",
      "Epoch: 00555 loss_train: 0.5313 acc_train: 0.7304 loss_val: 0.5593 acc_val: 0.7273 time: 0.0141s\n",
      "Epoch: 00556 loss_train: 0.5309 acc_train: 0.7348 loss_val: 0.5467 acc_val: 0.7532 time: 0.0112s\n",
      "Epoch: 00557 loss_train: 0.5501 acc_train: 0.7283 loss_val: 0.5554 acc_val: 0.7208 time: 0.0113s\n",
      "Epoch: 00558 loss_train: 0.5408 acc_train: 0.7435 loss_val: 0.5431 acc_val: 0.7403 time: 0.0119s\n",
      "Epoch: 00559 loss_train: 0.5398 acc_train: 0.7370 loss_val: 0.5383 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00560 loss_train: 0.5416 acc_train: 0.7370 loss_val: 0.5277 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00561 loss_train: 0.5140 acc_train: 0.7457 loss_val: 0.5503 acc_val: 0.7532 time: 0.0111s\n",
      "Epoch: 00562 loss_train: 0.5380 acc_train: 0.7500 loss_val: 0.5745 acc_val: 0.7273 time: 0.0132s\n",
      "Epoch: 00563 loss_train: 0.5313 acc_train: 0.7370 loss_val: 0.5616 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00564 loss_train: 0.5388 acc_train: 0.7304 loss_val: 0.5487 acc_val: 0.7338 time: 0.0127s\n",
      "Epoch: 00565 loss_train: 0.5331 acc_train: 0.7522 loss_val: 0.5445 acc_val: 0.7468 time: 0.0112s\n",
      "Epoch: 00566 loss_train: 0.5291 acc_train: 0.7391 loss_val: 0.5806 acc_val: 0.7273 time: 0.0159s\n",
      "Epoch: 00567 loss_train: 0.5271 acc_train: 0.7435 loss_val: 0.5679 acc_val: 0.7013 time: 0.0135s\n",
      "Epoch: 00568 loss_train: 0.5137 acc_train: 0.7391 loss_val: 0.5425 acc_val: 0.7208 time: 0.0145s\n",
      "Epoch: 00569 loss_train: 0.5327 acc_train: 0.7413 loss_val: 0.5774 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00570 loss_train: 0.5471 acc_train: 0.7435 loss_val: 0.5384 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00571 loss_train: 0.5112 acc_train: 0.7543 loss_val: 0.5516 acc_val: 0.7532 time: 0.0113s\n",
      "Epoch: 00572 loss_train: 0.5284 acc_train: 0.7370 loss_val: 0.5559 acc_val: 0.7273 time: 0.0163s\n",
      "Epoch: 00573 loss_train: 0.5224 acc_train: 0.7435 loss_val: 0.5679 acc_val: 0.7273 time: 0.0143s\n",
      "Epoch: 00574 loss_train: 0.5226 acc_train: 0.7391 loss_val: 0.5459 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00575 loss_train: 0.5381 acc_train: 0.7457 loss_val: 0.5466 acc_val: 0.7403 time: 0.0103s\n",
      "Epoch: 00576 loss_train: 0.5484 acc_train: 0.7326 loss_val: 0.5463 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00577 loss_train: 0.5364 acc_train: 0.7522 loss_val: 0.5773 acc_val: 0.7208 time: 0.0104s\n",
      "Epoch: 00578 loss_train: 0.5303 acc_train: 0.7370 loss_val: 0.5938 acc_val: 0.7403 time: 0.0117s\n",
      "Epoch: 00579 loss_train: 0.5406 acc_train: 0.7370 loss_val: 0.5506 acc_val: 0.7078 time: 0.0111s\n",
      "Epoch: 00580 loss_train: 0.5356 acc_train: 0.7304 loss_val: 0.5616 acc_val: 0.7273 time: 0.0104s\n",
      "Epoch: 00581 loss_train: 0.5274 acc_train: 0.7457 loss_val: 0.5781 acc_val: 0.7078 time: 0.0115s\n",
      "Epoch: 00582 loss_train: 0.5274 acc_train: 0.7500 loss_val: 0.5524 acc_val: 0.7208 time: 0.0114s\n",
      "Epoch: 00583 loss_train: 0.5309 acc_train: 0.7370 loss_val: 0.5624 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00584 loss_train: 0.5069 acc_train: 0.7478 loss_val: 0.5620 acc_val: 0.7208 time: 0.0123s\n",
      "Epoch: 00585 loss_train: 0.5274 acc_train: 0.7435 loss_val: 0.5714 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00586 loss_train: 0.5194 acc_train: 0.7565 loss_val: 0.5874 acc_val: 0.7338 time: 0.0103s\n",
      "Epoch: 00587 loss_train: 0.5403 acc_train: 0.7391 loss_val: 0.5634 acc_val: 0.7273 time: 0.0107s\n",
      "Epoch: 00588 loss_train: 0.5228 acc_train: 0.7457 loss_val: 0.5442 acc_val: 0.7143 time: 0.0097s\n",
      "Epoch: 00589 loss_train: 0.5399 acc_train: 0.7478 loss_val: 0.5645 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00590 loss_train: 0.5258 acc_train: 0.7370 loss_val: 0.5642 acc_val: 0.7208 time: 0.0123s\n",
      "Epoch: 00591 loss_train: 0.5476 acc_train: 0.7239 loss_val: 0.5641 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00592 loss_train: 0.5354 acc_train: 0.7326 loss_val: 0.5377 acc_val: 0.7338 time: 0.0098s\n",
      "Epoch: 00593 loss_train: 0.5481 acc_train: 0.7283 loss_val: 0.5594 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00594 loss_train: 0.5247 acc_train: 0.7326 loss_val: 0.5536 acc_val: 0.7078 time: 0.0091s\n",
      "Epoch: 00595 loss_train: 0.5339 acc_train: 0.7435 loss_val: 0.5513 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00596 loss_train: 0.5346 acc_train: 0.7413 loss_val: 0.5408 acc_val: 0.7338 time: 0.0094s\n",
      "Epoch: 00597 loss_train: 0.5519 acc_train: 0.7391 loss_val: 0.5796 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00598 loss_train: 0.5444 acc_train: 0.7391 loss_val: 0.5818 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00599 loss_train: 0.5354 acc_train: 0.7348 loss_val: 0.5477 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00600 loss_train: 0.5319 acc_train: 0.7435 loss_val: 0.5819 acc_val: 0.7338 time: 0.0135s\n",
      "Epoch: 00601 loss_train: 0.5351 acc_train: 0.7457 loss_val: 0.5570 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00602 loss_train: 0.5309 acc_train: 0.7370 loss_val: 0.5863 acc_val: 0.7273 time: 0.0098s\n",
      "Epoch: 00603 loss_train: 0.5250 acc_train: 0.7261 loss_val: 0.5354 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00604 loss_train: 0.5368 acc_train: 0.7391 loss_val: 0.5686 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00605 loss_train: 0.5303 acc_train: 0.7283 loss_val: 0.5773 acc_val: 0.7013 time: 0.0104s\n",
      "Epoch: 00606 loss_train: 0.5450 acc_train: 0.7065 loss_val: 0.5639 acc_val: 0.7468 time: 0.0097s\n",
      "Epoch: 00607 loss_train: 0.5364 acc_train: 0.7370 loss_val: 0.5577 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00608 loss_train: 0.5264 acc_train: 0.7391 loss_val: 0.5441 acc_val: 0.7078 time: 0.0098s\n",
      "Epoch: 00609 loss_train: 0.5201 acc_train: 0.7457 loss_val: 0.5745 acc_val: 0.7078 time: 0.0088s\n",
      "Epoch: 00610 loss_train: 0.5271 acc_train: 0.7217 loss_val: 0.5565 acc_val: 0.7273 time: 0.0110s\n",
      "Epoch: 00611 loss_train: 0.5416 acc_train: 0.7326 loss_val: 0.5519 acc_val: 0.7208 time: 0.0106s\n",
      "Epoch: 00612 loss_train: 0.5150 acc_train: 0.7478 loss_val: 0.5351 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00613 loss_train: 0.5302 acc_train: 0.7457 loss_val: 0.5576 acc_val: 0.7208 time: 0.0104s\n",
      "Epoch: 00614 loss_train: 0.5179 acc_train: 0.7543 loss_val: 0.5507 acc_val: 0.7078 time: 0.0082s\n",
      "Epoch: 00615 loss_train: 0.5363 acc_train: 0.7261 loss_val: 0.5670 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00616 loss_train: 0.5241 acc_train: 0.7413 loss_val: 0.5473 acc_val: 0.7468 time: 0.0089s\n",
      "Epoch: 00617 loss_train: 0.5293 acc_train: 0.7391 loss_val: 0.5483 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00618 loss_train: 0.5226 acc_train: 0.7413 loss_val: 0.5770 acc_val: 0.7208 time: 0.0089s\n",
      "Epoch: 00619 loss_train: 0.5254 acc_train: 0.7500 loss_val: 0.5318 acc_val: 0.7468 time: 0.0107s\n",
      "Epoch: 00620 loss_train: 0.5444 acc_train: 0.7326 loss_val: 0.5541 acc_val: 0.7338 time: 0.0130s\n",
      "Epoch: 00621 loss_train: 0.5383 acc_train: 0.7370 loss_val: 0.5682 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00622 loss_train: 0.5104 acc_train: 0.7478 loss_val: 0.5874 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00623 loss_train: 0.5365 acc_train: 0.7413 loss_val: 0.5266 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00624 loss_train: 0.5234 acc_train: 0.7348 loss_val: 0.5445 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00625 loss_train: 0.5289 acc_train: 0.7391 loss_val: 0.5503 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00626 loss_train: 0.5273 acc_train: 0.7500 loss_val: 0.5610 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00627 loss_train: 0.5339 acc_train: 0.7391 loss_val: 0.5425 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00628 loss_train: 0.5240 acc_train: 0.7391 loss_val: 0.5679 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00629 loss_train: 0.5372 acc_train: 0.7370 loss_val: 0.5584 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00630 loss_train: 0.5443 acc_train: 0.7391 loss_val: 0.5708 acc_val: 0.7338 time: 0.0133s\n",
      "Epoch: 00631 loss_train: 0.5347 acc_train: 0.7391 loss_val: 0.5479 acc_val: 0.7468 time: 0.0111s\n",
      "Epoch: 00632 loss_train: 0.5267 acc_train: 0.7478 loss_val: 0.5712 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00633 loss_train: 0.5177 acc_train: 0.7500 loss_val: 0.5907 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00634 loss_train: 0.5367 acc_train: 0.7478 loss_val: 0.5550 acc_val: 0.7403 time: 0.0101s\n",
      "Epoch: 00635 loss_train: 0.5338 acc_train: 0.7413 loss_val: 0.5644 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00636 loss_train: 0.5302 acc_train: 0.7391 loss_val: 0.5332 acc_val: 0.7338 time: 0.0088s\n",
      "Epoch: 00637 loss_train: 0.5346 acc_train: 0.7370 loss_val: 0.6008 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00638 loss_train: 0.5415 acc_train: 0.7217 loss_val: 0.5395 acc_val: 0.7338 time: 0.0113s\n",
      "Epoch: 00639 loss_train: 0.5294 acc_train: 0.7304 loss_val: 0.5544 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00640 loss_train: 0.5212 acc_train: 0.7391 loss_val: 0.5683 acc_val: 0.7273 time: 0.0155s\n",
      "Epoch: 00641 loss_train: 0.5233 acc_train: 0.7435 loss_val: 0.5658 acc_val: 0.7208 time: 0.0122s\n",
      "Epoch: 00642 loss_train: 0.5246 acc_train: 0.7391 loss_val: 0.5820 acc_val: 0.7273 time: 0.0116s\n",
      "Epoch: 00643 loss_train: 0.5195 acc_train: 0.7478 loss_val: 0.5681 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00644 loss_train: 0.5289 acc_train: 0.7478 loss_val: 0.5541 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00645 loss_train: 0.5148 acc_train: 0.7304 loss_val: 0.5749 acc_val: 0.6818 time: 0.0102s\n",
      "Epoch: 00646 loss_train: 0.5349 acc_train: 0.7348 loss_val: 0.5478 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00647 loss_train: 0.5226 acc_train: 0.7500 loss_val: 0.5426 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00648 loss_train: 0.5300 acc_train: 0.7391 loss_val: 0.5518 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00649 loss_train: 0.5558 acc_train: 0.7217 loss_val: 0.5433 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00650 loss_train: 0.5143 acc_train: 0.7522 loss_val: 0.5666 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00651 loss_train: 0.5286 acc_train: 0.7348 loss_val: 0.5537 acc_val: 0.7273 time: 0.0106s\n",
      "Epoch: 00652 loss_train: 0.5200 acc_train: 0.7391 loss_val: 0.5428 acc_val: 0.7468 time: 0.0110s\n",
      "Epoch: 00653 loss_train: 0.5236 acc_train: 0.7435 loss_val: 0.5838 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00654 loss_train: 0.5219 acc_train: 0.7457 loss_val: 0.5624 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00655 loss_train: 0.5259 acc_train: 0.7370 loss_val: 0.5728 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00656 loss_train: 0.5332 acc_train: 0.7413 loss_val: 0.6180 acc_val: 0.7208 time: 0.0103s\n",
      "Epoch: 00657 loss_train: 0.5272 acc_train: 0.7348 loss_val: 0.5523 acc_val: 0.7338 time: 0.0097s\n",
      "Epoch: 00658 loss_train: 0.5285 acc_train: 0.7435 loss_val: 0.5569 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00659 loss_train: 0.5353 acc_train: 0.7326 loss_val: 0.5468 acc_val: 0.7273 time: 0.0113s\n",
      "Epoch: 00660 loss_train: 0.5400 acc_train: 0.7261 loss_val: 0.5539 acc_val: 0.7143 time: 0.0105s\n",
      "Epoch: 00661 loss_train: 0.5177 acc_train: 0.7435 loss_val: 0.5573 acc_val: 0.7468 time: 0.0103s\n",
      "Epoch: 00662 loss_train: 0.5160 acc_train: 0.7522 loss_val: 0.5760 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00663 loss_train: 0.5357 acc_train: 0.7370 loss_val: 0.5399 acc_val: 0.7273 time: 0.0097s\n",
      "Epoch: 00664 loss_train: 0.5402 acc_train: 0.7457 loss_val: 0.5859 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00665 loss_train: 0.5569 acc_train: 0.7500 loss_val: 0.5809 acc_val: 0.7143 time: 0.0079s\n",
      "Epoch: 00666 loss_train: 0.5209 acc_train: 0.7478 loss_val: 0.5579 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00667 loss_train: 0.5305 acc_train: 0.7413 loss_val: 0.5542 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00668 loss_train: 0.5240 acc_train: 0.7348 loss_val: 0.5717 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00669 loss_train: 0.5311 acc_train: 0.7304 loss_val: 0.5651 acc_val: 0.7078 time: 0.0080s\n",
      "Epoch: 00670 loss_train: 0.5345 acc_train: 0.7348 loss_val: 0.5430 acc_val: 0.7338 time: 0.0117s\n",
      "Epoch: 00671 loss_train: 0.5330 acc_train: 0.7457 loss_val: 0.5517 acc_val: 0.7532 time: 0.0135s\n",
      "Epoch: 00672 loss_train: 0.5392 acc_train: 0.7370 loss_val: 0.5649 acc_val: 0.7208 time: 0.0104s\n",
      "Epoch: 00673 loss_train: 0.5237 acc_train: 0.7348 loss_val: 0.5615 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00674 loss_train: 0.5231 acc_train: 0.7435 loss_val: 0.5764 acc_val: 0.7403 time: 0.0097s\n",
      "Epoch: 00675 loss_train: 0.5193 acc_train: 0.7413 loss_val: 0.5745 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00676 loss_train: 0.5250 acc_train: 0.7391 loss_val: 0.5564 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00677 loss_train: 0.5352 acc_train: 0.7304 loss_val: 0.5648 acc_val: 0.7208 time: 0.0104s\n",
      "Epoch: 00678 loss_train: 0.5307 acc_train: 0.7391 loss_val: 0.6059 acc_val: 0.7208 time: 0.0087s\n",
      "Epoch: 00679 loss_train: 0.5382 acc_train: 0.7391 loss_val: 0.5901 acc_val: 0.7338 time: 0.0119s\n",
      "Epoch: 00680 loss_train: 0.5347 acc_train: 0.7391 loss_val: 0.5484 acc_val: 0.7143 time: 0.0096s\n",
      "Epoch: 00681 loss_train: 0.5433 acc_train: 0.7478 loss_val: 0.5588 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00682 loss_train: 0.5177 acc_train: 0.7413 loss_val: 0.5571 acc_val: 0.7468 time: 0.0127s\n",
      "Epoch: 00683 loss_train: 0.5254 acc_train: 0.7478 loss_val: 0.5557 acc_val: 0.7273 time: 0.0106s\n",
      "Epoch: 00684 loss_train: 0.5193 acc_train: 0.7457 loss_val: 0.5558 acc_val: 0.7208 time: 0.0112s\n",
      "Epoch: 00685 loss_train: 0.5243 acc_train: 0.7391 loss_val: 0.5606 acc_val: 0.7273 time: 0.0095s\n",
      "Epoch: 00686 loss_train: 0.5340 acc_train: 0.7478 loss_val: 0.5795 acc_val: 0.7273 time: 0.0068s\n",
      "Epoch: 00687 loss_train: 0.5385 acc_train: 0.7500 loss_val: 0.5600 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00688 loss_train: 0.5284 acc_train: 0.7457 loss_val: 0.5677 acc_val: 0.7468 time: 0.0107s\n",
      "Epoch: 00689 loss_train: 0.5284 acc_train: 0.7413 loss_val: 0.5434 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00690 loss_train: 0.5329 acc_train: 0.7457 loss_val: 0.5715 acc_val: 0.7338 time: 0.0130s\n",
      "Epoch: 00691 loss_train: 0.5384 acc_train: 0.7435 loss_val: 0.5598 acc_val: 0.7468 time: 0.0168s\n",
      "Epoch: 00692 loss_train: 0.5283 acc_train: 0.7435 loss_val: 0.5855 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00693 loss_train: 0.5222 acc_train: 0.7370 loss_val: 0.5743 acc_val: 0.7143 time: 0.0097s\n",
      "Epoch: 00694 loss_train: 0.5402 acc_train: 0.7500 loss_val: 0.5634 acc_val: 0.7468 time: 0.0114s\n",
      "Epoch: 00695 loss_train: 0.5324 acc_train: 0.7261 loss_val: 0.5433 acc_val: 0.7532 time: 0.0100s\n",
      "Epoch: 00696 loss_train: 0.5219 acc_train: 0.7522 loss_val: 0.5831 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00697 loss_train: 0.5226 acc_train: 0.7391 loss_val: 0.5402 acc_val: 0.7273 time: 0.0117s\n",
      "Epoch: 00698 loss_train: 0.5302 acc_train: 0.7413 loss_val: 0.5682 acc_val: 0.7338 time: 0.0131s\n",
      "Epoch: 00699 loss_train: 0.5217 acc_train: 0.7391 loss_val: 0.5586 acc_val: 0.7273 time: 0.0161s\n",
      "Epoch: 00700 loss_train: 0.5144 acc_train: 0.7413 loss_val: 0.5865 acc_val: 0.7208 time: 0.0126s\n",
      "Epoch: 00701 loss_train: 0.5321 acc_train: 0.7435 loss_val: 0.5414 acc_val: 0.7273 time: 0.0129s\n",
      "Epoch: 00702 loss_train: 0.5418 acc_train: 0.7348 loss_val: 0.5738 acc_val: 0.6948 time: 0.0128s\n",
      "Epoch: 00703 loss_train: 0.5160 acc_train: 0.7457 loss_val: 0.5438 acc_val: 0.7338 time: 0.0128s\n",
      "Epoch: 00704 loss_train: 0.5250 acc_train: 0.7413 loss_val: 0.5452 acc_val: 0.7208 time: 0.0144s\n",
      "Epoch: 00705 loss_train: 0.5318 acc_train: 0.7478 loss_val: 0.5639 acc_val: 0.7403 time: 0.0165s\n",
      "Epoch: 00706 loss_train: 0.5246 acc_train: 0.7413 loss_val: 0.5482 acc_val: 0.7468 time: 0.0135s\n",
      "Epoch: 00707 loss_train: 0.5250 acc_train: 0.7370 loss_val: 0.5485 acc_val: 0.7273 time: 0.0129s\n",
      "Epoch: 00708 loss_train: 0.5348 acc_train: 0.7370 loss_val: 0.5825 acc_val: 0.7273 time: 0.0157s\n",
      "Epoch: 00709 loss_train: 0.5264 acc_train: 0.7457 loss_val: 0.5797 acc_val: 0.7208 time: 0.0131s\n",
      "Epoch: 00710 loss_train: 0.5251 acc_train: 0.7435 loss_val: 0.5770 acc_val: 0.7143 time: 0.0138s\n",
      "Epoch: 00711 loss_train: 0.5422 acc_train: 0.7391 loss_val: 0.5687 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00712 loss_train: 0.5320 acc_train: 0.7457 loss_val: 0.5619 acc_val: 0.7338 time: 0.0106s\n",
      "Epoch: 00713 loss_train: 0.5176 acc_train: 0.7370 loss_val: 0.5717 acc_val: 0.7273 time: 0.0089s\n",
      "Epoch: 00714 loss_train: 0.5212 acc_train: 0.7478 loss_val: 0.5654 acc_val: 0.7208 time: 0.0106s\n",
      "Epoch: 00715 loss_train: 0.5309 acc_train: 0.7435 loss_val: 0.5663 acc_val: 0.7273 time: 0.0096s\n",
      "Epoch: 00716 loss_train: 0.5194 acc_train: 0.7478 loss_val: 0.5566 acc_val: 0.7338 time: 0.0117s\n",
      "Epoch: 00717 loss_train: 0.5387 acc_train: 0.7370 loss_val: 0.5343 acc_val: 0.7208 time: 0.0133s\n",
      "Epoch: 00718 loss_train: 0.5214 acc_train: 0.7391 loss_val: 0.5722 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00719 loss_train: 0.5247 acc_train: 0.7565 loss_val: 0.5172 acc_val: 0.7468 time: 0.0133s\n",
      "Epoch: 00720 loss_train: 0.5270 acc_train: 0.7435 loss_val: 0.5364 acc_val: 0.7078 time: 0.0112s\n",
      "Epoch: 00721 loss_train: 0.5181 acc_train: 0.7457 loss_val: 0.5442 acc_val: 0.7143 time: 0.0208s\n",
      "Epoch: 00722 loss_train: 0.5250 acc_train: 0.7435 loss_val: 0.5483 acc_val: 0.7338 time: 0.0162s\n",
      "Epoch: 00723 loss_train: 0.5278 acc_train: 0.7413 loss_val: 0.5492 acc_val: 0.7208 time: 0.0116s\n",
      "Epoch: 00724 loss_train: 0.5294 acc_train: 0.7457 loss_val: 0.5432 acc_val: 0.7338 time: 0.0133s\n",
      "Epoch: 00725 loss_train: 0.5279 acc_train: 0.7391 loss_val: 0.5428 acc_val: 0.7403 time: 0.0141s\n",
      "Epoch: 00726 loss_train: 0.5259 acc_train: 0.7326 loss_val: 0.5353 acc_val: 0.7403 time: 0.0121s\n",
      "Epoch: 00727 loss_train: 0.5392 acc_train: 0.7196 loss_val: 0.5724 acc_val: 0.7468 time: 0.0132s\n",
      "Epoch: 00728 loss_train: 0.5315 acc_train: 0.7435 loss_val: 0.5857 acc_val: 0.7078 time: 0.0146s\n",
      "Epoch: 00729 loss_train: 0.5355 acc_train: 0.7370 loss_val: 0.5592 acc_val: 0.7078 time: 0.0115s\n",
      "Epoch: 00730 loss_train: 0.5375 acc_train: 0.7261 loss_val: 0.5658 acc_val: 0.7273 time: 0.0116s\n",
      "Epoch: 00731 loss_train: 0.5271 acc_train: 0.7304 loss_val: 0.5575 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00732 loss_train: 0.5342 acc_train: 0.7348 loss_val: 0.5440 acc_val: 0.7273 time: 0.0109s\n",
      "Epoch: 00733 loss_train: 0.5224 acc_train: 0.7457 loss_val: 0.5736 acc_val: 0.7403 time: 0.0112s\n",
      "Epoch: 00734 loss_train: 0.5225 acc_train: 0.7391 loss_val: 0.5584 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00735 loss_train: 0.5202 acc_train: 0.7500 loss_val: 0.5775 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00736 loss_train: 0.5222 acc_train: 0.7457 loss_val: 0.5678 acc_val: 0.7143 time: 0.0099s\n",
      "Epoch: 00737 loss_train: 0.5263 acc_train: 0.7500 loss_val: 0.5683 acc_val: 0.7338 time: 0.0120s\n",
      "Epoch: 00738 loss_train: 0.5337 acc_train: 0.7435 loss_val: 0.5460 acc_val: 0.7208 time: 0.0086s\n",
      "Epoch: 00739 loss_train: 0.5312 acc_train: 0.7413 loss_val: 0.5608 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00740 loss_train: 0.5214 acc_train: 0.7500 loss_val: 0.5630 acc_val: 0.7468 time: 0.0121s\n",
      "Epoch: 00741 loss_train: 0.5281 acc_train: 0.7370 loss_val: 0.5760 acc_val: 0.7143 time: 0.0112s\n",
      "Epoch: 00742 loss_train: 0.5349 acc_train: 0.7348 loss_val: 0.5703 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00743 loss_train: 0.5303 acc_train: 0.7370 loss_val: 0.5655 acc_val: 0.7273 time: 0.0114s\n",
      "Epoch: 00744 loss_train: 0.5263 acc_train: 0.7370 loss_val: 0.5762 acc_val: 0.7338 time: 0.0098s\n",
      "Epoch: 00745 loss_train: 0.5315 acc_train: 0.7413 loss_val: 0.6050 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00746 loss_train: 0.5168 acc_train: 0.7435 loss_val: 0.5897 acc_val: 0.7013 time: 0.0099s\n",
      "Epoch: 00747 loss_train: 0.5137 acc_train: 0.7478 loss_val: 0.5745 acc_val: 0.7273 time: 0.0115s\n",
      "Epoch: 00748 loss_train: 0.5285 acc_train: 0.7391 loss_val: 0.5299 acc_val: 0.7273 time: 0.0098s\n",
      "Epoch: 00749 loss_train: 0.5282 acc_train: 0.7348 loss_val: 0.5550 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00750 loss_train: 0.5182 acc_train: 0.7413 loss_val: 0.5607 acc_val: 0.7403 time: 0.0097s\n",
      "Epoch: 00751 loss_train: 0.5377 acc_train: 0.7391 loss_val: 0.5418 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00752 loss_train: 0.5415 acc_train: 0.7413 loss_val: 0.5811 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00753 loss_train: 0.5375 acc_train: 0.7435 loss_val: 0.5705 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00754 loss_train: 0.5282 acc_train: 0.7478 loss_val: 0.5600 acc_val: 0.7208 time: 0.0107s\n",
      "Epoch: 00755 loss_train: 0.5255 acc_train: 0.7435 loss_val: 0.5457 acc_val: 0.7403 time: 0.0117s\n",
      "Epoch: 00756 loss_train: 0.5198 acc_train: 0.7457 loss_val: 0.5801 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00757 loss_train: 0.5275 acc_train: 0.7522 loss_val: 0.5632 acc_val: 0.7403 time: 0.0145s\n",
      "Epoch: 00758 loss_train: 0.5159 acc_train: 0.7435 loss_val: 0.5908 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00759 loss_train: 0.5214 acc_train: 0.7457 loss_val: 0.6007 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00760 loss_train: 0.5289 acc_train: 0.7348 loss_val: 0.5392 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00761 loss_train: 0.5166 acc_train: 0.7500 loss_val: 0.5680 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00762 loss_train: 0.5116 acc_train: 0.7478 loss_val: 0.5594 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00763 loss_train: 0.5241 acc_train: 0.7457 loss_val: 0.5876 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00764 loss_train: 0.5278 acc_train: 0.7370 loss_val: 0.5766 acc_val: 0.7013 time: 0.0111s\n",
      "Epoch: 00765 loss_train: 0.5133 acc_train: 0.7413 loss_val: 0.5670 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00766 loss_train: 0.5165 acc_train: 0.7391 loss_val: 0.5577 acc_val: 0.7403 time: 0.0154s\n",
      "Epoch: 00767 loss_train: 0.5132 acc_train: 0.7457 loss_val: 0.5711 acc_val: 0.7273 time: 0.0114s\n",
      "Epoch: 00768 loss_train: 0.5230 acc_train: 0.7457 loss_val: 0.5656 acc_val: 0.7403 time: 0.0113s\n",
      "Epoch: 00769 loss_train: 0.5354 acc_train: 0.7391 loss_val: 0.5311 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00770 loss_train: 0.5084 acc_train: 0.7543 loss_val: 0.5477 acc_val: 0.7403 time: 0.0094s\n",
      "Epoch: 00771 loss_train: 0.5291 acc_train: 0.7457 loss_val: 0.5751 acc_val: 0.7403 time: 0.0103s\n",
      "Epoch: 00772 loss_train: 0.5306 acc_train: 0.7457 loss_val: 0.5700 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00773 loss_train: 0.5311 acc_train: 0.7522 loss_val: 0.5673 acc_val: 0.7208 time: 0.0095s\n",
      "Epoch: 00774 loss_train: 0.5308 acc_train: 0.7391 loss_val: 0.5634 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00775 loss_train: 0.5258 acc_train: 0.7478 loss_val: 0.5527 acc_val: 0.7208 time: 0.0076s\n",
      "Epoch: 00776 loss_train: 0.5237 acc_train: 0.7457 loss_val: 0.5730 acc_val: 0.7208 time: 0.0128s\n",
      "Epoch: 00777 loss_train: 0.5335 acc_train: 0.7435 loss_val: 0.5905 acc_val: 0.7403 time: 0.0101s\n",
      "Epoch: 00778 loss_train: 0.5203 acc_train: 0.7457 loss_val: 0.5742 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00779 loss_train: 0.5247 acc_train: 0.7413 loss_val: 0.5391 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00780 loss_train: 0.5225 acc_train: 0.7435 loss_val: 0.5813 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00781 loss_train: 0.5355 acc_train: 0.7457 loss_val: 0.5745 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00782 loss_train: 0.5248 acc_train: 0.7435 loss_val: 0.5519 acc_val: 0.7338 time: 0.0104s\n",
      "Epoch: 00783 loss_train: 0.5434 acc_train: 0.7370 loss_val: 0.5686 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00784 loss_train: 0.5199 acc_train: 0.7391 loss_val: 0.5351 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00785 loss_train: 0.5269 acc_train: 0.7457 loss_val: 0.5814 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00786 loss_train: 0.5221 acc_train: 0.7413 loss_val: 0.5756 acc_val: 0.7273 time: 0.0106s\n",
      "Epoch: 00787 loss_train: 0.5277 acc_train: 0.7413 loss_val: 0.5636 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00788 loss_train: 0.5298 acc_train: 0.7457 loss_val: 0.5584 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00789 loss_train: 0.5229 acc_train: 0.7478 loss_val: 0.5857 acc_val: 0.7338 time: 0.0119s\n",
      "Epoch: 00790 loss_train: 0.5204 acc_train: 0.7478 loss_val: 0.5707 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00791 loss_train: 0.5275 acc_train: 0.7457 loss_val: 0.5471 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00792 loss_train: 0.5259 acc_train: 0.7348 loss_val: 0.5426 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00793 loss_train: 0.5258 acc_train: 0.7413 loss_val: 0.5341 acc_val: 0.7273 time: 0.0104s\n",
      "Epoch: 00794 loss_train: 0.5300 acc_train: 0.7348 loss_val: 0.5741 acc_val: 0.7143 time: 0.0102s\n",
      "Epoch: 00795 loss_train: 0.5214 acc_train: 0.7522 loss_val: 0.5788 acc_val: 0.7143 time: 0.0112s\n",
      "Epoch: 00796 loss_train: 0.5289 acc_train: 0.7478 loss_val: 0.5664 acc_val: 0.7078 time: 0.0104s\n",
      "Epoch: 00797 loss_train: 0.5318 acc_train: 0.7457 loss_val: 0.5768 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00798 loss_train: 0.5197 acc_train: 0.7457 loss_val: 0.5621 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00799 loss_train: 0.5202 acc_train: 0.7457 loss_val: 0.5535 acc_val: 0.7338 time: 0.0117s\n",
      "Epoch: 00800 loss_train: 0.5232 acc_train: 0.7457 loss_val: 0.5861 acc_val: 0.7208 time: 0.0118s\n",
      "Epoch: 00801 loss_train: 0.5327 acc_train: 0.7500 loss_val: 0.5544 acc_val: 0.7273 time: 0.0098s\n",
      "Epoch: 00802 loss_train: 0.5227 acc_train: 0.7413 loss_val: 0.5550 acc_val: 0.7273 time: 0.0094s\n",
      "Epoch: 00803 loss_train: 0.5360 acc_train: 0.7348 loss_val: 0.5677 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00804 loss_train: 0.5248 acc_train: 0.7543 loss_val: 0.5700 acc_val: 0.7338 time: 0.0116s\n",
      "Epoch: 00805 loss_train: 0.5225 acc_train: 0.7304 loss_val: 0.5883 acc_val: 0.7013 time: 0.0109s\n",
      "Epoch: 00806 loss_train: 0.5320 acc_train: 0.7348 loss_val: 0.5576 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00807 loss_train: 0.5332 acc_train: 0.7326 loss_val: 0.5386 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00808 loss_train: 0.5295 acc_train: 0.7326 loss_val: 0.5482 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00809 loss_train: 0.5164 acc_train: 0.7457 loss_val: 0.5655 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00810 loss_train: 0.5229 acc_train: 0.7370 loss_val: 0.5513 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00811 loss_train: 0.5398 acc_train: 0.7348 loss_val: 0.5563 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00812 loss_train: 0.5181 acc_train: 0.7457 loss_val: 0.5626 acc_val: 0.7273 time: 0.0092s\n",
      "Epoch: 00813 loss_train: 0.5189 acc_train: 0.7457 loss_val: 0.5941 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00814 loss_train: 0.5142 acc_train: 0.7478 loss_val: 0.5677 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00815 loss_train: 0.5347 acc_train: 0.7500 loss_val: 0.5451 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00816 loss_train: 0.5290 acc_train: 0.7565 loss_val: 0.5805 acc_val: 0.7143 time: 0.0167s\n",
      "Epoch: 00817 loss_train: 0.5173 acc_train: 0.7457 loss_val: 0.5791 acc_val: 0.7403 time: 0.0113s\n",
      "Epoch: 00818 loss_train: 0.5202 acc_train: 0.7500 loss_val: 0.5846 acc_val: 0.7078 time: 0.0081s\n",
      "Epoch: 00819 loss_train: 0.5118 acc_train: 0.7435 loss_val: 0.5426 acc_val: 0.7143 time: 0.0099s\n",
      "Epoch: 00820 loss_train: 0.5119 acc_train: 0.7522 loss_val: 0.5464 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00821 loss_train: 0.5166 acc_train: 0.7457 loss_val: 0.5635 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00822 loss_train: 0.5202 acc_train: 0.7435 loss_val: 0.5391 acc_val: 0.7078 time: 0.0070s\n",
      "Epoch: 00823 loss_train: 0.5212 acc_train: 0.7391 loss_val: 0.5357 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00824 loss_train: 0.5152 acc_train: 0.7522 loss_val: 0.5605 acc_val: 0.7468 time: 0.0110s\n",
      "Epoch: 00825 loss_train: 0.5200 acc_train: 0.7500 loss_val: 0.5733 acc_val: 0.7468 time: 0.0099s\n",
      "Epoch: 00826 loss_train: 0.5296 acc_train: 0.7435 loss_val: 0.5841 acc_val: 0.7273 time: 0.0113s\n",
      "Epoch: 00827 loss_train: 0.5214 acc_train: 0.7435 loss_val: 0.5924 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00828 loss_train: 0.5425 acc_train: 0.7348 loss_val: 0.5775 acc_val: 0.7013 time: 0.0097s\n",
      "Epoch: 00829 loss_train: 0.5250 acc_train: 0.7326 loss_val: 0.5702 acc_val: 0.7403 time: 0.0103s\n",
      "Epoch: 00830 loss_train: 0.5192 acc_train: 0.7500 loss_val: 0.5709 acc_val: 0.7273 time: 0.0095s\n",
      "Epoch: 00831 loss_train: 0.5173 acc_train: 0.7478 loss_val: 0.5520 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00832 loss_train: 0.5245 acc_train: 0.7413 loss_val: 0.5776 acc_val: 0.7403 time: 0.0106s\n",
      "Epoch: 00833 loss_train: 0.5283 acc_train: 0.7435 loss_val: 0.5902 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00834 loss_train: 0.5365 acc_train: 0.7435 loss_val: 0.5744 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00835 loss_train: 0.5071 acc_train: 0.7457 loss_val: 0.5675 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00836 loss_train: 0.5166 acc_train: 0.7413 loss_val: 0.5578 acc_val: 0.7338 time: 0.0129s\n",
      "Epoch: 00837 loss_train: 0.5250 acc_train: 0.7457 loss_val: 0.5555 acc_val: 0.7403 time: 0.0139s\n",
      "Epoch: 00838 loss_train: 0.5268 acc_train: 0.7457 loss_val: 0.5681 acc_val: 0.7468 time: 0.0107s\n",
      "Epoch: 00839 loss_train: 0.5188 acc_train: 0.7478 loss_val: 0.5441 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00840 loss_train: 0.5329 acc_train: 0.7391 loss_val: 0.5773 acc_val: 0.7338 time: 0.0119s\n",
      "Epoch: 00841 loss_train: 0.5332 acc_train: 0.7391 loss_val: 0.5467 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00842 loss_train: 0.5306 acc_train: 0.7370 loss_val: 0.5755 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00843 loss_train: 0.5254 acc_train: 0.7435 loss_val: 0.6044 acc_val: 0.7273 time: 0.0117s\n",
      "Epoch: 00844 loss_train: 0.5306 acc_train: 0.7457 loss_val: 0.5796 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00845 loss_train: 0.5235 acc_train: 0.7522 loss_val: 0.5286 acc_val: 0.7403 time: 0.0099s\n",
      "Epoch: 00846 loss_train: 0.5198 acc_train: 0.7478 loss_val: 0.5685 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00847 loss_train: 0.5036 acc_train: 0.7500 loss_val: 0.5719 acc_val: 0.7338 time: 0.0118s\n",
      "Epoch: 00848 loss_train: 0.5194 acc_train: 0.7500 loss_val: 0.5516 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00849 loss_train: 0.5346 acc_train: 0.7457 loss_val: 0.5567 acc_val: 0.7338 time: 0.0083s\n",
      "Epoch: 00850 loss_train: 0.5207 acc_train: 0.7413 loss_val: 0.5719 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00851 loss_train: 0.5222 acc_train: 0.7478 loss_val: 0.5719 acc_val: 0.7338 time: 0.0106s\n",
      "Epoch: 00852 loss_train: 0.5200 acc_train: 0.7478 loss_val: 0.5584 acc_val: 0.7338 time: 0.0095s\n",
      "Epoch: 00853 loss_train: 0.5190 acc_train: 0.7370 loss_val: 0.5416 acc_val: 0.7403 time: 0.0088s\n",
      "Epoch: 00854 loss_train: 0.5400 acc_train: 0.7391 loss_val: 0.5751 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00855 loss_train: 0.5153 acc_train: 0.7543 loss_val: 0.5489 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00856 loss_train: 0.5281 acc_train: 0.7457 loss_val: 0.5552 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00857 loss_train: 0.5289 acc_train: 0.7522 loss_val: 0.5888 acc_val: 0.7078 time: 0.0136s\n",
      "Epoch: 00858 loss_train: 0.5278 acc_train: 0.7478 loss_val: 0.5602 acc_val: 0.7338 time: 0.0099s\n",
      "Epoch: 00859 loss_train: 0.5278 acc_train: 0.7478 loss_val: 0.5654 acc_val: 0.7403 time: 0.0091s\n",
      "Epoch: 00860 loss_train: 0.5176 acc_train: 0.7435 loss_val: 0.5562 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00861 loss_train: 0.5300 acc_train: 0.7261 loss_val: 0.5431 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00862 loss_train: 0.5172 acc_train: 0.7413 loss_val: 0.5674 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00863 loss_train: 0.5178 acc_train: 0.7435 loss_val: 0.5797 acc_val: 0.7013 time: 0.0115s\n",
      "Epoch: 00864 loss_train: 0.5207 acc_train: 0.7435 loss_val: 0.5580 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00865 loss_train: 0.5229 acc_train: 0.7413 loss_val: 0.5453 acc_val: 0.7208 time: 0.0092s\n",
      "Epoch: 00866 loss_train: 0.5141 acc_train: 0.7500 loss_val: 0.5924 acc_val: 0.7208 time: 0.0089s\n",
      "Epoch: 00867 loss_train: 0.5214 acc_train: 0.7457 loss_val: 0.5917 acc_val: 0.7403 time: 0.0111s\n",
      "Epoch: 00868 loss_train: 0.5213 acc_train: 0.7478 loss_val: 0.5742 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00869 loss_train: 0.5142 acc_train: 0.7457 loss_val: 0.5398 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00870 loss_train: 0.5344 acc_train: 0.7478 loss_val: 0.5442 acc_val: 0.7273 time: 0.0083s\n",
      "Epoch: 00871 loss_train: 0.5196 acc_train: 0.7587 loss_val: 0.5555 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00872 loss_train: 0.5042 acc_train: 0.7413 loss_val: 0.5593 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00873 loss_train: 0.5128 acc_train: 0.7457 loss_val: 0.5558 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00874 loss_train: 0.5414 acc_train: 0.7500 loss_val: 0.6040 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00875 loss_train: 0.5156 acc_train: 0.7522 loss_val: 0.5258 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00876 loss_train: 0.5279 acc_train: 0.7435 loss_val: 0.5770 acc_val: 0.7403 time: 0.0096s\n",
      "Epoch: 00877 loss_train: 0.5284 acc_train: 0.7543 loss_val: 0.5641 acc_val: 0.7208 time: 0.0115s\n",
      "Epoch: 00878 loss_train: 0.5140 acc_train: 0.7478 loss_val: 0.5863 acc_val: 0.7143 time: 0.0117s\n",
      "Epoch: 00879 loss_train: 0.5277 acc_train: 0.7435 loss_val: 0.5725 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00880 loss_train: 0.5061 acc_train: 0.7413 loss_val: 0.5930 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00881 loss_train: 0.5234 acc_train: 0.7457 loss_val: 0.5675 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00882 loss_train: 0.5226 acc_train: 0.7543 loss_val: 0.5645 acc_val: 0.7403 time: 0.0112s\n",
      "Epoch: 00883 loss_train: 0.5261 acc_train: 0.7326 loss_val: 0.5779 acc_val: 0.7013 time: 0.0102s\n",
      "Epoch: 00884 loss_train: 0.5328 acc_train: 0.7457 loss_val: 0.5702 acc_val: 0.7273 time: 0.0119s\n",
      "Epoch: 00885 loss_train: 0.5239 acc_train: 0.7435 loss_val: 0.5515 acc_val: 0.7403 time: 0.0127s\n",
      "Epoch: 00886 loss_train: 0.5217 acc_train: 0.7543 loss_val: 0.5755 acc_val: 0.7338 time: 0.0098s\n",
      "Epoch: 00887 loss_train: 0.5289 acc_train: 0.7478 loss_val: 0.5531 acc_val: 0.7338 time: 0.0121s\n",
      "Epoch: 00888 loss_train: 0.5335 acc_train: 0.7457 loss_val: 0.5655 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00889 loss_train: 0.5249 acc_train: 0.7435 loss_val: 0.5545 acc_val: 0.7143 time: 0.0098s\n",
      "Epoch: 00890 loss_train: 0.5175 acc_train: 0.7478 loss_val: 0.5734 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00891 loss_train: 0.5086 acc_train: 0.7522 loss_val: 0.5793 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00892 loss_train: 0.5278 acc_train: 0.7413 loss_val: 0.5902 acc_val: 0.7208 time: 0.0085s\n",
      "Epoch: 00893 loss_train: 0.5394 acc_train: 0.7391 loss_val: 0.5662 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00894 loss_train: 0.5269 acc_train: 0.7413 loss_val: 0.5349 acc_val: 0.7403 time: 0.0097s\n",
      "Epoch: 00895 loss_train: 0.5281 acc_train: 0.7348 loss_val: 0.5427 acc_val: 0.7273 time: 0.0114s\n",
      "Epoch: 00896 loss_train: 0.5418 acc_train: 0.7370 loss_val: 0.5704 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00897 loss_train: 0.5219 acc_train: 0.7304 loss_val: 0.5806 acc_val: 0.7078 time: 0.0115s\n",
      "Epoch: 00898 loss_train: 0.5250 acc_train: 0.7413 loss_val: 0.5455 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00899 loss_train: 0.5190 acc_train: 0.7435 loss_val: 0.5621 acc_val: 0.7468 time: 0.0115s\n",
      "Epoch: 00900 loss_train: 0.5284 acc_train: 0.7413 loss_val: 0.5513 acc_val: 0.7338 time: 0.0198s\n",
      "Epoch: 00901 loss_train: 0.5320 acc_train: 0.7413 loss_val: 0.5764 acc_val: 0.7468 time: 0.0216s\n",
      "Epoch: 00902 loss_train: 0.5307 acc_train: 0.7435 loss_val: 0.5781 acc_val: 0.7143 time: 0.0170s\n",
      "Epoch: 00903 loss_train: 0.5299 acc_train: 0.7500 loss_val: 0.5630 acc_val: 0.7143 time: 0.0131s\n",
      "Epoch: 00904 loss_train: 0.5235 acc_train: 0.7435 loss_val: 0.5652 acc_val: 0.7403 time: 0.0186s\n",
      "Epoch: 00905 loss_train: 0.5341 acc_train: 0.7304 loss_val: 0.5515 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00906 loss_train: 0.5195 acc_train: 0.7457 loss_val: 0.5582 acc_val: 0.7143 time: 0.0142s\n",
      "Epoch: 00907 loss_train: 0.5265 acc_train: 0.7326 loss_val: 0.5422 acc_val: 0.7208 time: 0.0189s\n",
      "Epoch: 00908 loss_train: 0.5261 acc_train: 0.7413 loss_val: 0.5574 acc_val: 0.7403 time: 0.0111s\n",
      "Epoch: 00909 loss_train: 0.5181 acc_train: 0.7435 loss_val: 0.5467 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00910 loss_train: 0.5239 acc_train: 0.7478 loss_val: 0.5615 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00911 loss_train: 0.5296 acc_train: 0.7435 loss_val: 0.5632 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00912 loss_train: 0.5293 acc_train: 0.7478 loss_val: 0.5794 acc_val: 0.7078 time: 0.0103s\n",
      "Epoch: 00913 loss_train: 0.5342 acc_train: 0.7457 loss_val: 0.5792 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00914 loss_train: 0.5227 acc_train: 0.7370 loss_val: 0.5776 acc_val: 0.7273 time: 0.0121s\n",
      "Epoch: 00915 loss_train: 0.5296 acc_train: 0.7522 loss_val: 0.5732 acc_val: 0.7403 time: 0.0124s\n",
      "Epoch: 00916 loss_train: 0.5251 acc_train: 0.7500 loss_val: 0.5437 acc_val: 0.7338 time: 0.0190s\n",
      "Epoch: 00917 loss_train: 0.5241 acc_train: 0.7457 loss_val: 0.5764 acc_val: 0.7338 time: 0.0114s\n",
      "Epoch: 00918 loss_train: 0.5285 acc_train: 0.7370 loss_val: 0.5836 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00919 loss_train: 0.5358 acc_train: 0.7457 loss_val: 0.5590 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00920 loss_train: 0.5288 acc_train: 0.7348 loss_val: 0.5598 acc_val: 0.7338 time: 0.0112s\n",
      "Epoch: 00921 loss_train: 0.5415 acc_train: 0.7435 loss_val: 0.5699 acc_val: 0.7273 time: 0.0114s\n",
      "Epoch: 00922 loss_train: 0.5297 acc_train: 0.7348 loss_val: 0.5435 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00923 loss_train: 0.5312 acc_train: 0.7370 loss_val: 0.5602 acc_val: 0.7273 time: 0.0117s\n",
      "Epoch: 00924 loss_train: 0.5469 acc_train: 0.7370 loss_val: 0.5740 acc_val: 0.7208 time: 0.0116s\n",
      "Epoch: 00925 loss_train: 0.5339 acc_train: 0.7391 loss_val: 0.5559 acc_val: 0.7403 time: 0.0144s\n",
      "Epoch: 00926 loss_train: 0.5208 acc_train: 0.7391 loss_val: 0.5925 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00927 loss_train: 0.5379 acc_train: 0.7435 loss_val: 0.6270 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00928 loss_train: 0.5339 acc_train: 0.7478 loss_val: 0.5657 acc_val: 0.7338 time: 0.0116s\n",
      "Epoch: 00929 loss_train: 0.5293 acc_train: 0.7522 loss_val: 0.5439 acc_val: 0.7078 time: 0.0110s\n",
      "Epoch: 00930 loss_train: 0.5378 acc_train: 0.7370 loss_val: 0.5564 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00931 loss_train: 0.5271 acc_train: 0.7370 loss_val: 0.5397 acc_val: 0.7208 time: 0.0126s\n",
      "Epoch: 00932 loss_train: 0.5309 acc_train: 0.7435 loss_val: 0.5772 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00933 loss_train: 0.5278 acc_train: 0.7435 loss_val: 0.5644 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00934 loss_train: 0.5241 acc_train: 0.7500 loss_val: 0.5585 acc_val: 0.7338 time: 0.0164s\n",
      "Epoch: 00935 loss_train: 0.5176 acc_train: 0.7326 loss_val: 0.5771 acc_val: 0.7403 time: 0.0118s\n",
      "Epoch: 00936 loss_train: 0.5370 acc_train: 0.7326 loss_val: 0.5436 acc_val: 0.7403 time: 0.0119s\n",
      "Epoch: 00937 loss_train: 0.5250 acc_train: 0.7457 loss_val: 0.5531 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00938 loss_train: 0.5376 acc_train: 0.7326 loss_val: 0.5666 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00939 loss_train: 0.5293 acc_train: 0.7413 loss_val: 0.5529 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00940 loss_train: 0.5384 acc_train: 0.7413 loss_val: 0.5806 acc_val: 0.7208 time: 0.0110s\n",
      "Epoch: 00941 loss_train: 0.5230 acc_train: 0.7435 loss_val: 0.5697 acc_val: 0.7143 time: 0.0106s\n",
      "Epoch: 00942 loss_train: 0.5276 acc_train: 0.7304 loss_val: 0.5591 acc_val: 0.7273 time: 0.0111s\n",
      "Epoch: 00943 loss_train: 0.5268 acc_train: 0.7348 loss_val: 0.5686 acc_val: 0.7208 time: 0.0137s\n",
      "Epoch: 00944 loss_train: 0.5153 acc_train: 0.7304 loss_val: 0.5830 acc_val: 0.7338 time: 0.0117s\n",
      "Epoch: 00945 loss_train: 0.5293 acc_train: 0.7457 loss_val: 0.5797 acc_val: 0.7208 time: 0.0107s\n",
      "Epoch: 00946 loss_train: 0.5319 acc_train: 0.7348 loss_val: 0.5398 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00947 loss_train: 0.5442 acc_train: 0.7457 loss_val: 0.5976 acc_val: 0.7273 time: 0.0121s\n",
      "Epoch: 00948 loss_train: 0.5273 acc_train: 0.7457 loss_val: 0.5727 acc_val: 0.7338 time: 0.0125s\n",
      "Epoch: 00949 loss_train: 0.5262 acc_train: 0.7457 loss_val: 0.5755 acc_val: 0.7273 time: 0.0097s\n",
      "Epoch: 00950 loss_train: 0.5174 acc_train: 0.7326 loss_val: 0.5409 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00951 loss_train: 0.5267 acc_train: 0.7435 loss_val: 0.5505 acc_val: 0.7143 time: 0.0118s\n",
      "Epoch: 00952 loss_train: 0.5338 acc_train: 0.7500 loss_val: 0.5689 acc_val: 0.7338 time: 0.0129s\n",
      "Epoch: 00953 loss_train: 0.5161 acc_train: 0.7413 loss_val: 0.5840 acc_val: 0.7273 time: 0.0119s\n",
      "Epoch: 00954 loss_train: 0.5399 acc_train: 0.7478 loss_val: 0.5626 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00955 loss_train: 0.5195 acc_train: 0.7500 loss_val: 0.5623 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00956 loss_train: 0.5228 acc_train: 0.7500 loss_val: 0.5448 acc_val: 0.7403 time: 0.0110s\n",
      "Epoch: 00957 loss_train: 0.5322 acc_train: 0.7478 loss_val: 0.5918 acc_val: 0.7273 time: 0.0088s\n",
      "Epoch: 00958 loss_train: 0.5160 acc_train: 0.7457 loss_val: 0.5884 acc_val: 0.7273 time: 0.0094s\n",
      "Epoch: 00959 loss_train: 0.5216 acc_train: 0.7435 loss_val: 0.5652 acc_val: 0.7338 time: 0.0136s\n",
      "Epoch: 00960 loss_train: 0.5275 acc_train: 0.7457 loss_val: 0.6047 acc_val: 0.7338 time: 0.0128s\n",
      "Epoch: 00961 loss_train: 0.5325 acc_train: 0.7457 loss_val: 0.5782 acc_val: 0.7273 time: 0.0126s\n",
      "Epoch: 00962 loss_train: 0.5245 acc_train: 0.7370 loss_val: 0.5799 acc_val: 0.7208 time: 0.0108s\n",
      "Epoch: 00963 loss_train: 0.5097 acc_train: 0.7457 loss_val: 0.5788 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00964 loss_train: 0.5380 acc_train: 0.7435 loss_val: 0.5579 acc_val: 0.7338 time: 0.0107s\n",
      "Epoch: 00965 loss_train: 0.5247 acc_train: 0.7413 loss_val: 0.5708 acc_val: 0.7403 time: 0.0119s\n",
      "Epoch: 00966 loss_train: 0.5264 acc_train: 0.7478 loss_val: 0.5752 acc_val: 0.7403 time: 0.0135s\n",
      "Epoch: 00967 loss_train: 0.5264 acc_train: 0.7413 loss_val: 0.5541 acc_val: 0.7338 time: 0.0106s\n",
      "Epoch: 00968 loss_train: 0.5188 acc_train: 0.7435 loss_val: 0.5581 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00969 loss_train: 0.5254 acc_train: 0.7500 loss_val: 0.5719 acc_val: 0.7403 time: 0.0113s\n",
      "Epoch: 00970 loss_train: 0.5302 acc_train: 0.7370 loss_val: 0.5619 acc_val: 0.7143 time: 0.0126s\n",
      "Epoch: 00971 loss_train: 0.5215 acc_train: 0.7370 loss_val: 0.5666 acc_val: 0.7273 time: 0.0119s\n",
      "Epoch: 00972 loss_train: 0.5125 acc_train: 0.7413 loss_val: 0.5687 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00973 loss_train: 0.5213 acc_train: 0.7478 loss_val: 0.5354 acc_val: 0.7403 time: 0.0118s\n",
      "Epoch: 00974 loss_train: 0.5346 acc_train: 0.7391 loss_val: 0.5388 acc_val: 0.7403 time: 0.0110s\n",
      "Epoch: 00975 loss_train: 0.5127 acc_train: 0.7500 loss_val: 0.5530 acc_val: 0.7273 time: 0.0098s\n",
      "Epoch: 00976 loss_train: 0.5295 acc_train: 0.7457 loss_val: 0.5895 acc_val: 0.7273 time: 0.0112s\n",
      "Epoch: 00977 loss_train: 0.5178 acc_train: 0.7457 loss_val: 0.5902 acc_val: 0.7338 time: 0.0099s\n",
      "Epoch: 00978 loss_train: 0.5326 acc_train: 0.7391 loss_val: 0.5454 acc_val: 0.7338 time: 0.0117s\n",
      "Epoch: 00979 loss_train: 0.5207 acc_train: 0.7457 loss_val: 0.5632 acc_val: 0.7338 time: 0.0119s\n",
      "Epoch: 00980 loss_train: 0.5259 acc_train: 0.7457 loss_val: 0.5777 acc_val: 0.7338 time: 0.0145s\n",
      "Epoch: 00981 loss_train: 0.5387 acc_train: 0.7326 loss_val: 0.5463 acc_val: 0.7143 time: 0.0108s\n",
      "Epoch: 00982 loss_train: 0.5217 acc_train: 0.7478 loss_val: 0.5789 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00983 loss_train: 0.5398 acc_train: 0.7326 loss_val: 0.5579 acc_val: 0.7273 time: 0.0110s\n",
      "Epoch: 00984 loss_train: 0.5343 acc_train: 0.7435 loss_val: 0.5383 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00985 loss_train: 0.5550 acc_train: 0.7217 loss_val: 0.6126 acc_val: 0.7403 time: 0.0119s\n",
      "Epoch: 00986 loss_train: 0.5387 acc_train: 0.7283 loss_val: 0.5375 acc_val: 0.7143 time: 0.0122s\n",
      "Epoch: 00987 loss_train: 0.5377 acc_train: 0.7391 loss_val: 0.5758 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00988 loss_train: 0.5266 acc_train: 0.7457 loss_val: 0.5467 acc_val: 0.7338 time: 0.0139s\n",
      "Epoch: 00989 loss_train: 0.5290 acc_train: 0.7435 loss_val: 0.5469 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00990 loss_train: 0.5269 acc_train: 0.7391 loss_val: 0.5316 acc_val: 0.7338 time: 0.0161s\n",
      "Epoch: 00991 loss_train: 0.5177 acc_train: 0.7413 loss_val: 0.5557 acc_val: 0.7338 time: 0.0138s\n",
      "Epoch: 00992 loss_train: 0.5277 acc_train: 0.7457 loss_val: 0.5752 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00993 loss_train: 0.5242 acc_train: 0.7500 loss_val: 0.5433 acc_val: 0.7143 time: 0.0104s\n",
      "Epoch: 00994 loss_train: 0.5378 acc_train: 0.7348 loss_val: 0.5685 acc_val: 0.7273 time: 0.0116s\n",
      "Epoch: 00995 loss_train: 0.5230 acc_train: 0.7413 loss_val: 0.5651 acc_val: 0.7273 time: 0.0116s\n",
      "Epoch: 00996 loss_train: 0.5335 acc_train: 0.7500 loss_val: 0.5691 acc_val: 0.7273 time: 0.0108s\n",
      "Epoch: 00997 loss_train: 0.5259 acc_train: 0.7413 loss_val: 0.5634 acc_val: 0.7338 time: 0.0128s\n",
      "Epoch: 00998 loss_train: 0.5245 acc_train: 0.7478 loss_val: 0.5543 acc_val: 0.7403 time: 0.0115s\n",
      "Epoch: 00999 loss_train: 0.5341 acc_train: 0.7391 loss_val: 0.5630 acc_val: 0.7078 time: 0.0116s\n",
      "Epoch: 01000 loss_train: 0.5275 acc_train: 0.7435 loss_val: 0.5321 acc_val: 0.7468 time: 0.0102s\n"
     ]
    }
   ],
   "source": [
    "acc_trains = []\n",
    "acc_vals = []\n",
    "loss_trains = []\n",
    "loss_vals = []\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "        acc_train, acc_val, loss_train, loss_val = train(epoch)\n",
    "        acc_trains.append(acc_train)\n",
    "        acc_vals.append(acc_val)\n",
    "        loss_trains.append(loss_train)\n",
    "        loss_vals.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSMOTE's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0235593318939209,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37eb25bc9dc41789d4d69aa6ec5509e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 2.8455 loss_rec: 2.8455 acc_train: 0.3478 loss_val: 2.9772 acc_val: 0.3182 time: 0.0158s\n",
      "Epoch: 00002 loss_train: 0.9388 loss_rec: 0.9388 acc_train: 0.5348 loss_val: 1.0173 acc_val: 0.5390 time: 0.0153s\n",
      "Epoch: 00003 loss_train: 1.0843 loss_rec: 1.0843 acc_train: 0.7152 loss_val: 0.9144 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00004 loss_train: 1.2300 loss_rec: 1.2300 acc_train: 0.7217 loss_val: 1.1518 acc_val: 0.7143 time: 0.0095s\n",
      "Epoch: 00005 loss_train: 1.1638 loss_rec: 1.1638 acc_train: 0.7217 loss_val: 1.1945 acc_val: 0.7338 time: 0.0104s\n",
      "Epoch: 00006 loss_train: 1.0385 loss_rec: 1.0385 acc_train: 0.7348 loss_val: 1.0371 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00007 loss_train: 0.8708 loss_rec: 0.8708 acc_train: 0.7087 loss_val: 0.8196 acc_val: 0.6948 time: 0.0090s\n",
      "Epoch: 00008 loss_train: 0.8299 loss_rec: 0.8299 acc_train: 0.6826 loss_val: 0.7595 acc_val: 0.6818 time: 0.0081s\n",
      "Epoch: 00009 loss_train: 0.8320 loss_rec: 0.8320 acc_train: 0.6435 loss_val: 0.9202 acc_val: 0.6364 time: 0.0085s\n",
      "Epoch: 00010 loss_train: 0.8313 loss_rec: 0.8313 acc_train: 0.6283 loss_val: 0.8647 acc_val: 0.6364 time: 0.0066s\n",
      "Epoch: 00011 loss_train: 0.7373 loss_rec: 0.7373 acc_train: 0.6283 loss_val: 0.7605 acc_val: 0.5974 time: 0.0090s\n",
      "Epoch: 00012 loss_train: 0.6592 loss_rec: 0.6592 acc_train: 0.6609 loss_val: 0.7224 acc_val: 0.6169 time: 0.0077s\n",
      "Epoch: 00013 loss_train: 0.6142 loss_rec: 0.6142 acc_train: 0.6913 loss_val: 0.6269 acc_val: 0.7143 time: 0.0075s\n",
      "Epoch: 00014 loss_train: 0.6035 loss_rec: 0.6035 acc_train: 0.7217 loss_val: 0.6692 acc_val: 0.6623 time: 0.0061s\n",
      "Epoch: 00015 loss_train: 0.5806 loss_rec: 0.5806 acc_train: 0.7283 loss_val: 0.6340 acc_val: 0.6818 time: 0.0051s\n",
      "Epoch: 00016 loss_train: 0.5716 loss_rec: 0.5716 acc_train: 0.7261 loss_val: 0.6140 acc_val: 0.7013 time: 0.0083s\n",
      "Epoch: 00017 loss_train: 0.5836 loss_rec: 0.5836 acc_train: 0.7261 loss_val: 0.5681 acc_val: 0.7013 time: 0.0070s\n",
      "Epoch: 00018 loss_train: 0.5712 loss_rec: 0.5712 acc_train: 0.7217 loss_val: 0.5890 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00019 loss_train: 0.5453 loss_rec: 0.5453 acc_train: 0.7348 loss_val: 0.5620 acc_val: 0.7078 time: 0.0065s\n",
      "Epoch: 00020 loss_train: 0.5380 loss_rec: 0.5380 acc_train: 0.7500 loss_val: 0.5481 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00021 loss_train: 0.5517 loss_rec: 0.5517 acc_train: 0.7391 loss_val: 0.5706 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00022 loss_train: 0.5422 loss_rec: 0.5422 acc_train: 0.7457 loss_val: 0.5850 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00023 loss_train: 0.5635 loss_rec: 0.5635 acc_train: 0.7370 loss_val: 0.5826 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00024 loss_train: 0.5371 loss_rec: 0.5371 acc_train: 0.7522 loss_val: 0.5881 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00025 loss_train: 0.5430 loss_rec: 0.5430 acc_train: 0.7413 loss_val: 0.5752 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00026 loss_train: 0.5449 loss_rec: 0.5449 acc_train: 0.7435 loss_val: 0.5796 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00027 loss_train: 0.5299 loss_rec: 0.5299 acc_train: 0.7478 loss_val: 0.5840 acc_val: 0.7273 time: 0.0065s\n",
      "Epoch: 00028 loss_train: 0.5308 loss_rec: 0.5308 acc_train: 0.7435 loss_val: 0.5599 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00029 loss_train: 0.5397 loss_rec: 0.5397 acc_train: 0.7478 loss_val: 0.5905 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00030 loss_train: 0.5573 loss_rec: 0.5573 acc_train: 0.7435 loss_val: 0.5810 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00031 loss_train: 0.5417 loss_rec: 0.5417 acc_train: 0.7478 loss_val: 0.5476 acc_val: 0.7403 time: 0.0107s\n",
      "Epoch: 00032 loss_train: 0.5363 loss_rec: 0.5363 acc_train: 0.7457 loss_val: 0.5848 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00033 loss_train: 0.5435 loss_rec: 0.5435 acc_train: 0.7413 loss_val: 0.5381 acc_val: 0.7403 time: 0.0120s\n",
      "Epoch: 00034 loss_train: 0.5440 loss_rec: 0.5440 acc_train: 0.7348 loss_val: 0.5602 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00035 loss_train: 0.5344 loss_rec: 0.5344 acc_train: 0.7435 loss_val: 0.5660 acc_val: 0.7208 time: 0.0071s\n",
      "Epoch: 00036 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7370 loss_val: 0.5491 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00037 loss_train: 0.5415 loss_rec: 0.5415 acc_train: 0.7304 loss_val: 0.5489 acc_val: 0.7273 time: 0.0074s\n",
      "Epoch: 00038 loss_train: 0.5401 loss_rec: 0.5401 acc_train: 0.7435 loss_val: 0.5494 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00039 loss_train: 0.5469 loss_rec: 0.5469 acc_train: 0.7413 loss_val: 0.5410 acc_val: 0.7208 time: 0.0060s\n",
      "Epoch: 00040 loss_train: 0.5392 loss_rec: 0.5392 acc_train: 0.7457 loss_val: 0.5533 acc_val: 0.7403 time: 0.0065s\n",
      "Epoch: 00041 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7435 loss_val: 0.5609 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00042 loss_train: 0.5383 loss_rec: 0.5383 acc_train: 0.7391 loss_val: 0.5534 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00043 loss_train: 0.5347 loss_rec: 0.5347 acc_train: 0.7413 loss_val: 0.5472 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00044 loss_train: 0.5338 loss_rec: 0.5338 acc_train: 0.7478 loss_val: 0.5810 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00045 loss_train: 0.5395 loss_rec: 0.5395 acc_train: 0.7457 loss_val: 0.5634 acc_val: 0.7273 time: 0.0084s\n",
      "Epoch: 00046 loss_train: 0.5246 loss_rec: 0.5246 acc_train: 0.7391 loss_val: 0.5815 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00047 loss_train: 0.5349 loss_rec: 0.5349 acc_train: 0.7435 loss_val: 0.5671 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00048 loss_train: 0.5250 loss_rec: 0.5250 acc_train: 0.7413 loss_val: 0.5359 acc_val: 0.7403 time: 0.0166s\n",
      "Epoch: 00049 loss_train: 0.5230 loss_rec: 0.5230 acc_train: 0.7413 loss_val: 0.5891 acc_val: 0.7273 time: 0.0095s\n",
      "Epoch: 00050 loss_train: 0.5243 loss_rec: 0.5243 acc_train: 0.7391 loss_val: 0.5686 acc_val: 0.7532 time: 0.0085s\n",
      "Epoch: 00051 loss_train: 0.5483 loss_rec: 0.5483 acc_train: 0.7413 loss_val: 0.5446 acc_val: 0.7403 time: 0.0124s\n",
      "Epoch: 00052 loss_train: 0.5260 loss_rec: 0.5260 acc_train: 0.7478 loss_val: 0.5562 acc_val: 0.7338 time: 0.0075s\n",
      "Epoch: 00053 loss_train: 0.5335 loss_rec: 0.5335 acc_train: 0.7478 loss_val: 0.6054 acc_val: 0.7403 time: 0.0096s\n",
      "Epoch: 00054 loss_train: 0.5583 loss_rec: 0.5583 acc_train: 0.7261 loss_val: 0.5782 acc_val: 0.7078 time: 0.0065s\n",
      "Epoch: 00055 loss_train: 0.5362 loss_rec: 0.5362 acc_train: 0.7435 loss_val: 0.5567 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00056 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7413 loss_val: 0.5934 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00057 loss_train: 0.5309 loss_rec: 0.5309 acc_train: 0.7522 loss_val: 0.5390 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00058 loss_train: 0.5353 loss_rec: 0.5353 acc_train: 0.7413 loss_val: 0.5299 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00059 loss_train: 0.5352 loss_rec: 0.5352 acc_train: 0.7370 loss_val: 0.5435 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00060 loss_train: 0.5301 loss_rec: 0.5301 acc_train: 0.7391 loss_val: 0.5484 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00061 loss_train: 0.5290 loss_rec: 0.5290 acc_train: 0.7435 loss_val: 0.5580 acc_val: 0.7273 time: 0.0078s\n",
      "Epoch: 00062 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7435 loss_val: 0.5643 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00063 loss_train: 0.5316 loss_rec: 0.5316 acc_train: 0.7435 loss_val: 0.5559 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00064 loss_train: 0.5325 loss_rec: 0.5325 acc_train: 0.7522 loss_val: 0.5527 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00065 loss_train: 0.5292 loss_rec: 0.5292 acc_train: 0.7457 loss_val: 0.5630 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00066 loss_train: 0.5264 loss_rec: 0.5264 acc_train: 0.7457 loss_val: 0.5454 acc_val: 0.7403 time: 0.0083s\n",
      "Epoch: 00067 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7478 loss_val: 0.5606 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00068 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7457 loss_val: 0.5397 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00069 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7500 loss_val: 0.5634 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00070 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7500 loss_val: 0.5676 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00071 loss_train: 0.5291 loss_rec: 0.5291 acc_train: 0.7435 loss_val: 0.5378 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00072 loss_train: 0.5299 loss_rec: 0.5299 acc_train: 0.7435 loss_val: 0.6007 acc_val: 0.7143 time: 0.0063s\n",
      "Epoch: 00073 loss_train: 0.5251 loss_rec: 0.5251 acc_train: 0.7478 loss_val: 0.5692 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00074 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7500 loss_val: 0.5530 acc_val: 0.7273 time: 0.0065s\n",
      "Epoch: 00075 loss_train: 0.5413 loss_rec: 0.5413 acc_train: 0.7370 loss_val: 0.5828 acc_val: 0.7143 time: 0.0093s\n",
      "Epoch: 00076 loss_train: 0.5166 loss_rec: 0.5166 acc_train: 0.7500 loss_val: 0.5823 acc_val: 0.7338 time: 0.0071s\n",
      "Epoch: 00077 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7413 loss_val: 0.5703 acc_val: 0.7273 time: 0.0073s\n",
      "Epoch: 00078 loss_train: 0.5265 loss_rec: 0.5265 acc_train: 0.7478 loss_val: 0.5470 acc_val: 0.7338 time: 0.0051s\n",
      "Epoch: 00079 loss_train: 0.5325 loss_rec: 0.5325 acc_train: 0.7457 loss_val: 0.5456 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00080 loss_train: 0.5287 loss_rec: 0.5287 acc_train: 0.7500 loss_val: 0.5852 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00081 loss_train: 0.5265 loss_rec: 0.5265 acc_train: 0.7391 loss_val: 0.5493 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00082 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7370 loss_val: 0.5569 acc_val: 0.7208 time: 0.0071s\n",
      "Epoch: 00083 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7478 loss_val: 0.5274 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00084 loss_train: 0.5322 loss_rec: 0.5322 acc_train: 0.7348 loss_val: 0.5703 acc_val: 0.7143 time: 0.0082s\n",
      "Epoch: 00085 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7413 loss_val: 0.5470 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00086 loss_train: 0.5259 loss_rec: 0.5259 acc_train: 0.7370 loss_val: 0.5357 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00087 loss_train: 0.5271 loss_rec: 0.5271 acc_train: 0.7348 loss_val: 0.5859 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00088 loss_train: 0.5266 loss_rec: 0.5266 acc_train: 0.7457 loss_val: 0.5720 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00089 loss_train: 0.5145 loss_rec: 0.5145 acc_train: 0.7435 loss_val: 0.5673 acc_val: 0.7403 time: 0.0074s\n",
      "Epoch: 00090 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7478 loss_val: 0.5753 acc_val: 0.7338 time: 0.0067s\n",
      "Epoch: 00091 loss_train: 0.5268 loss_rec: 0.5268 acc_train: 0.7435 loss_val: 0.5620 acc_val: 0.7468 time: 0.0081s\n",
      "Epoch: 00092 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7435 loss_val: 0.5589 acc_val: 0.7338 time: 0.0063s\n",
      "Epoch: 00093 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7457 loss_val: 0.5530 acc_val: 0.7338 time: 0.0067s\n",
      "Epoch: 00094 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7478 loss_val: 0.5463 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00095 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7435 loss_val: 0.5697 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00096 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7435 loss_val: 0.5520 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00097 loss_train: 0.5115 loss_rec: 0.5115 acc_train: 0.7478 loss_val: 0.5240 acc_val: 0.7273 time: 0.0066s\n",
      "Epoch: 00098 loss_train: 0.5380 loss_rec: 0.5380 acc_train: 0.7413 loss_val: 0.5799 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00099 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7500 loss_val: 0.5706 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00100 loss_train: 0.5335 loss_rec: 0.5335 acc_train: 0.7478 loss_val: 0.5616 acc_val: 0.7338 time: 0.0069s\n",
      "Epoch: 00101 loss_train: 0.5399 loss_rec: 0.5399 acc_train: 0.7478 loss_val: 0.5635 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00102 loss_train: 0.5198 loss_rec: 0.5198 acc_train: 0.7413 loss_val: 0.5581 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00103 loss_train: 0.5350 loss_rec: 0.5350 acc_train: 0.7457 loss_val: 0.5533 acc_val: 0.7403 time: 0.0081s\n",
      "Epoch: 00104 loss_train: 0.5152 loss_rec: 0.5152 acc_train: 0.7435 loss_val: 0.5269 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00105 loss_train: 0.5349 loss_rec: 0.5349 acc_train: 0.7304 loss_val: 0.5578 acc_val: 0.7273 time: 0.0077s\n",
      "Epoch: 00106 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7413 loss_val: 0.5372 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00107 loss_train: 0.5358 loss_rec: 0.5358 acc_train: 0.7391 loss_val: 0.5543 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00108 loss_train: 0.5272 loss_rec: 0.5272 acc_train: 0.7500 loss_val: 0.5388 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00109 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7391 loss_val: 0.5649 acc_val: 0.7403 time: 0.0103s\n",
      "Epoch: 00110 loss_train: 0.5136 loss_rec: 0.5136 acc_train: 0.7435 loss_val: 0.5408 acc_val: 0.7338 time: 0.0067s\n",
      "Epoch: 00111 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7500 loss_val: 0.5531 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00112 loss_train: 0.5303 loss_rec: 0.5303 acc_train: 0.7326 loss_val: 0.5322 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00113 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7435 loss_val: 0.5265 acc_val: 0.7532 time: 0.0040s\n",
      "Epoch: 00114 loss_train: 0.5232 loss_rec: 0.5232 acc_train: 0.7500 loss_val: 0.6014 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00115 loss_train: 0.5245 loss_rec: 0.5245 acc_train: 0.7435 loss_val: 0.5606 acc_val: 0.7273 time: 0.0073s\n",
      "Epoch: 00116 loss_train: 0.5098 loss_rec: 0.5098 acc_train: 0.7457 loss_val: 0.5697 acc_val: 0.7403 time: 0.0061s\n",
      "Epoch: 00117 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7457 loss_val: 0.5449 acc_val: 0.7143 time: 0.0060s\n",
      "Epoch: 00118 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7478 loss_val: 0.5663 acc_val: 0.7338 time: 0.0061s\n",
      "Epoch: 00119 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7500 loss_val: 0.5758 acc_val: 0.7403 time: 0.0078s\n",
      "Epoch: 00120 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7500 loss_val: 0.5716 acc_val: 0.7338 time: 0.0055s\n",
      "Epoch: 00121 loss_train: 0.5292 loss_rec: 0.5292 acc_train: 0.7413 loss_val: 0.5504 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00122 loss_train: 0.5137 loss_rec: 0.5137 acc_train: 0.7478 loss_val: 0.5829 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00123 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7478 loss_val: 0.5824 acc_val: 0.7143 time: 0.0069s\n",
      "Epoch: 00124 loss_train: 0.5220 loss_rec: 0.5220 acc_train: 0.7413 loss_val: 0.5333 acc_val: 0.7208 time: 0.0060s\n",
      "Epoch: 00125 loss_train: 0.5333 loss_rec: 0.5333 acc_train: 0.7413 loss_val: 0.5589 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00126 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7478 loss_val: 0.5459 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00127 loss_train: 0.5199 loss_rec: 0.5199 acc_train: 0.7391 loss_val: 0.5410 acc_val: 0.7403 time: 0.0065s\n",
      "Epoch: 00128 loss_train: 0.5110 loss_rec: 0.5110 acc_train: 0.7478 loss_val: 0.5438 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00129 loss_train: 0.5310 loss_rec: 0.5310 acc_train: 0.7348 loss_val: 0.5719 acc_val: 0.7208 time: 0.0053s\n",
      "Epoch: 00130 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7435 loss_val: 0.5846 acc_val: 0.7078 time: 0.0094s\n",
      "Epoch: 00131 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7413 loss_val: 0.5641 acc_val: 0.7403 time: 0.0091s\n",
      "Epoch: 00132 loss_train: 0.5116 loss_rec: 0.5116 acc_train: 0.7457 loss_val: 0.5504 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00133 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7500 loss_val: 0.5706 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00134 loss_train: 0.5278 loss_rec: 0.5278 acc_train: 0.7435 loss_val: 0.5531 acc_val: 0.7273 time: 0.0067s\n",
      "Epoch: 00135 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7457 loss_val: 0.5746 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00136 loss_train: 0.5251 loss_rec: 0.5251 acc_train: 0.7435 loss_val: 0.5354 acc_val: 0.7013 time: 0.0060s\n",
      "Epoch: 00137 loss_train: 0.5224 loss_rec: 0.5224 acc_train: 0.7348 loss_val: 0.5425 acc_val: 0.7143 time: 0.0050s\n",
      "Epoch: 00138 loss_train: 0.5166 loss_rec: 0.5166 acc_train: 0.7413 loss_val: 0.5467 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00139 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7457 loss_val: 0.5980 acc_val: 0.7208 time: 0.0074s\n",
      "Epoch: 00140 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7457 loss_val: 0.5757 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00141 loss_train: 0.5273 loss_rec: 0.5273 acc_train: 0.7478 loss_val: 0.5596 acc_val: 0.7273 time: 0.0062s\n",
      "Epoch: 00142 loss_train: 0.5258 loss_rec: 0.5258 acc_train: 0.7413 loss_val: 0.5546 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00143 loss_train: 0.5276 loss_rec: 0.5276 acc_train: 0.7457 loss_val: 0.5574 acc_val: 0.7468 time: 0.0071s\n",
      "Epoch: 00144 loss_train: 0.5359 loss_rec: 0.5359 acc_train: 0.7348 loss_val: 0.5559 acc_val: 0.7532 time: 0.0071s\n",
      "Epoch: 00145 loss_train: 0.5110 loss_rec: 0.5110 acc_train: 0.7457 loss_val: 0.5539 acc_val: 0.7338 time: 0.0074s\n",
      "Epoch: 00146 loss_train: 0.5131 loss_rec: 0.5131 acc_train: 0.7391 loss_val: 0.5477 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00147 loss_train: 0.5301 loss_rec: 0.5301 acc_train: 0.7370 loss_val: 0.5358 acc_val: 0.7468 time: 0.0050s\n",
      "Epoch: 00148 loss_train: 0.5320 loss_rec: 0.5320 acc_train: 0.7370 loss_val: 0.5550 acc_val: 0.7338 time: 0.0066s\n",
      "Epoch: 00149 loss_train: 0.5324 loss_rec: 0.5324 acc_train: 0.7435 loss_val: 0.5551 acc_val: 0.7403 time: 0.0055s\n",
      "Epoch: 00150 loss_train: 0.5280 loss_rec: 0.5280 acc_train: 0.7500 loss_val: 0.5589 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00151 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7348 loss_val: 0.5360 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00152 loss_train: 0.5403 loss_rec: 0.5403 acc_train: 0.7283 loss_val: 0.5458 acc_val: 0.7273 time: 0.0073s\n",
      "Epoch: 00153 loss_train: 0.5285 loss_rec: 0.5285 acc_train: 0.7478 loss_val: 0.5394 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00154 loss_train: 0.5181 loss_rec: 0.5181 acc_train: 0.7413 loss_val: 0.5998 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00155 loss_train: 0.5264 loss_rec: 0.5264 acc_train: 0.7500 loss_val: 0.5373 acc_val: 0.7403 time: 0.0076s\n",
      "Epoch: 00156 loss_train: 0.5199 loss_rec: 0.5199 acc_train: 0.7478 loss_val: 0.5557 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00157 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7457 loss_val: 0.5581 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00158 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7435 loss_val: 0.5299 acc_val: 0.7532 time: 0.0068s\n",
      "Epoch: 00159 loss_train: 0.5140 loss_rec: 0.5140 acc_train: 0.7413 loss_val: 0.5981 acc_val: 0.7208 time: 0.0094s\n",
      "Epoch: 00160 loss_train: 0.5358 loss_rec: 0.5358 acc_train: 0.7348 loss_val: 0.6026 acc_val: 0.7208 time: 0.0087s\n",
      "Epoch: 00161 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7435 loss_val: 0.5708 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00162 loss_train: 0.5293 loss_rec: 0.5293 acc_train: 0.7478 loss_val: 0.5451 acc_val: 0.7468 time: 0.0084s\n",
      "Epoch: 00163 loss_train: 0.5345 loss_rec: 0.5345 acc_train: 0.7435 loss_val: 0.5571 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00164 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7391 loss_val: 0.5393 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00165 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7478 loss_val: 0.5870 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00166 loss_train: 0.5160 loss_rec: 0.5160 acc_train: 0.7522 loss_val: 0.5599 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00167 loss_train: 0.5147 loss_rec: 0.5147 acc_train: 0.7522 loss_val: 0.5356 acc_val: 0.7532 time: 0.0091s\n",
      "Epoch: 00168 loss_train: 0.5235 loss_rec: 0.5235 acc_train: 0.7435 loss_val: 0.5574 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00169 loss_train: 0.5293 loss_rec: 0.5293 acc_train: 0.7457 loss_val: 0.5767 acc_val: 0.7208 time: 0.0102s\n",
      "Epoch: 00170 loss_train: 0.5384 loss_rec: 0.5384 acc_train: 0.7348 loss_val: 0.5514 acc_val: 0.7403 time: 0.0086s\n",
      "Epoch: 00171 loss_train: 0.5285 loss_rec: 0.5285 acc_train: 0.7370 loss_val: 0.5555 acc_val: 0.7208 time: 0.0088s\n",
      "Epoch: 00172 loss_train: 0.5285 loss_rec: 0.5285 acc_train: 0.7413 loss_val: 0.5703 acc_val: 0.7208 time: 0.0077s\n",
      "Epoch: 00173 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7500 loss_val: 0.5699 acc_val: 0.7208 time: 0.0108s\n",
      "Epoch: 00174 loss_train: 0.5197 loss_rec: 0.5197 acc_train: 0.7370 loss_val: 0.5581 acc_val: 0.7143 time: 0.0113s\n",
      "Epoch: 00175 loss_train: 0.5199 loss_rec: 0.5199 acc_train: 0.7391 loss_val: 0.5629 acc_val: 0.7143 time: 0.0072s\n",
      "Epoch: 00176 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7565 loss_val: 0.5390 acc_val: 0.7338 time: 0.0092s\n",
      "Epoch: 00177 loss_train: 0.5188 loss_rec: 0.5188 acc_train: 0.7348 loss_val: 0.5854 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00178 loss_train: 0.5120 loss_rec: 0.5120 acc_train: 0.7500 loss_val: 0.5709 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00179 loss_train: 0.5290 loss_rec: 0.5290 acc_train: 0.7370 loss_val: 0.5443 acc_val: 0.7273 time: 0.0079s\n",
      "Epoch: 00180 loss_train: 0.5077 loss_rec: 0.5077 acc_train: 0.7522 loss_val: 0.5523 acc_val: 0.7468 time: 0.0082s\n",
      "Epoch: 00181 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7457 loss_val: 0.6299 acc_val: 0.7013 time: 0.0065s\n",
      "Epoch: 00182 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7457 loss_val: 0.5631 acc_val: 0.7468 time: 0.0072s\n",
      "Epoch: 00183 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7391 loss_val: 0.5581 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00184 loss_train: 0.5295 loss_rec: 0.5295 acc_train: 0.7500 loss_val: 0.5493 acc_val: 0.7273 time: 0.0074s\n",
      "Epoch: 00185 loss_train: 0.5094 loss_rec: 0.5094 acc_train: 0.7435 loss_val: 0.5700 acc_val: 0.7468 time: 0.0065s\n",
      "Epoch: 00186 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7457 loss_val: 0.5546 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00187 loss_train: 0.5276 loss_rec: 0.5276 acc_train: 0.7457 loss_val: 0.6071 acc_val: 0.7403 time: 0.0076s\n",
      "Epoch: 00188 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7413 loss_val: 0.5797 acc_val: 0.7338 time: 0.0092s\n",
      "Epoch: 00189 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7457 loss_val: 0.5707 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00190 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7565 loss_val: 0.5662 acc_val: 0.7208 time: 0.0072s\n",
      "Epoch: 00191 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7478 loss_val: 0.5518 acc_val: 0.7208 time: 0.0045s\n",
      "Epoch: 00192 loss_train: 0.5303 loss_rec: 0.5303 acc_train: 0.7413 loss_val: 0.5939 acc_val: 0.7208 time: 0.0063s\n",
      "Epoch: 00193 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7457 loss_val: 0.5344 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00194 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7435 loss_val: 0.5574 acc_val: 0.7403 time: 0.0074s\n",
      "Epoch: 00195 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7413 loss_val: 0.5501 acc_val: 0.7208 time: 0.0106s\n",
      "Epoch: 00196 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7391 loss_val: 0.5751 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00197 loss_train: 0.5304 loss_rec: 0.5304 acc_train: 0.7391 loss_val: 0.5593 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00198 loss_train: 0.5149 loss_rec: 0.5149 acc_train: 0.7413 loss_val: 0.5373 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00199 loss_train: 0.5271 loss_rec: 0.5271 acc_train: 0.7435 loss_val: 0.5496 acc_val: 0.7208 time: 0.0065s\n",
      "Epoch: 00200 loss_train: 0.5232 loss_rec: 0.5232 acc_train: 0.7500 loss_val: 0.5564 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00201 loss_train: 0.5230 loss_rec: 0.5230 acc_train: 0.7413 loss_val: 0.5440 acc_val: 0.7468 time: 0.0075s\n",
      "Epoch: 00202 loss_train: 0.5247 loss_rec: 0.5247 acc_train: 0.7413 loss_val: 0.5780 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00203 loss_train: 0.5246 loss_rec: 0.5246 acc_train: 0.7457 loss_val: 0.5605 acc_val: 0.7208 time: 0.0066s\n",
      "Epoch: 00204 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7478 loss_val: 0.5705 acc_val: 0.7403 time: 0.0072s\n",
      "Epoch: 00205 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7391 loss_val: 0.5293 acc_val: 0.7273 time: 0.0065s\n",
      "Epoch: 00206 loss_train: 0.5212 loss_rec: 0.5212 acc_train: 0.7413 loss_val: 0.5576 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00207 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7457 loss_val: 0.5632 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00208 loss_train: 0.5219 loss_rec: 0.5219 acc_train: 0.7391 loss_val: 0.5625 acc_val: 0.7078 time: 0.0040s\n",
      "Epoch: 00209 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7457 loss_val: 0.5607 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00210 loss_train: 0.5327 loss_rec: 0.5327 acc_train: 0.7500 loss_val: 0.5748 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00211 loss_train: 0.5252 loss_rec: 0.5252 acc_train: 0.7500 loss_val: 0.5726 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00212 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7283 loss_val: 0.5753 acc_val: 0.7208 time: 0.0060s\n",
      "Epoch: 00213 loss_train: 0.5152 loss_rec: 0.5152 acc_train: 0.7478 loss_val: 0.5564 acc_val: 0.7143 time: 0.0077s\n",
      "Epoch: 00214 loss_train: 0.5315 loss_rec: 0.5315 acc_train: 0.7348 loss_val: 0.5555 acc_val: 0.7078 time: 0.0074s\n",
      "Epoch: 00215 loss_train: 0.5118 loss_rec: 0.5118 acc_train: 0.7457 loss_val: 0.5744 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00216 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7478 loss_val: 0.5424 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00217 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7457 loss_val: 0.5641 acc_val: 0.7078 time: 0.0072s\n",
      "Epoch: 00218 loss_train: 0.5241 loss_rec: 0.5241 acc_train: 0.7478 loss_val: 0.5566 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00219 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7435 loss_val: 0.5781 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00220 loss_train: 0.5267 loss_rec: 0.5267 acc_train: 0.7500 loss_val: 0.5604 acc_val: 0.7273 time: 0.0084s\n",
      "Epoch: 00221 loss_train: 0.5276 loss_rec: 0.5276 acc_train: 0.7457 loss_val: 0.5528 acc_val: 0.7013 time: 0.0070s\n",
      "Epoch: 00222 loss_train: 0.5288 loss_rec: 0.5288 acc_train: 0.7565 loss_val: 0.5430 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00223 loss_train: 0.5217 loss_rec: 0.5217 acc_train: 0.7522 loss_val: 0.5539 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00224 loss_train: 0.5189 loss_rec: 0.5189 acc_train: 0.7413 loss_val: 0.5514 acc_val: 0.7468 time: 0.0060s\n",
      "Epoch: 00225 loss_train: 0.5232 loss_rec: 0.5232 acc_train: 0.7478 loss_val: 0.5886 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00226 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7304 loss_val: 0.5555 acc_val: 0.7532 time: 0.0070s\n",
      "Epoch: 00227 loss_train: 0.5181 loss_rec: 0.5181 acc_train: 0.7457 loss_val: 0.5500 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00228 loss_train: 0.5315 loss_rec: 0.5315 acc_train: 0.7478 loss_val: 0.5625 acc_val: 0.7338 time: 0.0063s\n",
      "Epoch: 00229 loss_train: 0.5243 loss_rec: 0.5243 acc_train: 0.7391 loss_val: 0.5402 acc_val: 0.7403 time: 0.0069s\n",
      "Epoch: 00230 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7413 loss_val: 0.5542 acc_val: 0.7338 time: 0.0094s\n",
      "Epoch: 00231 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7500 loss_val: 0.5615 acc_val: 0.7208 time: 0.0066s\n",
      "Epoch: 00232 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7391 loss_val: 0.5642 acc_val: 0.7143 time: 0.0088s\n",
      "Epoch: 00233 loss_train: 0.5188 loss_rec: 0.5188 acc_train: 0.7413 loss_val: 0.5643 acc_val: 0.7338 time: 0.0066s\n",
      "Epoch: 00234 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7457 loss_val: 0.5667 acc_val: 0.7403 time: 0.0062s\n",
      "Epoch: 00235 loss_train: 0.5153 loss_rec: 0.5153 acc_train: 0.7478 loss_val: 0.5459 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00236 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7413 loss_val: 0.5758 acc_val: 0.7208 time: 0.0068s\n",
      "Epoch: 00237 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7435 loss_val: 0.5442 acc_val: 0.7273 time: 0.0067s\n",
      "Epoch: 00238 loss_train: 0.5138 loss_rec: 0.5138 acc_train: 0.7500 loss_val: 0.5801 acc_val: 0.7208 time: 0.0065s\n",
      "Epoch: 00239 loss_train: 0.5288 loss_rec: 0.5288 acc_train: 0.7370 loss_val: 0.5554 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00240 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7522 loss_val: 0.5658 acc_val: 0.7273 time: 0.0065s\n",
      "Epoch: 00241 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7435 loss_val: 0.5608 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00242 loss_train: 0.5117 loss_rec: 0.5117 acc_train: 0.7478 loss_val: 0.5615 acc_val: 0.7273 time: 0.0065s\n",
      "Epoch: 00243 loss_train: 0.5199 loss_rec: 0.5199 acc_train: 0.7370 loss_val: 0.5453 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00244 loss_train: 0.5252 loss_rec: 0.5252 acc_train: 0.7457 loss_val: 0.5740 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00245 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7370 loss_val: 0.5586 acc_val: 0.7208 time: 0.0104s\n",
      "Epoch: 00246 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7457 loss_val: 0.5709 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00247 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7457 loss_val: 0.5647 acc_val: 0.7338 time: 0.0094s\n",
      "Epoch: 00248 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7478 loss_val: 0.5716 acc_val: 0.7338 time: 0.0077s\n",
      "Epoch: 00249 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7478 loss_val: 0.5724 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00250 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7478 loss_val: 0.5796 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00251 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7478 loss_val: 0.5642 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00252 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7391 loss_val: 0.5506 acc_val: 0.7208 time: 0.0078s\n",
      "Epoch: 00253 loss_train: 0.5246 loss_rec: 0.5246 acc_train: 0.7500 loss_val: 0.5638 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00254 loss_train: 0.5371 loss_rec: 0.5371 acc_train: 0.7435 loss_val: 0.5394 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00255 loss_train: 0.5290 loss_rec: 0.5290 acc_train: 0.7478 loss_val: 0.5471 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00256 loss_train: 0.5019 loss_rec: 0.5019 acc_train: 0.7457 loss_val: 0.5501 acc_val: 0.7143 time: 0.0079s\n",
      "Epoch: 00257 loss_train: 0.5205 loss_rec: 0.5205 acc_train: 0.7457 loss_val: 0.5680 acc_val: 0.7338 time: 0.0122s\n",
      "Epoch: 00258 loss_train: 0.5297 loss_rec: 0.5297 acc_train: 0.7391 loss_val: 0.5559 acc_val: 0.7143 time: 0.0128s\n",
      "Epoch: 00259 loss_train: 0.5175 loss_rec: 0.5175 acc_train: 0.7457 loss_val: 0.5626 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00260 loss_train: 0.5160 loss_rec: 0.5160 acc_train: 0.7413 loss_val: 0.5956 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00261 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7500 loss_val: 0.5773 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00262 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7457 loss_val: 0.5614 acc_val: 0.7208 time: 0.0118s\n",
      "Epoch: 00263 loss_train: 0.5150 loss_rec: 0.5150 acc_train: 0.7457 loss_val: 0.5607 acc_val: 0.7338 time: 0.0055s\n",
      "Epoch: 00264 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7500 loss_val: 0.5563 acc_val: 0.7143 time: 0.0095s\n",
      "Epoch: 00265 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7457 loss_val: 0.5737 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00266 loss_train: 0.5144 loss_rec: 0.5144 acc_train: 0.7457 loss_val: 0.5729 acc_val: 0.7338 time: 0.0095s\n",
      "Epoch: 00267 loss_train: 0.5130 loss_rec: 0.5130 acc_train: 0.7457 loss_val: 0.5638 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00268 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7435 loss_val: 0.5678 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00269 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7500 loss_val: 0.5512 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00270 loss_train: 0.5218 loss_rec: 0.5218 acc_train: 0.7457 loss_val: 0.5706 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00271 loss_train: 0.5314 loss_rec: 0.5314 acc_train: 0.7457 loss_val: 0.5756 acc_val: 0.7403 time: 0.0081s\n",
      "Epoch: 00272 loss_train: 0.5167 loss_rec: 0.5167 acc_train: 0.7478 loss_val: 0.5667 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00273 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7457 loss_val: 0.5584 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00274 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7457 loss_val: 0.5468 acc_val: 0.7208 time: 0.0055s\n",
      "Epoch: 00275 loss_train: 0.5296 loss_rec: 0.5296 acc_train: 0.7478 loss_val: 0.5434 acc_val: 0.7468 time: 0.0076s\n",
      "Epoch: 00276 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7457 loss_val: 0.5831 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00277 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7435 loss_val: 0.5316 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00278 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7478 loss_val: 0.5439 acc_val: 0.7403 time: 0.0089s\n",
      "Epoch: 00279 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7478 loss_val: 0.5747 acc_val: 0.7338 time: 0.0115s\n",
      "Epoch: 00280 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7413 loss_val: 0.5591 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00281 loss_train: 0.5122 loss_rec: 0.5122 acc_train: 0.7391 loss_val: 0.5283 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00282 loss_train: 0.5333 loss_rec: 0.5333 acc_train: 0.7413 loss_val: 0.5488 acc_val: 0.7078 time: 0.0073s\n",
      "Epoch: 00283 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7478 loss_val: 0.5569 acc_val: 0.7013 time: 0.0111s\n",
      "Epoch: 00284 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7457 loss_val: 0.5581 acc_val: 0.7403 time: 0.0082s\n",
      "Epoch: 00285 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7478 loss_val: 0.5808 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00286 loss_train: 0.5147 loss_rec: 0.5147 acc_train: 0.7500 loss_val: 0.5425 acc_val: 0.7403 time: 0.0083s\n",
      "Epoch: 00287 loss_train: 0.5236 loss_rec: 0.5236 acc_train: 0.7413 loss_val: 0.5524 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00288 loss_train: 0.5219 loss_rec: 0.5219 acc_train: 0.7348 loss_val: 0.5453 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00289 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7500 loss_val: 0.5521 acc_val: 0.7208 time: 0.0098s\n",
      "Epoch: 00290 loss_train: 0.5283 loss_rec: 0.5283 acc_train: 0.7413 loss_val: 0.5606 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00291 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7478 loss_val: 0.5404 acc_val: 0.7273 time: 0.0110s\n",
      "Epoch: 00292 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7413 loss_val: 0.5702 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00293 loss_train: 0.5302 loss_rec: 0.5302 acc_train: 0.7391 loss_val: 0.5825 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00294 loss_train: 0.5168 loss_rec: 0.5168 acc_train: 0.7587 loss_val: 0.5534 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00295 loss_train: 0.5127 loss_rec: 0.5127 acc_train: 0.7413 loss_val: 0.5465 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00296 loss_train: 0.5281 loss_rec: 0.5281 acc_train: 0.7326 loss_val: 0.5725 acc_val: 0.7208 time: 0.0081s\n",
      "Epoch: 00297 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7457 loss_val: 0.5484 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00298 loss_train: 0.5266 loss_rec: 0.5266 acc_train: 0.7478 loss_val: 0.5533 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00299 loss_train: 0.5370 loss_rec: 0.5370 acc_train: 0.7435 loss_val: 0.5652 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00300 loss_train: 0.5220 loss_rec: 0.5220 acc_train: 0.7543 loss_val: 0.5458 acc_val: 0.7338 time: 0.0099s\n",
      "Epoch: 00301 loss_train: 0.5180 loss_rec: 0.5180 acc_train: 0.7478 loss_val: 0.5549 acc_val: 0.7208 time: 0.0065s\n",
      "Epoch: 00302 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7413 loss_val: 0.5649 acc_val: 0.7338 time: 0.0106s\n",
      "Epoch: 00303 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7391 loss_val: 0.5672 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00304 loss_train: 0.5321 loss_rec: 0.5321 acc_train: 0.7413 loss_val: 0.5586 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00305 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7500 loss_val: 0.5802 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00306 loss_train: 0.5157 loss_rec: 0.5157 acc_train: 0.7457 loss_val: 0.5731 acc_val: 0.7273 time: 0.0089s\n",
      "Epoch: 00307 loss_train: 0.5303 loss_rec: 0.5303 acc_train: 0.7435 loss_val: 0.5615 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00308 loss_train: 0.5179 loss_rec: 0.5179 acc_train: 0.7391 loss_val: 0.5673 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00309 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7500 loss_val: 0.5404 acc_val: 0.7338 time: 0.0110s\n",
      "Epoch: 00310 loss_train: 0.5216 loss_rec: 0.5216 acc_train: 0.7391 loss_val: 0.5492 acc_val: 0.7403 time: 0.0999s\n",
      "Epoch: 00311 loss_train: 0.5351 loss_rec: 0.5351 acc_train: 0.7413 loss_val: 0.5377 acc_val: 0.7403 time: 0.0135s\n",
      "Epoch: 00312 loss_train: 0.5279 loss_rec: 0.5279 acc_train: 0.7413 loss_val: 0.5474 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00313 loss_train: 0.5224 loss_rec: 0.5224 acc_train: 0.7413 loss_val: 0.5771 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00314 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7500 loss_val: 0.5686 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00315 loss_train: 0.5143 loss_rec: 0.5143 acc_train: 0.7457 loss_val: 0.5469 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00316 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7457 loss_val: 0.5956 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00317 loss_train: 0.5061 loss_rec: 0.5061 acc_train: 0.7457 loss_val: 0.5586 acc_val: 0.7468 time: 0.0102s\n",
      "Epoch: 00318 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7457 loss_val: 0.5606 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00319 loss_train: 0.5178 loss_rec: 0.5178 acc_train: 0.7435 loss_val: 0.5845 acc_val: 0.7208 time: 0.0111s\n",
      "Epoch: 00320 loss_train: 0.5216 loss_rec: 0.5216 acc_train: 0.7500 loss_val: 0.5729 acc_val: 0.7403 time: 0.0144s\n",
      "Epoch: 00321 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7435 loss_val: 0.5657 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00322 loss_train: 0.5145 loss_rec: 0.5145 acc_train: 0.7457 loss_val: 0.5692 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00323 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7435 loss_val: 0.5667 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00324 loss_train: 0.5103 loss_rec: 0.5103 acc_train: 0.7457 loss_val: 0.5571 acc_val: 0.7273 time: 0.0141s\n",
      "Epoch: 00325 loss_train: 0.5096 loss_rec: 0.5096 acc_train: 0.7478 loss_val: 0.5479 acc_val: 0.7468 time: 0.0140s\n",
      "Epoch: 00326 loss_train: 0.5154 loss_rec: 0.5154 acc_train: 0.7457 loss_val: 0.5565 acc_val: 0.7403 time: 0.0102s\n",
      "Epoch: 00327 loss_train: 0.5144 loss_rec: 0.5144 acc_train: 0.7478 loss_val: 0.5521 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00328 loss_train: 0.5306 loss_rec: 0.5306 acc_train: 0.7478 loss_val: 0.5708 acc_val: 0.7143 time: 0.0072s\n",
      "Epoch: 00329 loss_train: 0.5252 loss_rec: 0.5252 acc_train: 0.7435 loss_val: 0.5659 acc_val: 0.7338 time: 0.0074s\n",
      "Epoch: 00330 loss_train: 0.5255 loss_rec: 0.5255 acc_train: 0.7435 loss_val: 0.5578 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00331 loss_train: 0.5216 loss_rec: 0.5216 acc_train: 0.7413 loss_val: 0.5352 acc_val: 0.7273 time: 0.0084s\n",
      "Epoch: 00332 loss_train: 0.5218 loss_rec: 0.5218 acc_train: 0.7500 loss_val: 0.5957 acc_val: 0.7013 time: 0.0086s\n",
      "Epoch: 00333 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7457 loss_val: 0.5559 acc_val: 0.7078 time: 0.0081s\n",
      "Epoch: 00334 loss_train: 0.5077 loss_rec: 0.5077 acc_train: 0.7500 loss_val: 0.5715 acc_val: 0.7532 time: 0.0072s\n",
      "Epoch: 00335 loss_train: 0.5197 loss_rec: 0.5197 acc_train: 0.7478 loss_val: 0.5376 acc_val: 0.7468 time: 0.0086s\n",
      "Epoch: 00336 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7413 loss_val: 0.5736 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00337 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7326 loss_val: 0.5654 acc_val: 0.7013 time: 0.0083s\n",
      "Epoch: 00338 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7457 loss_val: 0.5561 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00339 loss_train: 0.5290 loss_rec: 0.5290 acc_train: 0.7391 loss_val: 0.5648 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00340 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7435 loss_val: 0.5539 acc_val: 0.7208 time: 0.0067s\n",
      "Epoch: 00341 loss_train: 0.5198 loss_rec: 0.5198 acc_train: 0.7522 loss_val: 0.5345 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00342 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7391 loss_val: 0.5906 acc_val: 0.6948 time: 0.0082s\n",
      "Epoch: 00343 loss_train: 0.5143 loss_rec: 0.5143 acc_train: 0.7565 loss_val: 0.5584 acc_val: 0.7013 time: 0.0098s\n",
      "Epoch: 00344 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7457 loss_val: 0.5430 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00345 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7435 loss_val: 0.5495 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00346 loss_train: 0.5121 loss_rec: 0.5121 acc_train: 0.7457 loss_val: 0.5456 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00347 loss_train: 0.5335 loss_rec: 0.5335 acc_train: 0.7413 loss_val: 0.5602 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00348 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7500 loss_val: 0.5854 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00349 loss_train: 0.5203 loss_rec: 0.5203 acc_train: 0.7543 loss_val: 0.5503 acc_val: 0.7208 time: 0.0082s\n",
      "Epoch: 00350 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7435 loss_val: 0.5604 acc_val: 0.7403 time: 0.0128s\n",
      "Epoch: 00351 loss_train: 0.5098 loss_rec: 0.5098 acc_train: 0.7500 loss_val: 0.5548 acc_val: 0.7208 time: 0.0076s\n",
      "Epoch: 00352 loss_train: 0.5243 loss_rec: 0.5243 acc_train: 0.7435 loss_val: 0.5768 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00353 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7478 loss_val: 0.5878 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00354 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7391 loss_val: 0.5608 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00355 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7435 loss_val: 0.5245 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00356 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7435 loss_val: 0.5608 acc_val: 0.7468 time: 0.0089s\n",
      "Epoch: 00357 loss_train: 0.5284 loss_rec: 0.5284 acc_train: 0.7457 loss_val: 0.5686 acc_val: 0.7468 time: 0.0076s\n",
      "Epoch: 00358 loss_train: 0.5101 loss_rec: 0.5101 acc_train: 0.7413 loss_val: 0.5738 acc_val: 0.7078 time: 0.0101s\n",
      "Epoch: 00359 loss_train: 0.5279 loss_rec: 0.5279 acc_train: 0.7457 loss_val: 0.5687 acc_val: 0.7143 time: 0.0089s\n",
      "Epoch: 00360 loss_train: 0.5262 loss_rec: 0.5262 acc_train: 0.7413 loss_val: 0.5326 acc_val: 0.7532 time: 0.0051s\n",
      "Epoch: 00361 loss_train: 0.5276 loss_rec: 0.5276 acc_train: 0.7348 loss_val: 0.5536 acc_val: 0.7208 time: 0.0040s\n",
      "Epoch: 00362 loss_train: 0.5247 loss_rec: 0.5247 acc_train: 0.7543 loss_val: 0.5643 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00363 loss_train: 0.5301 loss_rec: 0.5301 acc_train: 0.7391 loss_val: 0.5550 acc_val: 0.7208 time: 0.0065s\n",
      "Epoch: 00364 loss_train: 0.5298 loss_rec: 0.5298 acc_train: 0.7413 loss_val: 0.5469 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00365 loss_train: 0.5267 loss_rec: 0.5267 acc_train: 0.7478 loss_val: 0.5625 acc_val: 0.7338 time: 0.0066s\n",
      "Epoch: 00366 loss_train: 0.5193 loss_rec: 0.5193 acc_train: 0.7457 loss_val: 0.5425 acc_val: 0.7403 time: 0.0062s\n",
      "Epoch: 00367 loss_train: 0.5216 loss_rec: 0.5216 acc_train: 0.7478 loss_val: 0.5690 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00368 loss_train: 0.5318 loss_rec: 0.5318 acc_train: 0.7435 loss_val: 0.5503 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00369 loss_train: 0.5179 loss_rec: 0.5179 acc_train: 0.7522 loss_val: 0.5507 acc_val: 0.7273 time: 0.0055s\n",
      "Epoch: 00370 loss_train: 0.5219 loss_rec: 0.5219 acc_train: 0.7435 loss_val: 0.5449 acc_val: 0.7273 time: 0.0055s\n",
      "Epoch: 00371 loss_train: 0.5324 loss_rec: 0.5324 acc_train: 0.7391 loss_val: 0.5688 acc_val: 0.7273 time: 0.0057s\n",
      "Epoch: 00372 loss_train: 0.5057 loss_rec: 0.5057 acc_train: 0.7457 loss_val: 0.5414 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00373 loss_train: 0.5285 loss_rec: 0.5285 acc_train: 0.7391 loss_val: 0.5466 acc_val: 0.7403 time: 0.0065s\n",
      "Epoch: 00374 loss_train: 0.5295 loss_rec: 0.5295 acc_train: 0.7413 loss_val: 0.5694 acc_val: 0.7143 time: 0.0065s\n",
      "Epoch: 00375 loss_train: 0.5296 loss_rec: 0.5296 acc_train: 0.7478 loss_val: 0.5486 acc_val: 0.7532 time: 0.0061s\n",
      "Epoch: 00376 loss_train: 0.5089 loss_rec: 0.5089 acc_train: 0.7522 loss_val: 0.5518 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00377 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7457 loss_val: 0.5553 acc_val: 0.7468 time: 0.0065s\n",
      "Epoch: 00378 loss_train: 0.5016 loss_rec: 0.5016 acc_train: 0.7478 loss_val: 0.5583 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00379 loss_train: 0.5093 loss_rec: 0.5093 acc_train: 0.7500 loss_val: 0.5501 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00380 loss_train: 0.5078 loss_rec: 0.5078 acc_train: 0.7500 loss_val: 0.5410 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00381 loss_train: 0.5215 loss_rec: 0.5215 acc_train: 0.7435 loss_val: 0.5446 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00382 loss_train: 0.5176 loss_rec: 0.5176 acc_train: 0.7543 loss_val: 0.5481 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00383 loss_train: 0.5300 loss_rec: 0.5300 acc_train: 0.7457 loss_val: 0.5637 acc_val: 0.7273 time: 0.0051s\n",
      "Epoch: 00384 loss_train: 0.5258 loss_rec: 0.5258 acc_train: 0.7500 loss_val: 0.5737 acc_val: 0.7338 time: 0.0089s\n",
      "Epoch: 00385 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7478 loss_val: 0.5695 acc_val: 0.7078 time: 0.0066s\n",
      "Epoch: 00386 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7457 loss_val: 0.5485 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00387 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7500 loss_val: 0.5469 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00388 loss_train: 0.5388 loss_rec: 0.5388 acc_train: 0.7478 loss_val: 0.5842 acc_val: 0.7078 time: 0.0050s\n",
      "Epoch: 00389 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7435 loss_val: 0.5883 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00390 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7478 loss_val: 0.5574 acc_val: 0.7403 time: 0.0065s\n",
      "Epoch: 00391 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7543 loss_val: 0.5563 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00392 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7304 loss_val: 0.6013 acc_val: 0.7143 time: 0.0073s\n",
      "Epoch: 00393 loss_train: 0.5252 loss_rec: 0.5252 acc_train: 0.7457 loss_val: 0.5864 acc_val: 0.6883 time: 0.0072s\n",
      "Epoch: 00394 loss_train: 0.5184 loss_rec: 0.5184 acc_train: 0.7435 loss_val: 0.5653 acc_val: 0.7143 time: 0.0074s\n",
      "Epoch: 00395 loss_train: 0.5205 loss_rec: 0.5205 acc_train: 0.7435 loss_val: 0.5497 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00396 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7435 loss_val: 0.5628 acc_val: 0.7208 time: 0.0081s\n",
      "Epoch: 00397 loss_train: 0.5224 loss_rec: 0.5224 acc_train: 0.7391 loss_val: 0.5765 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00398 loss_train: 0.5121 loss_rec: 0.5121 acc_train: 0.7478 loss_val: 0.5244 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00399 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7500 loss_val: 0.5498 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00400 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7370 loss_val: 0.5593 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00401 loss_train: 0.5205 loss_rec: 0.5205 acc_train: 0.7391 loss_val: 0.5472 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00402 loss_train: 0.5138 loss_rec: 0.5138 acc_train: 0.7478 loss_val: 0.5366 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00403 loss_train: 0.5283 loss_rec: 0.5283 acc_train: 0.7413 loss_val: 0.5493 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00404 loss_train: 0.5089 loss_rec: 0.5089 acc_train: 0.7478 loss_val: 0.5563 acc_val: 0.7338 time: 0.0083s\n",
      "Epoch: 00405 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7413 loss_val: 0.5530 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00406 loss_train: 0.5164 loss_rec: 0.5164 acc_train: 0.7391 loss_val: 0.5686 acc_val: 0.7208 time: 0.0092s\n",
      "Epoch: 00407 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7478 loss_val: 0.5500 acc_val: 0.7338 time: 0.0077s\n",
      "Epoch: 00408 loss_train: 0.5157 loss_rec: 0.5157 acc_train: 0.7457 loss_val: 0.5654 acc_val: 0.7208 time: 0.0076s\n",
      "Epoch: 00409 loss_train: 0.5393 loss_rec: 0.5393 acc_train: 0.7457 loss_val: 0.5757 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00410 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7543 loss_val: 0.5585 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00411 loss_train: 0.5220 loss_rec: 0.5220 acc_train: 0.7457 loss_val: 0.5440 acc_val: 0.7403 time: 0.0081s\n",
      "Epoch: 00412 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7435 loss_val: 0.5698 acc_val: 0.7338 time: 0.0083s\n",
      "Epoch: 00413 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7435 loss_val: 0.5493 acc_val: 0.7143 time: 0.0072s\n",
      "Epoch: 00414 loss_train: 0.5246 loss_rec: 0.5246 acc_train: 0.7435 loss_val: 0.5684 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00415 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7522 loss_val: 0.5666 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00416 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7457 loss_val: 0.5438 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00417 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7413 loss_val: 0.5920 acc_val: 0.7208 time: 0.0083s\n",
      "Epoch: 00418 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7435 loss_val: 0.5618 acc_val: 0.7208 time: 0.0062s\n",
      "Epoch: 00419 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7478 loss_val: 0.5378 acc_val: 0.7468 time: 0.0073s\n",
      "Epoch: 00420 loss_train: 0.5234 loss_rec: 0.5234 acc_train: 0.7391 loss_val: 0.5792 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00421 loss_train: 0.5189 loss_rec: 0.5189 acc_train: 0.7413 loss_val: 0.5407 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00422 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7413 loss_val: 0.5646 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00423 loss_train: 0.5124 loss_rec: 0.5124 acc_train: 0.7457 loss_val: 0.5629 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00424 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7326 loss_val: 0.5568 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00425 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7565 loss_val: 0.5697 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 00426 loss_train: 0.5093 loss_rec: 0.5093 acc_train: 0.7522 loss_val: 0.5624 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00427 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7457 loss_val: 0.5523 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00428 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7478 loss_val: 0.5352 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00429 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7370 loss_val: 0.5601 acc_val: 0.7403 time: 0.0082s\n",
      "Epoch: 00430 loss_train: 0.5212 loss_rec: 0.5212 acc_train: 0.7413 loss_val: 0.5674 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00431 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7522 loss_val: 0.5805 acc_val: 0.7143 time: 0.0087s\n",
      "Epoch: 00432 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7478 loss_val: 0.5505 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00433 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7522 loss_val: 0.5724 acc_val: 0.7078 time: 0.0083s\n",
      "Epoch: 00434 loss_train: 0.5272 loss_rec: 0.5272 acc_train: 0.7522 loss_val: 0.5911 acc_val: 0.6948 time: 0.0083s\n",
      "Epoch: 00435 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7391 loss_val: 0.5738 acc_val: 0.7403 time: 0.0088s\n",
      "Epoch: 00436 loss_train: 0.5297 loss_rec: 0.5297 acc_train: 0.7413 loss_val: 0.5567 acc_val: 0.7468 time: 0.0099s\n",
      "Epoch: 00437 loss_train: 0.5272 loss_rec: 0.5272 acc_train: 0.7413 loss_val: 0.5656 acc_val: 0.7273 time: 0.0109s\n",
      "Epoch: 00438 loss_train: 0.5161 loss_rec: 0.5161 acc_train: 0.7522 loss_val: 0.5565 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00439 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7522 loss_val: 0.5531 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00440 loss_train: 0.5250 loss_rec: 0.5250 acc_train: 0.7457 loss_val: 0.5532 acc_val: 0.7143 time: 0.0110s\n",
      "Epoch: 00441 loss_train: 0.5150 loss_rec: 0.5150 acc_train: 0.7543 loss_val: 0.5506 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00442 loss_train: 0.5162 loss_rec: 0.5162 acc_train: 0.7478 loss_val: 0.5563 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00443 loss_train: 0.5098 loss_rec: 0.5098 acc_train: 0.7565 loss_val: 0.5478 acc_val: 0.7208 time: 0.0079s\n",
      "Epoch: 00444 loss_train: 0.5134 loss_rec: 0.5134 acc_train: 0.7543 loss_val: 0.5549 acc_val: 0.7273 time: 0.0074s\n",
      "Epoch: 00445 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7478 loss_val: 0.5465 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00446 loss_train: 0.5116 loss_rec: 0.5116 acc_train: 0.7500 loss_val: 0.5662 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00447 loss_train: 0.5196 loss_rec: 0.5196 acc_train: 0.7478 loss_val: 0.5516 acc_val: 0.7273 time: 0.0077s\n",
      "Epoch: 00448 loss_train: 0.5076 loss_rec: 0.5076 acc_train: 0.7478 loss_val: 0.5556 acc_val: 0.7143 time: 0.0060s\n",
      "Epoch: 00449 loss_train: 0.5131 loss_rec: 0.5131 acc_train: 0.7435 loss_val: 0.5666 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00450 loss_train: 0.5188 loss_rec: 0.5188 acc_train: 0.7500 loss_val: 0.5559 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00451 loss_train: 0.5154 loss_rec: 0.5154 acc_train: 0.7478 loss_val: 0.5633 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00452 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7500 loss_val: 0.5781 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00453 loss_train: 0.5251 loss_rec: 0.5251 acc_train: 0.7435 loss_val: 0.5437 acc_val: 0.7403 time: 0.0073s\n",
      "Epoch: 00454 loss_train: 0.5283 loss_rec: 0.5283 acc_train: 0.7391 loss_val: 0.5693 acc_val: 0.7078 time: 0.0050s\n",
      "Epoch: 00455 loss_train: 0.5253 loss_rec: 0.5253 acc_train: 0.7478 loss_val: 0.5748 acc_val: 0.7338 time: 0.0045s\n",
      "Epoch: 00456 loss_train: 0.5218 loss_rec: 0.5218 acc_train: 0.7478 loss_val: 0.5697 acc_val: 0.7208 time: 0.0063s\n",
      "Epoch: 00457 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7478 loss_val: 0.5487 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00458 loss_train: 0.5205 loss_rec: 0.5205 acc_train: 0.7457 loss_val: 0.5618 acc_val: 0.7273 time: 0.0062s\n",
      "Epoch: 00459 loss_train: 0.5039 loss_rec: 0.5039 acc_train: 0.7435 loss_val: 0.5502 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00460 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7304 loss_val: 0.5726 acc_val: 0.7273 time: 0.0062s\n",
      "Epoch: 00461 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7413 loss_val: 0.5652 acc_val: 0.7208 time: 0.0088s\n",
      "Epoch: 00462 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7500 loss_val: 0.5575 acc_val: 0.7468 time: 0.0071s\n",
      "Epoch: 00463 loss_train: 0.5224 loss_rec: 0.5224 acc_train: 0.7500 loss_val: 0.5631 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00464 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7478 loss_val: 0.5692 acc_val: 0.7338 time: 0.0063s\n",
      "Epoch: 00465 loss_train: 0.5008 loss_rec: 0.5008 acc_train: 0.7522 loss_val: 0.5755 acc_val: 0.7208 time: 0.0094s\n",
      "Epoch: 00466 loss_train: 0.5083 loss_rec: 0.5083 acc_train: 0.7543 loss_val: 0.5516 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00467 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7391 loss_val: 0.5893 acc_val: 0.7013 time: 0.0060s\n",
      "Epoch: 00468 loss_train: 0.5046 loss_rec: 0.5046 acc_train: 0.7500 loss_val: 0.5636 acc_val: 0.7532 time: 0.0080s\n",
      "Epoch: 00469 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7435 loss_val: 0.5779 acc_val: 0.7208 time: 0.0059s\n",
      "Epoch: 00470 loss_train: 0.5266 loss_rec: 0.5266 acc_train: 0.7391 loss_val: 0.5497 acc_val: 0.7403 time: 0.0078s\n",
      "Epoch: 00471 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7478 loss_val: 0.5462 acc_val: 0.7338 time: 0.0079s\n",
      "Epoch: 00472 loss_train: 0.5167 loss_rec: 0.5167 acc_train: 0.7500 loss_val: 0.5573 acc_val: 0.7468 time: 0.0067s\n",
      "Epoch: 00473 loss_train: 0.5327 loss_rec: 0.5327 acc_train: 0.7413 loss_val: 0.5729 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00474 loss_train: 0.5184 loss_rec: 0.5184 acc_train: 0.7522 loss_val: 0.5481 acc_val: 0.7338 time: 0.0052s\n",
      "Epoch: 00475 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7413 loss_val: 0.5634 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00476 loss_train: 0.5142 loss_rec: 0.5142 acc_train: 0.7478 loss_val: 0.5737 acc_val: 0.7338 time: 0.0069s\n",
      "Epoch: 00477 loss_train: 0.5180 loss_rec: 0.5180 acc_train: 0.7478 loss_val: 0.5701 acc_val: 0.7403 time: 0.0062s\n",
      "Epoch: 00478 loss_train: 0.5229 loss_rec: 0.5229 acc_train: 0.7478 loss_val: 0.5642 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00479 loss_train: 0.5296 loss_rec: 0.5296 acc_train: 0.7522 loss_val: 0.5601 acc_val: 0.7143 time: 0.0060s\n",
      "Epoch: 00480 loss_train: 0.5152 loss_rec: 0.5152 acc_train: 0.7283 loss_val: 0.5702 acc_val: 0.7143 time: 0.0081s\n",
      "Epoch: 00481 loss_train: 0.5328 loss_rec: 0.5328 acc_train: 0.7370 loss_val: 0.5593 acc_val: 0.7273 time: 0.0062s\n",
      "Epoch: 00482 loss_train: 0.5262 loss_rec: 0.5262 acc_train: 0.7500 loss_val: 0.5759 acc_val: 0.7403 time: 0.0082s\n",
      "Epoch: 00483 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7522 loss_val: 0.5896 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00484 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7522 loss_val: 0.5917 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00485 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7457 loss_val: 0.5647 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00486 loss_train: 0.5142 loss_rec: 0.5142 acc_train: 0.7457 loss_val: 0.5802 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00487 loss_train: 0.5215 loss_rec: 0.5215 acc_train: 0.7457 loss_val: 0.5630 acc_val: 0.7338 time: 0.0058s\n",
      "Epoch: 00488 loss_train: 0.5273 loss_rec: 0.5273 acc_train: 0.7348 loss_val: 0.5722 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00489 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7391 loss_val: 0.5711 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00490 loss_train: 0.5160 loss_rec: 0.5160 acc_train: 0.7435 loss_val: 0.5812 acc_val: 0.7208 time: 0.0045s\n",
      "Epoch: 00491 loss_train: 0.5289 loss_rec: 0.5289 acc_train: 0.7391 loss_val: 0.5573 acc_val: 0.7403 time: 0.0055s\n",
      "Epoch: 00492 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7478 loss_val: 0.5523 acc_val: 0.7338 time: 0.0094s\n",
      "Epoch: 00493 loss_train: 0.5083 loss_rec: 0.5083 acc_train: 0.7543 loss_val: 0.5526 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00494 loss_train: 0.5115 loss_rec: 0.5115 acc_train: 0.7478 loss_val: 0.5606 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00495 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7370 loss_val: 0.5711 acc_val: 0.7273 time: 0.0068s\n",
      "Epoch: 00496 loss_train: 0.5279 loss_rec: 0.5279 acc_train: 0.7435 loss_val: 0.5571 acc_val: 0.7208 time: 0.0062s\n",
      "Epoch: 00497 loss_train: 0.5216 loss_rec: 0.5216 acc_train: 0.7500 loss_val: 0.5747 acc_val: 0.7403 time: 0.0063s\n",
      "Epoch: 00498 loss_train: 0.5071 loss_rec: 0.5071 acc_train: 0.7457 loss_val: 0.5927 acc_val: 0.7338 time: 0.0077s\n",
      "Epoch: 00499 loss_train: 0.5193 loss_rec: 0.5193 acc_train: 0.7413 loss_val: 0.5533 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00500 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7522 loss_val: 0.5532 acc_val: 0.7338 time: 0.0078s\n",
      "Epoch: 00501 loss_train: 0.5238 loss_rec: 0.5238 acc_train: 0.7413 loss_val: 0.5696 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00502 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7457 loss_val: 0.5924 acc_val: 0.7208 time: 0.0063s\n",
      "Epoch: 00503 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7435 loss_val: 0.5409 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00504 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7413 loss_val: 0.5665 acc_val: 0.7208 time: 0.0072s\n",
      "Epoch: 00505 loss_train: 0.5196 loss_rec: 0.5196 acc_train: 0.7543 loss_val: 0.5836 acc_val: 0.7013 time: 0.0070s\n",
      "Epoch: 00506 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7413 loss_val: 0.5905 acc_val: 0.7273 time: 0.0063s\n",
      "Epoch: 00507 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7478 loss_val: 0.5618 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00508 loss_train: 0.5073 loss_rec: 0.5073 acc_train: 0.7543 loss_val: 0.5653 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00509 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7565 loss_val: 0.5734 acc_val: 0.7273 time: 0.0055s\n",
      "Epoch: 00510 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7478 loss_val: 0.5811 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00511 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7478 loss_val: 0.5840 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00512 loss_train: 0.5368 loss_rec: 0.5368 acc_train: 0.7435 loss_val: 0.5593 acc_val: 0.7338 time: 0.0073s\n",
      "Epoch: 00513 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7457 loss_val: 0.5552 acc_val: 0.7273 time: 0.0084s\n",
      "Epoch: 00514 loss_train: 0.5045 loss_rec: 0.5045 acc_train: 0.7565 loss_val: 0.5841 acc_val: 0.6818 time: 0.0083s\n",
      "Epoch: 00515 loss_train: 0.5247 loss_rec: 0.5247 acc_train: 0.7348 loss_val: 0.5498 acc_val: 0.7078 time: 0.0076s\n",
      "Epoch: 00516 loss_train: 0.5131 loss_rec: 0.5131 acc_train: 0.7413 loss_val: 0.5900 acc_val: 0.6948 time: 0.0067s\n",
      "Epoch: 00517 loss_train: 0.5253 loss_rec: 0.5253 acc_train: 0.7413 loss_val: 0.5518 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00518 loss_train: 0.5224 loss_rec: 0.5224 acc_train: 0.7500 loss_val: 0.5657 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00519 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7413 loss_val: 0.5182 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00520 loss_train: 0.5092 loss_rec: 0.5092 acc_train: 0.7370 loss_val: 0.5287 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00521 loss_train: 0.5258 loss_rec: 0.5258 acc_train: 0.7413 loss_val: 0.5934 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00522 loss_train: 0.5322 loss_rec: 0.5322 acc_train: 0.7413 loss_val: 0.5428 acc_val: 0.7338 time: 0.0125s\n",
      "Epoch: 00523 loss_train: 0.5054 loss_rec: 0.5054 acc_train: 0.7413 loss_val: 0.5700 acc_val: 0.7273 time: 0.0051s\n",
      "Epoch: 00524 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7565 loss_val: 0.5608 acc_val: 0.7208 time: 0.0072s\n",
      "Epoch: 00525 loss_train: 0.5149 loss_rec: 0.5149 acc_train: 0.7522 loss_val: 0.5793 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00526 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7500 loss_val: 0.5551 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00527 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7413 loss_val: 0.5704 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00528 loss_train: 0.5145 loss_rec: 0.5145 acc_train: 0.7370 loss_val: 0.5850 acc_val: 0.7273 time: 0.0062s\n",
      "Epoch: 00529 loss_train: 0.5178 loss_rec: 0.5178 acc_train: 0.7391 loss_val: 0.5621 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00530 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7457 loss_val: 0.5804 acc_val: 0.7013 time: 0.0070s\n",
      "Epoch: 00531 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7413 loss_val: 0.5638 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00532 loss_train: 0.5271 loss_rec: 0.5271 acc_train: 0.7391 loss_val: 0.5412 acc_val: 0.7208 time: 0.0060s\n",
      "Epoch: 00533 loss_train: 0.5099 loss_rec: 0.5099 acc_train: 0.7478 loss_val: 0.5589 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00534 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7435 loss_val: 0.5608 acc_val: 0.7403 time: 0.0069s\n",
      "Epoch: 00535 loss_train: 0.5171 loss_rec: 0.5171 acc_train: 0.7435 loss_val: 0.5368 acc_val: 0.7338 time: 0.0055s\n",
      "Epoch: 00536 loss_train: 0.5178 loss_rec: 0.5178 acc_train: 0.7391 loss_val: 0.5534 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00537 loss_train: 0.5121 loss_rec: 0.5121 acc_train: 0.7478 loss_val: 0.5647 acc_val: 0.7143 time: 0.0071s\n",
      "Epoch: 00538 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7478 loss_val: 0.5512 acc_val: 0.7468 time: 0.0076s\n",
      "Epoch: 00539 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7522 loss_val: 0.5425 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00540 loss_train: 0.5080 loss_rec: 0.5080 acc_train: 0.7413 loss_val: 0.5408 acc_val: 0.7338 time: 0.0055s\n",
      "Epoch: 00541 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7500 loss_val: 0.5520 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00542 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7413 loss_val: 0.5623 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00543 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7304 loss_val: 0.5571 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00544 loss_train: 0.5093 loss_rec: 0.5093 acc_train: 0.7500 loss_val: 0.5331 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00545 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7413 loss_val: 0.5540 acc_val: 0.7208 time: 0.0073s\n",
      "Epoch: 00546 loss_train: 0.5102 loss_rec: 0.5102 acc_train: 0.7478 loss_val: 0.5610 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00547 loss_train: 0.5259 loss_rec: 0.5259 acc_train: 0.7435 loss_val: 0.6004 acc_val: 0.7013 time: 0.0065s\n",
      "Epoch: 00548 loss_train: 0.5151 loss_rec: 0.5151 acc_train: 0.7522 loss_val: 0.5635 acc_val: 0.7078 time: 0.0073s\n",
      "Epoch: 00549 loss_train: 0.5149 loss_rec: 0.5149 acc_train: 0.7500 loss_val: 0.5595 acc_val: 0.7338 time: 0.0060s\n",
      "Epoch: 00550 loss_train: 0.5198 loss_rec: 0.5198 acc_train: 0.7174 loss_val: 0.5828 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00551 loss_train: 0.5355 loss_rec: 0.5355 acc_train: 0.7413 loss_val: 0.5421 acc_val: 0.7273 time: 0.0069s\n",
      "Epoch: 00552 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7413 loss_val: 0.5646 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00553 loss_train: 0.5104 loss_rec: 0.5104 acc_train: 0.7413 loss_val: 0.5703 acc_val: 0.7208 time: 0.0050s\n",
      "Epoch: 00554 loss_train: 0.5262 loss_rec: 0.5262 acc_train: 0.7413 loss_val: 0.5773 acc_val: 0.7273 time: 0.0073s\n",
      "Epoch: 00555 loss_train: 0.5184 loss_rec: 0.5184 acc_train: 0.7348 loss_val: 0.5760 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00556 loss_train: 0.5123 loss_rec: 0.5123 acc_train: 0.7500 loss_val: 0.5773 acc_val: 0.7143 time: 0.0060s\n",
      "Epoch: 00557 loss_train: 0.5122 loss_rec: 0.5122 acc_train: 0.7348 loss_val: 0.5750 acc_val: 0.7208 time: 0.0089s\n",
      "Epoch: 00558 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7413 loss_val: 0.5919 acc_val: 0.7468 time: 0.0077s\n",
      "Epoch: 00559 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7522 loss_val: 0.5398 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00560 loss_train: 0.5248 loss_rec: 0.5248 acc_train: 0.7457 loss_val: 0.5691 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00561 loss_train: 0.5256 loss_rec: 0.5256 acc_train: 0.7435 loss_val: 0.5426 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00562 loss_train: 0.5119 loss_rec: 0.5119 acc_train: 0.7457 loss_val: 0.5410 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00563 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7413 loss_val: 0.5718 acc_val: 0.7338 time: 0.0071s\n",
      "Epoch: 00564 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7500 loss_val: 0.5560 acc_val: 0.7273 time: 0.0066s\n",
      "Epoch: 00565 loss_train: 0.5256 loss_rec: 0.5256 acc_train: 0.7500 loss_val: 0.5727 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00566 loss_train: 0.5097 loss_rec: 0.5097 acc_train: 0.7478 loss_val: 0.5908 acc_val: 0.7143 time: 0.0063s\n",
      "Epoch: 00567 loss_train: 0.5151 loss_rec: 0.5151 acc_train: 0.7478 loss_val: 0.5547 acc_val: 0.7468 time: 0.0073s\n",
      "Epoch: 00568 loss_train: 0.5377 loss_rec: 0.5377 acc_train: 0.7370 loss_val: 0.5641 acc_val: 0.7468 time: 0.0083s\n",
      "Epoch: 00569 loss_train: 0.5225 loss_rec: 0.5225 acc_train: 0.7478 loss_val: 0.5575 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00570 loss_train: 0.5167 loss_rec: 0.5167 acc_train: 0.7522 loss_val: 0.5557 acc_val: 0.7403 time: 0.0077s\n",
      "Epoch: 00571 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7478 loss_val: 0.5500 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00572 loss_train: 0.5230 loss_rec: 0.5230 acc_train: 0.7413 loss_val: 0.5608 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00573 loss_train: 0.5116 loss_rec: 0.5116 acc_train: 0.7435 loss_val: 0.5696 acc_val: 0.7013 time: 0.0060s\n",
      "Epoch: 00574 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7522 loss_val: 0.5511 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00575 loss_train: 0.5306 loss_rec: 0.5306 acc_train: 0.7370 loss_val: 0.5644 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00576 loss_train: 0.5248 loss_rec: 0.5248 acc_train: 0.7457 loss_val: 0.5744 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00577 loss_train: 0.5291 loss_rec: 0.5291 acc_train: 0.7391 loss_val: 0.5577 acc_val: 0.7013 time: 0.0072s\n",
      "Epoch: 00578 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7391 loss_val: 0.5661 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00579 loss_train: 0.5284 loss_rec: 0.5284 acc_train: 0.7391 loss_val: 0.5393 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00580 loss_train: 0.5231 loss_rec: 0.5231 acc_train: 0.7391 loss_val: 0.5823 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00581 loss_train: 0.5235 loss_rec: 0.5235 acc_train: 0.7413 loss_val: 0.5493 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00582 loss_train: 0.5217 loss_rec: 0.5217 acc_train: 0.7391 loss_val: 0.5682 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00583 loss_train: 0.5280 loss_rec: 0.5280 acc_train: 0.7478 loss_val: 0.5771 acc_val: 0.7468 time: 0.0085s\n",
      "Epoch: 00584 loss_train: 0.5273 loss_rec: 0.5273 acc_train: 0.7326 loss_val: 0.5867 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00585 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7457 loss_val: 0.5532 acc_val: 0.7532 time: 0.0090s\n",
      "Epoch: 00586 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7500 loss_val: 0.5542 acc_val: 0.7338 time: 0.0068s\n",
      "Epoch: 00587 loss_train: 0.5240 loss_rec: 0.5240 acc_train: 0.7500 loss_val: 0.5612 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00588 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7391 loss_val: 0.5435 acc_val: 0.7208 time: 0.0065s\n",
      "Epoch: 00589 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7435 loss_val: 0.5486 acc_val: 0.7403 time: 0.0083s\n",
      "Epoch: 00590 loss_train: 0.5100 loss_rec: 0.5100 acc_train: 0.7413 loss_val: 0.5618 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00591 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7413 loss_val: 0.5467 acc_val: 0.7208 time: 0.0074s\n",
      "Epoch: 00592 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7435 loss_val: 0.5752 acc_val: 0.7208 time: 0.0068s\n",
      "Epoch: 00593 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7391 loss_val: 0.5405 acc_val: 0.7208 time: 0.0089s\n",
      "Epoch: 00594 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7435 loss_val: 0.5583 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00595 loss_train: 0.5114 loss_rec: 0.5114 acc_train: 0.7457 loss_val: 0.5786 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00596 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7435 loss_val: 0.5728 acc_val: 0.7403 time: 0.0084s\n",
      "Epoch: 00597 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7457 loss_val: 0.5606 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00598 loss_train: 0.5285 loss_rec: 0.5285 acc_train: 0.7500 loss_val: 0.5606 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00599 loss_train: 0.5260 loss_rec: 0.5260 acc_train: 0.7413 loss_val: 0.5541 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00600 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7435 loss_val: 0.5589 acc_val: 0.7403 time: 0.0065s\n",
      "Epoch: 00601 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7413 loss_val: 0.5732 acc_val: 0.7273 time: 0.0079s\n",
      "Epoch: 00602 loss_train: 0.5313 loss_rec: 0.5313 acc_train: 0.7413 loss_val: 0.5625 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00603 loss_train: 0.5225 loss_rec: 0.5225 acc_train: 0.7478 loss_val: 0.5775 acc_val: 0.7143 time: 0.0079s\n",
      "Epoch: 00604 loss_train: 0.5136 loss_rec: 0.5136 acc_train: 0.7435 loss_val: 0.5532 acc_val: 0.7338 time: 0.0071s\n",
      "Epoch: 00605 loss_train: 0.5176 loss_rec: 0.5176 acc_train: 0.7500 loss_val: 0.5796 acc_val: 0.7078 time: 0.0076s\n",
      "Epoch: 00606 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7413 loss_val: 0.5715 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00607 loss_train: 0.5240 loss_rec: 0.5240 acc_train: 0.7478 loss_val: 0.5875 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00608 loss_train: 0.5234 loss_rec: 0.5234 acc_train: 0.7370 loss_val: 0.5770 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00609 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7478 loss_val: 0.5814 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00610 loss_train: 0.5277 loss_rec: 0.5277 acc_train: 0.7435 loss_val: 0.5609 acc_val: 0.7273 time: 0.0060s\n",
      "Epoch: 00611 loss_train: 0.5028 loss_rec: 0.5028 acc_train: 0.7500 loss_val: 0.5804 acc_val: 0.7403 time: 0.0064s\n",
      "Epoch: 00612 loss_train: 0.5113 loss_rec: 0.5113 acc_train: 0.7522 loss_val: 0.5662 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00613 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7457 loss_val: 0.5449 acc_val: 0.7403 time: 0.0061s\n",
      "Epoch: 00614 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7457 loss_val: 0.5504 acc_val: 0.7338 time: 0.0063s\n",
      "Epoch: 00615 loss_train: 0.5318 loss_rec: 0.5318 acc_train: 0.7391 loss_val: 0.5480 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00616 loss_train: 0.5196 loss_rec: 0.5196 acc_train: 0.7478 loss_val: 0.5678 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00617 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7457 loss_val: 0.5652 acc_val: 0.7273 time: 0.0070s\n",
      "Epoch: 00618 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7500 loss_val: 0.5590 acc_val: 0.7338 time: 0.0055s\n",
      "Epoch: 00619 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7413 loss_val: 0.5964 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00620 loss_train: 0.5113 loss_rec: 0.5113 acc_train: 0.7478 loss_val: 0.5806 acc_val: 0.7273 time: 0.0050s\n",
      "Epoch: 00621 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7457 loss_val: 0.5580 acc_val: 0.7468 time: 0.0069s\n",
      "Epoch: 00622 loss_train: 0.5095 loss_rec: 0.5095 acc_train: 0.7587 loss_val: 0.5612 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00623 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7413 loss_val: 0.5921 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00624 loss_train: 0.5256 loss_rec: 0.5256 acc_train: 0.7435 loss_val: 0.5668 acc_val: 0.7273 time: 0.0050s\n",
      "Epoch: 00625 loss_train: 0.5220 loss_rec: 0.5220 acc_train: 0.7457 loss_val: 0.5683 acc_val: 0.7338 time: 0.0062s\n",
      "Epoch: 00626 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7457 loss_val: 0.5368 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00627 loss_train: 0.5102 loss_rec: 0.5102 acc_train: 0.7500 loss_val: 0.5565 acc_val: 0.7338 time: 0.0072s\n",
      "Epoch: 00628 loss_train: 0.5154 loss_rec: 0.5154 acc_train: 0.7500 loss_val: 0.5624 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00629 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7478 loss_val: 0.5821 acc_val: 0.7338 time: 0.0076s\n",
      "Epoch: 00630 loss_train: 0.5302 loss_rec: 0.5302 acc_train: 0.7435 loss_val: 0.5661 acc_val: 0.7338 time: 0.0067s\n",
      "Epoch: 00631 loss_train: 0.5198 loss_rec: 0.5198 acc_train: 0.7391 loss_val: 0.5645 acc_val: 0.7143 time: 0.0073s\n",
      "Epoch: 00632 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7500 loss_val: 0.5968 acc_val: 0.7338 time: 0.0063s\n",
      "Epoch: 00633 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7391 loss_val: 0.5836 acc_val: 0.7143 time: 0.0070s\n",
      "Epoch: 00634 loss_train: 0.5268 loss_rec: 0.5268 acc_train: 0.7391 loss_val: 0.5637 acc_val: 0.7143 time: 0.0064s\n",
      "Epoch: 00635 loss_train: 0.5115 loss_rec: 0.5115 acc_train: 0.7478 loss_val: 0.5610 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00636 loss_train: 0.5184 loss_rec: 0.5184 acc_train: 0.7478 loss_val: 0.5843 acc_val: 0.7403 time: 0.0078s\n",
      "Epoch: 00637 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7326 loss_val: 0.5753 acc_val: 0.7078 time: 0.0073s\n",
      "Epoch: 00638 loss_train: 0.5149 loss_rec: 0.5149 acc_train: 0.7478 loss_val: 0.5935 acc_val: 0.7208 time: 0.0076s\n",
      "Epoch: 00639 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7500 loss_val: 0.5603 acc_val: 0.7338 time: 0.0162s\n",
      "Epoch: 00640 loss_train: 0.5136 loss_rec: 0.5136 acc_train: 0.7457 loss_val: 0.5714 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00641 loss_train: 0.5159 loss_rec: 0.5159 acc_train: 0.7500 loss_val: 0.5453 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00642 loss_train: 0.5189 loss_rec: 0.5189 acc_train: 0.7435 loss_val: 0.5647 acc_val: 0.7403 time: 0.0067s\n",
      "Epoch: 00643 loss_train: 0.5259 loss_rec: 0.5259 acc_train: 0.7435 loss_val: 0.5733 acc_val: 0.7208 time: 0.0061s\n",
      "Epoch: 00644 loss_train: 0.5306 loss_rec: 0.5306 acc_train: 0.7457 loss_val: 0.5663 acc_val: 0.7338 time: 0.0065s\n",
      "Epoch: 00645 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7457 loss_val: 0.5732 acc_val: 0.7273 time: 0.0055s\n",
      "Epoch: 00646 loss_train: 0.5152 loss_rec: 0.5152 acc_train: 0.7435 loss_val: 0.5861 acc_val: 0.7468 time: 0.0082s\n",
      "Epoch: 00647 loss_train: 0.5188 loss_rec: 0.5188 acc_train: 0.7413 loss_val: 0.5766 acc_val: 0.7273 time: 0.0071s\n",
      "Epoch: 00648 loss_train: 0.5052 loss_rec: 0.5052 acc_train: 0.7478 loss_val: 0.5568 acc_val: 0.7403 time: 0.0071s\n",
      "Epoch: 00649 loss_train: 0.5197 loss_rec: 0.5197 acc_train: 0.7587 loss_val: 0.5659 acc_val: 0.7078 time: 0.0070s\n",
      "Epoch: 00650 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7283 loss_val: 0.5533 acc_val: 0.7468 time: 0.0073s\n",
      "Epoch: 00651 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7391 loss_val: 0.5553 acc_val: 0.7143 time: 0.0073s\n",
      "Epoch: 00652 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7500 loss_val: 0.5538 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00653 loss_train: 0.5284 loss_rec: 0.5284 acc_train: 0.7478 loss_val: 0.5574 acc_val: 0.7273 time: 0.0061s\n",
      "Epoch: 00654 loss_train: 0.5080 loss_rec: 0.5080 acc_train: 0.7522 loss_val: 0.5619 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00655 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7522 loss_val: 0.5709 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00656 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7435 loss_val: 0.5699 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00657 loss_train: 0.5292 loss_rec: 0.5292 acc_train: 0.7457 loss_val: 0.5830 acc_val: 0.7143 time: 0.0067s\n",
      "Epoch: 00658 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7326 loss_val: 0.5665 acc_val: 0.7078 time: 0.0068s\n",
      "Epoch: 00659 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7478 loss_val: 0.5694 acc_val: 0.7338 time: 0.0075s\n",
      "Epoch: 00660 loss_train: 0.5222 loss_rec: 0.5222 acc_train: 0.7413 loss_val: 0.5744 acc_val: 0.7208 time: 0.0067s\n",
      "Epoch: 00661 loss_train: 0.5255 loss_rec: 0.5255 acc_train: 0.7435 loss_val: 0.5757 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00662 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7478 loss_val: 0.5794 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00663 loss_train: 0.5181 loss_rec: 0.5181 acc_train: 0.7457 loss_val: 0.5537 acc_val: 0.7468 time: 0.0075s\n",
      "Epoch: 00664 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7478 loss_val: 0.5699 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00665 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7478 loss_val: 0.5698 acc_val: 0.7403 time: 0.0079s\n",
      "Epoch: 00666 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7435 loss_val: 0.5569 acc_val: 0.7403 time: 0.0084s\n",
      "Epoch: 00667 loss_train: 0.5181 loss_rec: 0.5181 acc_train: 0.7413 loss_val: 0.5816 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00668 loss_train: 0.5151 loss_rec: 0.5151 acc_train: 0.7522 loss_val: 0.5932 acc_val: 0.7078 time: 0.0094s\n",
      "Epoch: 00669 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7565 loss_val: 0.5373 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00670 loss_train: 0.5067 loss_rec: 0.5067 acc_train: 0.7457 loss_val: 0.5620 acc_val: 0.7468 time: 0.0093s\n",
      "Epoch: 00671 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7587 loss_val: 0.5553 acc_val: 0.7532 time: 0.0096s\n",
      "Epoch: 00672 loss_train: 0.5082 loss_rec: 0.5082 acc_train: 0.7435 loss_val: 0.6122 acc_val: 0.7078 time: 0.0075s\n",
      "Epoch: 00673 loss_train: 0.5137 loss_rec: 0.5137 acc_train: 0.7435 loss_val: 0.5794 acc_val: 0.7143 time: 0.0092s\n",
      "Epoch: 00674 loss_train: 0.5113 loss_rec: 0.5113 acc_train: 0.7478 loss_val: 0.5829 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00675 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7304 loss_val: 0.5810 acc_val: 0.7013 time: 0.0092s\n",
      "Epoch: 00676 loss_train: 0.5104 loss_rec: 0.5104 acc_train: 0.7391 loss_val: 0.5917 acc_val: 0.6948 time: 0.0050s\n",
      "Epoch: 00677 loss_train: 0.5094 loss_rec: 0.5094 acc_train: 0.7370 loss_val: 0.5760 acc_val: 0.7273 time: 0.0078s\n",
      "Epoch: 00678 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7326 loss_val: 0.5598 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00679 loss_train: 0.5144 loss_rec: 0.5144 acc_train: 0.7435 loss_val: 0.5544 acc_val: 0.7273 time: 0.0089s\n",
      "Epoch: 00680 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7500 loss_val: 0.5743 acc_val: 0.7208 time: 0.0055s\n",
      "Epoch: 00681 loss_train: 0.5377 loss_rec: 0.5377 acc_train: 0.7391 loss_val: 0.5738 acc_val: 0.7403 time: 0.0072s\n",
      "Epoch: 00682 loss_train: 0.5193 loss_rec: 0.5193 acc_train: 0.7391 loss_val: 0.5490 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00683 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7522 loss_val: 0.5620 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00684 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7457 loss_val: 0.5604 acc_val: 0.7468 time: 0.0070s\n",
      "Epoch: 00685 loss_train: 0.5255 loss_rec: 0.5255 acc_train: 0.7522 loss_val: 0.5481 acc_val: 0.7338 time: 0.0070s\n",
      "Epoch: 00686 loss_train: 0.5255 loss_rec: 0.5255 acc_train: 0.7457 loss_val: 0.5611 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00687 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7478 loss_val: 0.5654 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00688 loss_train: 0.5142 loss_rec: 0.5142 acc_train: 0.7478 loss_val: 0.5538 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00689 loss_train: 0.5230 loss_rec: 0.5230 acc_train: 0.7391 loss_val: 0.5500 acc_val: 0.7338 time: 0.0092s\n",
      "Epoch: 00690 loss_train: 0.5147 loss_rec: 0.5147 acc_train: 0.7435 loss_val: 0.5311 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00691 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7457 loss_val: 0.5448 acc_val: 0.7338 time: 0.0108s\n",
      "Epoch: 00692 loss_train: 0.5260 loss_rec: 0.5260 acc_train: 0.7478 loss_val: 0.5768 acc_val: 0.7273 time: 0.0120s\n",
      "Epoch: 00693 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7500 loss_val: 0.5690 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00694 loss_train: 0.5291 loss_rec: 0.5291 acc_train: 0.7391 loss_val: 0.5893 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00695 loss_train: 0.5088 loss_rec: 0.5088 acc_train: 0.7543 loss_val: 0.5674 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00696 loss_train: 0.5104 loss_rec: 0.5104 acc_train: 0.7478 loss_val: 0.5549 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00697 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7457 loss_val: 0.5588 acc_val: 0.7208 time: 0.0085s\n",
      "Epoch: 00698 loss_train: 0.5290 loss_rec: 0.5290 acc_train: 0.7457 loss_val: 0.5677 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00699 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7348 loss_val: 0.5793 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00700 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7500 loss_val: 0.5657 acc_val: 0.7208 time: 0.0070s\n",
      "Epoch: 00701 loss_train: 0.5147 loss_rec: 0.5147 acc_train: 0.7500 loss_val: 0.5470 acc_val: 0.7468 time: 0.0101s\n",
      "Epoch: 00702 loss_train: 0.5244 loss_rec: 0.5244 acc_train: 0.7457 loss_val: 0.5630 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00703 loss_train: 0.5136 loss_rec: 0.5136 acc_train: 0.7391 loss_val: 0.5735 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00704 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7522 loss_val: 0.5622 acc_val: 0.7273 time: 0.0078s\n",
      "Epoch: 00705 loss_train: 0.5021 loss_rec: 0.5021 acc_train: 0.7478 loss_val: 0.5712 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00706 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7304 loss_val: 0.5591 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00707 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7370 loss_val: 0.5597 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00708 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7457 loss_val: 0.5562 acc_val: 0.7208 time: 0.0075s\n",
      "Epoch: 00709 loss_train: 0.5277 loss_rec: 0.5277 acc_train: 0.7500 loss_val: 0.5368 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00710 loss_train: 0.5030 loss_rec: 0.5030 acc_train: 0.7478 loss_val: 0.5722 acc_val: 0.7532 time: 0.0069s\n",
      "Epoch: 00711 loss_train: 0.5128 loss_rec: 0.5128 acc_train: 0.7435 loss_val: 0.5305 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00712 loss_train: 0.5197 loss_rec: 0.5197 acc_train: 0.7370 loss_val: 0.5963 acc_val: 0.7143 time: 0.0097s\n",
      "Epoch: 00713 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7348 loss_val: 0.5861 acc_val: 0.7403 time: 0.0094s\n",
      "Epoch: 00714 loss_train: 0.5164 loss_rec: 0.5164 acc_train: 0.7457 loss_val: 0.5591 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00715 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7478 loss_val: 0.5507 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00716 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7413 loss_val: 0.5644 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00717 loss_train: 0.5150 loss_rec: 0.5150 acc_train: 0.7435 loss_val: 0.5879 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00718 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7435 loss_val: 0.5660 acc_val: 0.7403 time: 0.0110s\n",
      "Epoch: 00719 loss_train: 0.5238 loss_rec: 0.5238 acc_train: 0.7500 loss_val: 0.5620 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00720 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7478 loss_val: 0.5502 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00721 loss_train: 0.5134 loss_rec: 0.5134 acc_train: 0.7457 loss_val: 0.5677 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00722 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7370 loss_val: 0.5950 acc_val: 0.7013 time: 0.0087s\n",
      "Epoch: 00723 loss_train: 0.5307 loss_rec: 0.5307 acc_train: 0.7413 loss_val: 0.5558 acc_val: 0.7338 time: 0.0075s\n",
      "Epoch: 00724 loss_train: 0.5138 loss_rec: 0.5138 acc_train: 0.7435 loss_val: 0.5630 acc_val: 0.7403 time: 0.0096s\n",
      "Epoch: 00725 loss_train: 0.5101 loss_rec: 0.5101 acc_train: 0.7413 loss_val: 0.5771 acc_val: 0.7338 time: 0.0061s\n",
      "Epoch: 00726 loss_train: 0.5128 loss_rec: 0.5128 acc_train: 0.7283 loss_val: 0.5594 acc_val: 0.7208 time: 0.0084s\n",
      "Epoch: 00727 loss_train: 0.5249 loss_rec: 0.5249 acc_train: 0.7457 loss_val: 0.5556 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00728 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7478 loss_val: 0.5636 acc_val: 0.7208 time: 0.0089s\n",
      "Epoch: 00729 loss_train: 0.5219 loss_rec: 0.5219 acc_train: 0.7478 loss_val: 0.5719 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00730 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7500 loss_val: 0.5722 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00731 loss_train: 0.5094 loss_rec: 0.5094 acc_train: 0.7457 loss_val: 0.5650 acc_val: 0.7403 time: 0.0097s\n",
      "Epoch: 00732 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7522 loss_val: 0.5916 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00733 loss_train: 0.5327 loss_rec: 0.5327 acc_train: 0.7370 loss_val: 0.5715 acc_val: 0.7338 time: 0.0083s\n",
      "Epoch: 00734 loss_train: 0.5095 loss_rec: 0.5095 acc_train: 0.7500 loss_val: 0.6028 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00735 loss_train: 0.5056 loss_rec: 0.5056 acc_train: 0.7522 loss_val: 0.5647 acc_val: 0.7403 time: 0.0086s\n",
      "Epoch: 00736 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7391 loss_val: 0.5587 acc_val: 0.7338 time: 0.0101s\n",
      "Epoch: 00737 loss_train: 0.5054 loss_rec: 0.5054 acc_train: 0.7478 loss_val: 0.5631 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00738 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7457 loss_val: 0.5790 acc_val: 0.7208 time: 0.0086s\n",
      "Epoch: 00739 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7457 loss_val: 0.5778 acc_val: 0.7338 time: 0.0092s\n",
      "Epoch: 00740 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7457 loss_val: 0.5723 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00741 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7370 loss_val: 0.5566 acc_val: 0.6948 time: 0.0081s\n",
      "Epoch: 00742 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7457 loss_val: 0.5579 acc_val: 0.7208 time: 0.0078s\n",
      "Epoch: 00743 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7500 loss_val: 0.5639 acc_val: 0.7403 time: 0.0105s\n",
      "Epoch: 00744 loss_train: 0.5325 loss_rec: 0.5325 acc_train: 0.7435 loss_val: 0.5482 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00745 loss_train: 0.5164 loss_rec: 0.5164 acc_train: 0.7391 loss_val: 0.5887 acc_val: 0.7273 time: 0.0096s\n",
      "Epoch: 00746 loss_train: 0.5104 loss_rec: 0.5104 acc_train: 0.7457 loss_val: 0.5632 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00747 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7435 loss_val: 0.5806 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00748 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7478 loss_val: 0.5747 acc_val: 0.7403 time: 0.0060s\n",
      "Epoch: 00749 loss_train: 0.5217 loss_rec: 0.5217 acc_train: 0.7370 loss_val: 0.5712 acc_val: 0.7078 time: 0.0080s\n",
      "Epoch: 00750 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7261 loss_val: 0.5589 acc_val: 0.7078 time: 0.0091s\n",
      "Epoch: 00751 loss_train: 0.5251 loss_rec: 0.5251 acc_train: 0.7478 loss_val: 0.5516 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00752 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7413 loss_val: 0.5718 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00753 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7370 loss_val: 0.5705 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00754 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7522 loss_val: 0.5625 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00755 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7565 loss_val: 0.5636 acc_val: 0.7403 time: 0.0118s\n",
      "Epoch: 00756 loss_train: 0.5236 loss_rec: 0.5236 acc_train: 0.7413 loss_val: 0.5898 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00757 loss_train: 0.5175 loss_rec: 0.5175 acc_train: 0.7478 loss_val: 0.5656 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00758 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7522 loss_val: 0.5778 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00759 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7500 loss_val: 0.5737 acc_val: 0.7208 time: 0.0095s\n",
      "Epoch: 00760 loss_train: 0.5244 loss_rec: 0.5244 acc_train: 0.7391 loss_val: 0.5785 acc_val: 0.7273 time: 0.0083s\n",
      "Epoch: 00761 loss_train: 0.5225 loss_rec: 0.5225 acc_train: 0.7413 loss_val: 0.5765 acc_val: 0.7208 time: 0.0083s\n",
      "Epoch: 00762 loss_train: 0.5218 loss_rec: 0.5218 acc_train: 0.7435 loss_val: 0.5378 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00763 loss_train: 0.5162 loss_rec: 0.5162 acc_train: 0.7500 loss_val: 0.5830 acc_val: 0.7273 time: 0.0061s\n",
      "Epoch: 00764 loss_train: 0.5143 loss_rec: 0.5143 acc_train: 0.7457 loss_val: 0.5621 acc_val: 0.7273 time: 0.0097s\n",
      "Epoch: 00765 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7457 loss_val: 0.5521 acc_val: 0.7468 time: 0.0097s\n",
      "Epoch: 00766 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7457 loss_val: 0.5599 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00767 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7391 loss_val: 0.5825 acc_val: 0.7403 time: 0.0111s\n",
      "Epoch: 00768 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7457 loss_val: 0.5588 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00769 loss_train: 0.5053 loss_rec: 0.5053 acc_train: 0.7478 loss_val: 0.5853 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00770 loss_train: 0.5160 loss_rec: 0.5160 acc_train: 0.7457 loss_val: 0.5839 acc_val: 0.7403 time: 0.0089s\n",
      "Epoch: 00771 loss_train: 0.5103 loss_rec: 0.5103 acc_train: 0.7457 loss_val: 0.5523 acc_val: 0.7208 time: 0.0100s\n",
      "Epoch: 00772 loss_train: 0.5217 loss_rec: 0.5217 acc_train: 0.7500 loss_val: 0.5691 acc_val: 0.7273 time: 0.0088s\n",
      "Epoch: 00773 loss_train: 0.5119 loss_rec: 0.5119 acc_train: 0.7543 loss_val: 0.5784 acc_val: 0.7208 time: 0.0092s\n",
      "Epoch: 00774 loss_train: 0.5087 loss_rec: 0.5087 acc_train: 0.7326 loss_val: 0.5789 acc_val: 0.7013 time: 0.0091s\n",
      "Epoch: 00775 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7522 loss_val: 0.6093 acc_val: 0.7013 time: 0.0092s\n",
      "Epoch: 00776 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7413 loss_val: 0.5816 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00777 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7478 loss_val: 0.5667 acc_val: 0.7273 time: 0.0092s\n",
      "Epoch: 00778 loss_train: 0.5351 loss_rec: 0.5351 acc_train: 0.7413 loss_val: 0.5583 acc_val: 0.7403 time: 0.0089s\n",
      "Epoch: 00779 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7391 loss_val: 0.5864 acc_val: 0.7078 time: 0.0061s\n",
      "Epoch: 00780 loss_train: 0.5138 loss_rec: 0.5138 acc_train: 0.7478 loss_val: 0.5372 acc_val: 0.7338 time: 0.0050s\n",
      "Epoch: 00781 loss_train: 0.5166 loss_rec: 0.5166 acc_train: 0.7435 loss_val: 0.5898 acc_val: 0.7338 time: 0.0097s\n",
      "Epoch: 00782 loss_train: 0.5215 loss_rec: 0.5215 acc_train: 0.7522 loss_val: 0.5545 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00783 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7304 loss_val: 0.5782 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00784 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7435 loss_val: 0.5740 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00785 loss_train: 0.5081 loss_rec: 0.5081 acc_train: 0.7500 loss_val: 0.5599 acc_val: 0.7273 time: 0.0094s\n",
      "Epoch: 00786 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7522 loss_val: 0.5593 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00787 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7522 loss_val: 0.5473 acc_val: 0.7273 time: 0.0081s\n",
      "Epoch: 00788 loss_train: 0.5124 loss_rec: 0.5124 acc_train: 0.7457 loss_val: 0.5608 acc_val: 0.7273 time: 0.0075s\n",
      "Epoch: 00789 loss_train: 0.5241 loss_rec: 0.5241 acc_train: 0.7413 loss_val: 0.5760 acc_val: 0.7403 time: 0.0096s\n",
      "Epoch: 00790 loss_train: 0.5261 loss_rec: 0.5261 acc_train: 0.7435 loss_val: 0.5357 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00791 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7478 loss_val: 0.5946 acc_val: 0.7273 time: 0.0095s\n",
      "Epoch: 00792 loss_train: 0.5242 loss_rec: 0.5242 acc_train: 0.7413 loss_val: 0.5748 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00793 loss_train: 0.5289 loss_rec: 0.5289 acc_train: 0.7413 loss_val: 0.5857 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00794 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7457 loss_val: 0.5787 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00795 loss_train: 0.5323 loss_rec: 0.5323 acc_train: 0.7478 loss_val: 0.5613 acc_val: 0.7208 time: 0.0083s\n",
      "Epoch: 00796 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7435 loss_val: 0.5732 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00797 loss_train: 0.5241 loss_rec: 0.5241 acc_train: 0.7522 loss_val: 0.5600 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00798 loss_train: 0.5376 loss_rec: 0.5376 acc_train: 0.7457 loss_val: 0.5674 acc_val: 0.7208 time: 0.0083s\n",
      "Epoch: 00799 loss_train: 0.5175 loss_rec: 0.5175 acc_train: 0.7457 loss_val: 0.5451 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00800 loss_train: 0.5129 loss_rec: 0.5129 acc_train: 0.7348 loss_val: 0.5496 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00801 loss_train: 0.5210 loss_rec: 0.5210 acc_train: 0.7478 loss_val: 0.5755 acc_val: 0.7078 time: 0.0084s\n",
      "Epoch: 00802 loss_train: 0.5280 loss_rec: 0.5280 acc_train: 0.7413 loss_val: 0.5355 acc_val: 0.7403 time: 0.0063s\n",
      "Epoch: 00803 loss_train: 0.5212 loss_rec: 0.5212 acc_train: 0.7543 loss_val: 0.5382 acc_val: 0.7338 time: 0.0130s\n",
      "Epoch: 00804 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7457 loss_val: 0.5859 acc_val: 0.7208 time: 0.0882s\n",
      "Epoch: 00805 loss_train: 0.5332 loss_rec: 0.5332 acc_train: 0.7348 loss_val: 0.5593 acc_val: 0.7208 time: 0.0201s\n",
      "Epoch: 00806 loss_train: 0.5207 loss_rec: 0.5207 acc_train: 0.7565 loss_val: 0.5615 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00807 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7478 loss_val: 0.5591 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00808 loss_train: 0.5276 loss_rec: 0.5276 acc_train: 0.7457 loss_val: 0.5508 acc_val: 0.7468 time: 0.0084s\n",
      "Epoch: 00809 loss_train: 0.5154 loss_rec: 0.5154 acc_train: 0.7457 loss_val: 0.5679 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00810 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7435 loss_val: 0.5615 acc_val: 0.7403 time: 0.0096s\n",
      "Epoch: 00811 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7413 loss_val: 0.5806 acc_val: 0.7143 time: 0.0094s\n",
      "Epoch: 00812 loss_train: 0.5282 loss_rec: 0.5282 acc_train: 0.7435 loss_val: 0.5784 acc_val: 0.7403 time: 0.0087s\n",
      "Epoch: 00813 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7478 loss_val: 0.5851 acc_val: 0.7143 time: 0.0094s\n",
      "Epoch: 00814 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7543 loss_val: 0.5811 acc_val: 0.7078 time: 0.0060s\n",
      "Epoch: 00815 loss_train: 0.5241 loss_rec: 0.5241 acc_train: 0.7522 loss_val: 0.5632 acc_val: 0.7273 time: 0.0103s\n",
      "Epoch: 00816 loss_train: 0.5264 loss_rec: 0.5264 acc_train: 0.7522 loss_val: 0.5845 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00817 loss_train: 0.5091 loss_rec: 0.5091 acc_train: 0.7457 loss_val: 0.5767 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00818 loss_train: 0.5211 loss_rec: 0.5211 acc_train: 0.7413 loss_val: 0.5786 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00819 loss_train: 0.5318 loss_rec: 0.5318 acc_train: 0.7478 loss_val: 0.5539 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00820 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7413 loss_val: 0.5604 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00821 loss_train: 0.5262 loss_rec: 0.5262 acc_train: 0.7500 loss_val: 0.5517 acc_val: 0.7143 time: 0.0080s\n",
      "Epoch: 00822 loss_train: 0.5199 loss_rec: 0.5199 acc_train: 0.7435 loss_val: 0.5660 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00823 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7391 loss_val: 0.5843 acc_val: 0.7338 time: 0.0095s\n",
      "Epoch: 00824 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7326 loss_val: 0.5829 acc_val: 0.6948 time: 0.0091s\n",
      "Epoch: 00825 loss_train: 0.5277 loss_rec: 0.5277 acc_train: 0.7413 loss_val: 0.5668 acc_val: 0.7078 time: 0.0099s\n",
      "Epoch: 00826 loss_train: 0.5186 loss_rec: 0.5186 acc_train: 0.7413 loss_val: 0.5578 acc_val: 0.7143 time: 0.0118s\n",
      "Epoch: 00827 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7413 loss_val: 0.5605 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00828 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7522 loss_val: 0.5727 acc_val: 0.7273 time: 0.0095s\n",
      "Epoch: 00829 loss_train: 0.5337 loss_rec: 0.5337 acc_train: 0.7348 loss_val: 0.5370 acc_val: 0.7208 time: 0.0083s\n",
      "Epoch: 00830 loss_train: 0.5233 loss_rec: 0.5233 acc_train: 0.7370 loss_val: 0.5420 acc_val: 0.7273 time: 0.0094s\n",
      "Epoch: 00831 loss_train: 0.5253 loss_rec: 0.5253 acc_train: 0.7326 loss_val: 0.5562 acc_val: 0.7403 time: 0.0103s\n",
      "Epoch: 00832 loss_train: 0.5231 loss_rec: 0.5231 acc_train: 0.7413 loss_val: 0.5797 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00833 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7478 loss_val: 0.5422 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00834 loss_train: 0.5118 loss_rec: 0.5118 acc_train: 0.7457 loss_val: 0.5620 acc_val: 0.7273 time: 0.0076s\n",
      "Epoch: 00835 loss_train: 0.5180 loss_rec: 0.5180 acc_train: 0.7500 loss_val: 0.5619 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00836 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7435 loss_val: 0.5585 acc_val: 0.7273 time: 0.0083s\n",
      "Epoch: 00837 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7478 loss_val: 0.5722 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00838 loss_train: 0.5281 loss_rec: 0.5281 acc_train: 0.7435 loss_val: 0.6048 acc_val: 0.7273 time: 0.0083s\n",
      "Epoch: 00839 loss_train: 0.5235 loss_rec: 0.5235 acc_train: 0.7391 loss_val: 0.5756 acc_val: 0.7273 time: 0.0079s\n",
      "Epoch: 00840 loss_train: 0.5314 loss_rec: 0.5314 acc_train: 0.7435 loss_val: 0.5620 acc_val: 0.7273 time: 0.0089s\n",
      "Epoch: 00841 loss_train: 0.5188 loss_rec: 0.5188 acc_train: 0.7478 loss_val: 0.5715 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00842 loss_train: 0.5141 loss_rec: 0.5141 acc_train: 0.7500 loss_val: 0.5845 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00843 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7435 loss_val: 0.5801 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00844 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7435 loss_val: 0.5543 acc_val: 0.7143 time: 0.0081s\n",
      "Epoch: 00845 loss_train: 0.5202 loss_rec: 0.5202 acc_train: 0.7478 loss_val: 0.5533 acc_val: 0.7273 time: 0.0089s\n",
      "Epoch: 00846 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7348 loss_val: 0.5750 acc_val: 0.7208 time: 0.0055s\n",
      "Epoch: 00847 loss_train: 0.5245 loss_rec: 0.5245 acc_train: 0.7348 loss_val: 0.5579 acc_val: 0.7403 time: 0.0070s\n",
      "Epoch: 00848 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7435 loss_val: 0.5685 acc_val: 0.7338 time: 0.0092s\n",
      "Epoch: 00849 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7543 loss_val: 0.5701 acc_val: 0.7013 time: 0.0116s\n",
      "Epoch: 00850 loss_train: 0.5309 loss_rec: 0.5309 acc_train: 0.7391 loss_val: 0.5560 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00851 loss_train: 0.5229 loss_rec: 0.5229 acc_train: 0.7326 loss_val: 0.5567 acc_val: 0.7468 time: 0.0079s\n",
      "Epoch: 00852 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7457 loss_val: 0.5310 acc_val: 0.7338 time: 0.0095s\n",
      "Epoch: 00853 loss_train: 0.5183 loss_rec: 0.5183 acc_train: 0.7457 loss_val: 0.5488 acc_val: 0.7468 time: 0.0099s\n",
      "Epoch: 00854 loss_train: 0.5296 loss_rec: 0.5296 acc_train: 0.7413 loss_val: 0.5448 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00855 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7522 loss_val: 0.5788 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00856 loss_train: 0.5147 loss_rec: 0.5147 acc_train: 0.7457 loss_val: 0.5663 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00857 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7391 loss_val: 0.5472 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00858 loss_train: 0.5238 loss_rec: 0.5238 acc_train: 0.7500 loss_val: 0.5590 acc_val: 0.7273 time: 0.0100s\n",
      "Epoch: 00859 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7457 loss_val: 0.5570 acc_val: 0.7208 time: 0.0118s\n",
      "Epoch: 00860 loss_train: 0.5019 loss_rec: 0.5019 acc_train: 0.7543 loss_val: 0.5796 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00861 loss_train: 0.5227 loss_rec: 0.5227 acc_train: 0.7457 loss_val: 0.5769 acc_val: 0.7338 time: 0.0103s\n",
      "Epoch: 00862 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7435 loss_val: 0.5528 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00863 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7304 loss_val: 0.5554 acc_val: 0.7273 time: 0.0072s\n",
      "Epoch: 00864 loss_train: 0.5238 loss_rec: 0.5238 acc_train: 0.7500 loss_val: 0.5616 acc_val: 0.7403 time: 0.0053s\n",
      "Epoch: 00865 loss_train: 0.5151 loss_rec: 0.5151 acc_train: 0.7457 loss_val: 0.5730 acc_val: 0.7403 time: 0.0078s\n",
      "Epoch: 00866 loss_train: 0.5090 loss_rec: 0.5090 acc_train: 0.7522 loss_val: 0.5720 acc_val: 0.7273 time: 0.0078s\n",
      "Epoch: 00867 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7435 loss_val: 0.5737 acc_val: 0.7338 time: 0.0088s\n",
      "Epoch: 00868 loss_train: 0.5235 loss_rec: 0.5235 acc_train: 0.7500 loss_val: 0.5600 acc_val: 0.7078 time: 0.0091s\n",
      "Epoch: 00869 loss_train: 0.5124 loss_rec: 0.5124 acc_train: 0.7370 loss_val: 0.5967 acc_val: 0.7208 time: 0.0093s\n",
      "Epoch: 00870 loss_train: 0.5309 loss_rec: 0.5309 acc_train: 0.7478 loss_val: 0.5941 acc_val: 0.7078 time: 0.0100s\n",
      "Epoch: 00871 loss_train: 0.5093 loss_rec: 0.5093 acc_train: 0.7457 loss_val: 0.5828 acc_val: 0.7078 time: 0.0113s\n",
      "Epoch: 00872 loss_train: 0.5171 loss_rec: 0.5171 acc_train: 0.7370 loss_val: 0.5692 acc_val: 0.7338 time: 0.0089s\n",
      "Epoch: 00873 loss_train: 0.5236 loss_rec: 0.5236 acc_train: 0.7326 loss_val: 0.5700 acc_val: 0.7403 time: 0.0089s\n",
      "Epoch: 00874 loss_train: 0.5323 loss_rec: 0.5323 acc_train: 0.7500 loss_val: 0.5712 acc_val: 0.7273 time: 0.0080s\n",
      "Epoch: 00875 loss_train: 0.5218 loss_rec: 0.5218 acc_train: 0.7413 loss_val: 0.5528 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00876 loss_train: 0.5366 loss_rec: 0.5366 acc_train: 0.7326 loss_val: 0.5502 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00877 loss_train: 0.5231 loss_rec: 0.5231 acc_train: 0.7500 loss_val: 0.5844 acc_val: 0.7143 time: 0.0100s\n",
      "Epoch: 00878 loss_train: 0.5154 loss_rec: 0.5154 acc_train: 0.7522 loss_val: 0.5384 acc_val: 0.7273 time: 0.0088s\n",
      "Epoch: 00879 loss_train: 0.5164 loss_rec: 0.5164 acc_train: 0.7413 loss_val: 0.5417 acc_val: 0.7143 time: 0.0083s\n",
      "Epoch: 00880 loss_train: 0.5089 loss_rec: 0.5089 acc_train: 0.7457 loss_val: 0.5795 acc_val: 0.7143 time: 0.0091s\n",
      "Epoch: 00881 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7435 loss_val: 0.5708 acc_val: 0.7403 time: 0.0091s\n",
      "Epoch: 00882 loss_train: 0.5000 loss_rec: 0.5000 acc_train: 0.7522 loss_val: 0.5530 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00883 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7478 loss_val: 0.5481 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 00884 loss_train: 0.5131 loss_rec: 0.5131 acc_train: 0.7435 loss_val: 0.5902 acc_val: 0.7338 time: 0.0093s\n",
      "Epoch: 00885 loss_train: 0.5159 loss_rec: 0.5159 acc_train: 0.7500 loss_val: 0.5576 acc_val: 0.7403 time: 0.0086s\n",
      "Epoch: 00886 loss_train: 0.5281 loss_rec: 0.5281 acc_train: 0.7457 loss_val: 0.5870 acc_val: 0.7403 time: 0.0094s\n",
      "Epoch: 00887 loss_train: 0.5310 loss_rec: 0.5310 acc_train: 0.7391 loss_val: 0.5463 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00888 loss_train: 0.5127 loss_rec: 0.5127 acc_train: 0.7413 loss_val: 0.5628 acc_val: 0.7338 time: 0.0089s\n",
      "Epoch: 00889 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7500 loss_val: 0.5705 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00890 loss_train: 0.5250 loss_rec: 0.5250 acc_train: 0.7500 loss_val: 0.5555 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00891 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7457 loss_val: 0.5462 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00892 loss_train: 0.5252 loss_rec: 0.5252 acc_train: 0.7457 loss_val: 0.5624 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00893 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7391 loss_val: 0.5491 acc_val: 0.7403 time: 0.0104s\n",
      "Epoch: 00894 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7543 loss_val: 0.5719 acc_val: 0.7468 time: 0.0065s\n",
      "Epoch: 00895 loss_train: 0.5170 loss_rec: 0.5170 acc_train: 0.7391 loss_val: 0.5628 acc_val: 0.7338 time: 0.0069s\n",
      "Epoch: 00896 loss_train: 0.5230 loss_rec: 0.5230 acc_train: 0.7435 loss_val: 0.5409 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00897 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7435 loss_val: 0.5471 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00898 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7522 loss_val: 0.5512 acc_val: 0.7338 time: 0.0085s\n",
      "Epoch: 00899 loss_train: 0.5137 loss_rec: 0.5137 acc_train: 0.7413 loss_val: 0.5889 acc_val: 0.6948 time: 0.0092s\n",
      "Epoch: 00900 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7348 loss_val: 0.5621 acc_val: 0.7143 time: 0.0072s\n",
      "Epoch: 00901 loss_train: 0.5225 loss_rec: 0.5225 acc_train: 0.7391 loss_val: 0.5721 acc_val: 0.7338 time: 0.0111s\n",
      "Epoch: 00902 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7348 loss_val: 0.5634 acc_val: 0.7273 time: 0.0084s\n",
      "Epoch: 00903 loss_train: 0.4990 loss_rec: 0.4990 acc_train: 0.7370 loss_val: 0.5378 acc_val: 0.7273 time: 0.0096s\n",
      "Epoch: 00904 loss_train: 0.5248 loss_rec: 0.5248 acc_train: 0.7391 loss_val: 0.5543 acc_val: 0.7143 time: 0.0108s\n",
      "Epoch: 00905 loss_train: 0.5133 loss_rec: 0.5133 acc_train: 0.7478 loss_val: 0.5650 acc_val: 0.7403 time: 0.0085s\n",
      "Epoch: 00906 loss_train: 0.5407 loss_rec: 0.5407 acc_train: 0.7370 loss_val: 0.5534 acc_val: 0.7273 time: 0.0091s\n",
      "Epoch: 00907 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7478 loss_val: 0.5622 acc_val: 0.7532 time: 0.0099s\n",
      "Epoch: 00908 loss_train: 0.5165 loss_rec: 0.5165 acc_train: 0.7457 loss_val: 0.5615 acc_val: 0.7338 time: 0.0067s\n",
      "Epoch: 00909 loss_train: 0.5091 loss_rec: 0.5091 acc_train: 0.7457 loss_val: 0.5678 acc_val: 0.7468 time: 0.0081s\n",
      "Epoch: 00910 loss_train: 0.5127 loss_rec: 0.5127 acc_train: 0.7478 loss_val: 0.5335 acc_val: 0.7338 time: 0.0077s\n",
      "Epoch: 00911 loss_train: 0.5329 loss_rec: 0.5329 acc_train: 0.7457 loss_val: 0.5789 acc_val: 0.7143 time: 0.0086s\n",
      "Epoch: 00912 loss_train: 0.5069 loss_rec: 0.5069 acc_train: 0.7522 loss_val: 0.5572 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00913 loss_train: 0.5136 loss_rec: 0.5136 acc_train: 0.7478 loss_val: 0.5405 acc_val: 0.7532 time: 0.0085s\n",
      "Epoch: 00914 loss_train: 0.5208 loss_rec: 0.5208 acc_train: 0.7413 loss_val: 0.5511 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00915 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7500 loss_val: 0.5536 acc_val: 0.7403 time: 0.0080s\n",
      "Epoch: 00916 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7457 loss_val: 0.5847 acc_val: 0.7208 time: 0.0105s\n",
      "Epoch: 00917 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7457 loss_val: 0.5660 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00918 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7478 loss_val: 0.5718 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00919 loss_train: 0.5135 loss_rec: 0.5135 acc_train: 0.7457 loss_val: 0.5866 acc_val: 0.7273 time: 0.0082s\n",
      "Epoch: 00920 loss_train: 0.5232 loss_rec: 0.5232 acc_train: 0.7457 loss_val: 0.5662 acc_val: 0.7403 time: 0.0082s\n",
      "Epoch: 00921 loss_train: 0.5235 loss_rec: 0.5235 acc_train: 0.7457 loss_val: 0.5804 acc_val: 0.7208 time: 0.0106s\n",
      "Epoch: 00922 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7435 loss_val: 0.5755 acc_val: 0.7273 time: 0.0090s\n",
      "Epoch: 00923 loss_train: 0.5286 loss_rec: 0.5286 acc_train: 0.7457 loss_val: 0.5459 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00924 loss_train: 0.5229 loss_rec: 0.5229 acc_train: 0.7500 loss_val: 0.5859 acc_val: 0.7403 time: 0.0089s\n",
      "Epoch: 00925 loss_train: 0.5118 loss_rec: 0.5118 acc_train: 0.7500 loss_val: 0.5615 acc_val: 0.7403 time: 0.0084s\n",
      "Epoch: 00926 loss_train: 0.5172 loss_rec: 0.5172 acc_train: 0.7478 loss_val: 0.5695 acc_val: 0.7403 time: 0.0074s\n",
      "Epoch: 00927 loss_train: 0.5203 loss_rec: 0.5203 acc_train: 0.7326 loss_val: 0.5557 acc_val: 0.7143 time: 0.0086s\n",
      "Epoch: 00928 loss_train: 0.5131 loss_rec: 0.5131 acc_train: 0.7435 loss_val: 0.5547 acc_val: 0.7338 time: 0.0086s\n",
      "Epoch: 00929 loss_train: 0.5306 loss_rec: 0.5306 acc_train: 0.7413 loss_val: 0.5740 acc_val: 0.7208 time: 0.0091s\n",
      "Epoch: 00930 loss_train: 0.5121 loss_rec: 0.5121 acc_train: 0.7543 loss_val: 0.5791 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00931 loss_train: 0.5310 loss_rec: 0.5310 acc_train: 0.7370 loss_val: 0.5516 acc_val: 0.7468 time: 0.0080s\n",
      "Epoch: 00932 loss_train: 0.5168 loss_rec: 0.5168 acc_train: 0.7348 loss_val: 0.5608 acc_val: 0.7532 time: 0.0100s\n",
      "Epoch: 00933 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7457 loss_val: 0.5741 acc_val: 0.7468 time: 0.0102s\n",
      "Epoch: 00934 loss_train: 0.5305 loss_rec: 0.5305 acc_train: 0.7413 loss_val: 0.5928 acc_val: 0.7143 time: 0.0085s\n",
      "Epoch: 00935 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7435 loss_val: 0.5691 acc_val: 0.7078 time: 0.0076s\n",
      "Epoch: 00936 loss_train: 0.5239 loss_rec: 0.5239 acc_train: 0.7478 loss_val: 0.5788 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00937 loss_train: 0.5082 loss_rec: 0.5082 acc_train: 0.7478 loss_val: 0.5690 acc_val: 0.7208 time: 0.0101s\n",
      "Epoch: 00938 loss_train: 0.5191 loss_rec: 0.5191 acc_train: 0.7435 loss_val: 0.5453 acc_val: 0.7403 time: 0.0108s\n",
      "Epoch: 00939 loss_train: 0.5280 loss_rec: 0.5280 acc_train: 0.7391 loss_val: 0.5825 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00940 loss_train: 0.5052 loss_rec: 0.5052 acc_train: 0.7457 loss_val: 0.5640 acc_val: 0.7403 time: 0.0088s\n",
      "Epoch: 00941 loss_train: 0.5171 loss_rec: 0.5171 acc_train: 0.7478 loss_val: 0.5690 acc_val: 0.7273 time: 0.0102s\n",
      "Epoch: 00942 loss_train: 0.5108 loss_rec: 0.5108 acc_train: 0.7435 loss_val: 0.5584 acc_val: 0.7273 time: 0.0093s\n",
      "Epoch: 00943 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7391 loss_val: 0.5571 acc_val: 0.7338 time: 0.0094s\n",
      "Epoch: 00944 loss_train: 0.5143 loss_rec: 0.5143 acc_train: 0.7500 loss_val: 0.5562 acc_val: 0.7143 time: 0.0090s\n",
      "Epoch: 00945 loss_train: 0.5162 loss_rec: 0.5162 acc_train: 0.7500 loss_val: 0.5453 acc_val: 0.7338 time: 0.0071s\n",
      "Epoch: 00946 loss_train: 0.5203 loss_rec: 0.5203 acc_train: 0.7478 loss_val: 0.5400 acc_val: 0.7403 time: 0.0120s\n",
      "Epoch: 00947 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7413 loss_val: 0.5643 acc_val: 0.7208 time: 0.0099s\n",
      "Epoch: 00948 loss_train: 0.5295 loss_rec: 0.5295 acc_train: 0.7370 loss_val: 0.5728 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00949 loss_train: 0.5241 loss_rec: 0.5241 acc_train: 0.7478 loss_val: 0.5933 acc_val: 0.7273 time: 0.0104s\n",
      "Epoch: 00950 loss_train: 0.5149 loss_rec: 0.5149 acc_train: 0.7478 loss_val: 0.5331 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00951 loss_train: 0.5200 loss_rec: 0.5200 acc_train: 0.7435 loss_val: 0.5478 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00952 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7522 loss_val: 0.5571 acc_val: 0.7338 time: 0.0080s\n",
      "Epoch: 00953 loss_train: 0.5231 loss_rec: 0.5231 acc_train: 0.7500 loss_val: 0.5582 acc_val: 0.7208 time: 0.0094s\n",
      "Epoch: 00954 loss_train: 0.5255 loss_rec: 0.5255 acc_train: 0.7457 loss_val: 0.5708 acc_val: 0.7403 time: 0.0092s\n",
      "Epoch: 00955 loss_train: 0.5173 loss_rec: 0.5173 acc_train: 0.7370 loss_val: 0.5665 acc_val: 0.7273 time: 0.0088s\n",
      "Epoch: 00956 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7435 loss_val: 0.5497 acc_val: 0.7468 time: 0.0100s\n",
      "Epoch: 00957 loss_train: 0.5307 loss_rec: 0.5307 acc_train: 0.7435 loss_val: 0.5574 acc_val: 0.7468 time: 0.0090s\n",
      "Epoch: 00958 loss_train: 0.5192 loss_rec: 0.5192 acc_train: 0.7457 loss_val: 0.5679 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00959 loss_train: 0.5114 loss_rec: 0.5114 acc_train: 0.7500 loss_val: 0.5596 acc_val: 0.7273 time: 0.0066s\n",
      "Epoch: 00960 loss_train: 0.5334 loss_rec: 0.5334 acc_train: 0.7457 loss_val: 0.5798 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00961 loss_train: 0.5278 loss_rec: 0.5278 acc_train: 0.7435 loss_val: 0.5535 acc_val: 0.7273 time: 0.0087s\n",
      "Epoch: 00962 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7435 loss_val: 0.5765 acc_val: 0.7338 time: 0.0082s\n",
      "Epoch: 00963 loss_train: 0.5156 loss_rec: 0.5156 acc_train: 0.7457 loss_val: 0.5699 acc_val: 0.7273 time: 0.0074s\n",
      "Epoch: 00964 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7478 loss_val: 0.5752 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00965 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7478 loss_val: 0.5908 acc_val: 0.7338 time: 0.0102s\n",
      "Epoch: 00966 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7478 loss_val: 0.5411 acc_val: 0.7468 time: 0.0093s\n",
      "Epoch: 00967 loss_train: 0.5196 loss_rec: 0.5196 acc_train: 0.7435 loss_val: 0.5600 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00968 loss_train: 0.5203 loss_rec: 0.5203 acc_train: 0.7457 loss_val: 0.5745 acc_val: 0.7338 time: 0.0100s\n",
      "Epoch: 00969 loss_train: 0.5167 loss_rec: 0.5167 acc_train: 0.7457 loss_val: 0.5652 acc_val: 0.7403 time: 0.0093s\n",
      "Epoch: 00970 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7457 loss_val: 0.5704 acc_val: 0.7208 time: 0.0080s\n",
      "Epoch: 00971 loss_train: 0.5265 loss_rec: 0.5265 acc_train: 0.7413 loss_val: 0.5867 acc_val: 0.7273 time: 0.0099s\n",
      "Epoch: 00972 loss_train: 0.5281 loss_rec: 0.5281 acc_train: 0.7435 loss_val: 0.5643 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00973 loss_train: 0.5161 loss_rec: 0.5161 acc_train: 0.7500 loss_val: 0.5734 acc_val: 0.7338 time: 0.0105s\n",
      "Epoch: 00974 loss_train: 0.5099 loss_rec: 0.5099 acc_train: 0.7478 loss_val: 0.5804 acc_val: 0.7208 time: 0.0096s\n",
      "Epoch: 00975 loss_train: 0.5055 loss_rec: 0.5055 acc_train: 0.7435 loss_val: 0.5647 acc_val: 0.7403 time: 0.0075s\n",
      "Epoch: 00976 loss_train: 0.5087 loss_rec: 0.5087 acc_train: 0.7435 loss_val: 0.5767 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 00977 loss_train: 0.5167 loss_rec: 0.5167 acc_train: 0.7522 loss_val: 0.5761 acc_val: 0.6948 time: 0.0095s\n",
      "Epoch: 00978 loss_train: 0.5126 loss_rec: 0.5126 acc_train: 0.7522 loss_val: 0.5789 acc_val: 0.7078 time: 0.0099s\n",
      "Epoch: 00979 loss_train: 0.5099 loss_rec: 0.5099 acc_train: 0.7457 loss_val: 0.5489 acc_val: 0.7338 time: 0.0097s\n",
      "Epoch: 00980 loss_train: 0.5270 loss_rec: 0.5270 acc_train: 0.7500 loss_val: 0.5760 acc_val: 0.7338 time: 0.0095s\n",
      "Epoch: 00981 loss_train: 0.5176 loss_rec: 0.5176 acc_train: 0.7522 loss_val: 0.5656 acc_val: 0.7273 time: 0.0098s\n",
      "Epoch: 00982 loss_train: 0.5194 loss_rec: 0.5194 acc_train: 0.7217 loss_val: 0.5637 acc_val: 0.7078 time: 0.0112s\n",
      "Epoch: 00983 loss_train: 0.5212 loss_rec: 0.5212 acc_train: 0.7478 loss_val: 0.5976 acc_val: 0.7338 time: 0.0081s\n",
      "Epoch: 00984 loss_train: 0.5229 loss_rec: 0.5229 acc_train: 0.7457 loss_val: 0.5802 acc_val: 0.7273 time: 0.0085s\n",
      "Epoch: 00985 loss_train: 0.5100 loss_rec: 0.5100 acc_train: 0.7522 loss_val: 0.5840 acc_val: 0.7338 time: 0.0088s\n",
      "Epoch: 00986 loss_train: 0.5313 loss_rec: 0.5313 acc_train: 0.7435 loss_val: 0.5648 acc_val: 0.7338 time: 0.0091s\n",
      "Epoch: 00987 loss_train: 0.5110 loss_rec: 0.5110 acc_train: 0.7457 loss_val: 0.5649 acc_val: 0.7403 time: 0.0100s\n",
      "Epoch: 00988 loss_train: 0.5213 loss_rec: 0.5213 acc_train: 0.7478 loss_val: 0.5503 acc_val: 0.7338 time: 0.0084s\n",
      "Epoch: 00989 loss_train: 0.5214 loss_rec: 0.5214 acc_train: 0.7457 loss_val: 0.5678 acc_val: 0.7403 time: 0.0095s\n",
      "Epoch: 00990 loss_train: 0.5246 loss_rec: 0.5246 acc_train: 0.7370 loss_val: 0.5559 acc_val: 0.7338 time: 0.0090s\n",
      "Epoch: 00991 loss_train: 0.5179 loss_rec: 0.5179 acc_train: 0.7413 loss_val: 0.5534 acc_val: 0.7403 time: 0.0086s\n",
      "Epoch: 00992 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7457 loss_val: 0.5492 acc_val: 0.7403 time: 0.0090s\n",
      "Epoch: 00993 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7522 loss_val: 0.5709 acc_val: 0.7273 time: 0.0101s\n",
      "Epoch: 00994 loss_train: 0.5160 loss_rec: 0.5160 acc_train: 0.7478 loss_val: 0.5309 acc_val: 0.7338 time: 0.0087s\n",
      "Epoch: 00995 loss_train: 0.5145 loss_rec: 0.5145 acc_train: 0.7500 loss_val: 0.5506 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 00996 loss_train: 0.5228 loss_rec: 0.5228 acc_train: 0.7413 loss_val: 0.5593 acc_val: 0.7338 time: 0.0096s\n",
      "Epoch: 00997 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7457 loss_val: 0.5754 acc_val: 0.7273 time: 0.0086s\n",
      "Epoch: 00998 loss_train: 0.5221 loss_rec: 0.5221 acc_train: 0.7500 loss_val: 0.5779 acc_val: 0.7143 time: 0.0101s\n",
      "Epoch: 00999 loss_train: 0.5232 loss_rec: 0.5232 acc_train: 0.7413 loss_val: 0.5810 acc_val: 0.7208 time: 0.0090s\n",
      "Epoch: 01000 loss_train: 0.5146 loss_rec: 0.5146 acc_train: 0.7522 loss_val: 0.5901 acc_val: 0.7143 time: 0.0090s\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "encoder_s = GCN_Encoder_s(nfeat=features.shape[1],\n",
    "        nhid=args.n_hidden,\n",
    "        nembed=args.n_hidden,\n",
    "        dropout=args.dropout)\n",
    "classifier_s = GCN_Classifier_s(nembed=args.n_hidden, \n",
    "        nhid=args.n_hidden, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=args.dropout, device = device)\n",
    "decoder_s = Decoder_s(nembed=args.n_hidden,\n",
    "        dropout=args.dropout)\n",
    "optimizer_en = optim.Adam(encoder_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer_cls = optim.Adam(classifier_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer_de = optim.Adam(decoder_s.parameters(),\n",
    "                       lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder_s.train()\n",
    "    classifier_s.train()\n",
    "    decoder_s.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj_mtx)\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon' or args.setting == 'newG_cls':\n",
    "        ori_num = labels.shape[0]\n",
    "        embed, labels_new, idx_train_new, adj_up = utils.recon_upsample(embed, labels, train_idx, adj = adj_mtx.detach().to_dense(), portion = args.up_scale, im_class_num = args.im_class_num)\n",
    "        generated_G = decoder_s(embed)\n",
    "\n",
    "        loss_rec = utils.adj_mse_loss(generated_G[: ori_num, :][:, : ori_num], adj_mtx.detach().to_dense())\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = copy.deepcopy(generated_G.detach())\n",
    "            threshold = 0.5\n",
    "            adj_new[adj_new < threshold] = 0.0\n",
    "            adj_new[adj_new >= threshold] = 1.0\n",
    "\n",
    "            #ipdb.set_trace()\n",
    "            edge_ac = adj_new[: ori_num, : ori_num].eq(adj_mtx.to_dense()).double().sum()/(ori_num**2)\n",
    "        else:\n",
    "            adj_new = generated_G\n",
    "            edge_ac = F.l1_loss(adj_new[: ori_num, : ori_num], adj_mtx.to_dense(), reduction = 'mean')\n",
    "\n",
    "\n",
    "        #calculate generation information\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new = torch.mul(adj_up, adj_new)\n",
    "\n",
    "        exist_edge_prob = adj_new[:ori_num, :ori_num].mean() #edge prob for existing nodes\n",
    "        generated_edge_prob = adj_new[ori_num:, :ori_num].mean() #edge prob for generated nodes\n",
    "        print(\"after filtering, edge acc: {:.4f}, exist_edge_prob: {:.4f}, generated_edge_prob: {:.4f}\".format(edge_ac.item(), exist_edge_prob.item(), generated_edge_prob.item()))\n",
    "\n",
    "\n",
    "        adj_new[:ori_num, :][:, :ori_num] = adj_mtx.detach().to_dense()\n",
    "        #adj_new = adj_new.to_sparse()\n",
    "        #ipdb.set_trace()\n",
    "\n",
    "        if not args.opt_new_G:\n",
    "            adj_new = adj_new.detach()\n",
    "\n",
    "        if args.setting == 'newG_cls':\n",
    "            idx_train_new = train_idx\n",
    "\n",
    "    elif args.setting == 'embed_up':\n",
    "        #perform SMOTE in embedding space\n",
    "        embed, labels_new, idx_train_new = utils.recon_upsample(embed, labels, train_idx, portion=args.up_scale, im_class_num = args.im_class_num)\n",
    "        adj_new = adj_mtx\n",
    "    else:\n",
    "        labels_new = labels\n",
    "        idx_train_new = train_idx\n",
    "        adj_new = adj_mtx\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    output = classifier_s(embed, adj_new)\n",
    "\n",
    "\n",
    "\n",
    "    if args.setting == 'reweight':\n",
    "        weight = features.new((labels.max().item() + 1)).fill_(1)\n",
    "        weight[-args.im_class_num:] = 1 + args.up_scale\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1), weight=weight)\n",
    "    else:\n",
    "        loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new].reshape(-1))\n",
    "\n",
    "    acc_train = accuracy(output[train_idx], labels_new[train_idx].reshape(-1))\n",
    "    if args.setting == 'recon_newG':\n",
    "        loss = loss_train + loss_rec * args.rec_weight\n",
    "    elif args.setting == 'recon':\n",
    "        loss = loss_rec + 0 * loss_train\n",
    "    else:\n",
    "        loss = loss_train\n",
    "        loss_rec = loss_train\n",
    "\n",
    "    loss.backward()\n",
    "    if args.setting == 'newG_cls':\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_de.zero_grad()\n",
    "    else:\n",
    "        optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "\n",
    "    if args.setting == 'recon_newG' or args.setting == 'recon':\n",
    "        optimizer_de.step()\n",
    "\n",
    "    loss_val = F.cross_entropy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "    acc_val = accuracy(output[val_idx], labels[val_idx].reshape(-1))\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier with Reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.GraphConvolution import GCN_Encoder3, GCN_Classifier, GCN_Encoder_w\n",
    "from utils.reweight import next\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.evaluation import accuracy, print_class_acc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "\n",
    "# encoder = GCN_Encoder3(nfeat=n_features,\n",
    "#         nhid=n_hidden,\n",
    "#         nembed=n_hidden[-1],\n",
    "#         dropout=args.dropout,\n",
    "#         nclass=args.n_classes,\n",
    "#         order=1)\n",
    "# classifier = GCN_Classifier(nembed=n_hidden[-1], \n",
    "#         nhid=n_hidden[-1], \n",
    "#         nclass=int(labels.max().item()) + 1, \n",
    "#         dropout=args.dropout, device=device)\n",
    "encoder = GCN_Encoder_w(nfeat = n_features, nembed = n_hidden[-1], nhid = n_hidden[-1], nclass = int(labels.max().item()) + 1, dropout = args.dropout, device = device)\n",
    "optimizer = optim.Adam(encoder.parameters(),\n",
    "                       lr = args.learning_rate, weight_decay = args.weight_decay)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "def train(epoch):\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        t = time.time()\n",
    "        optimizer_en.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "        with higher.innerloop_ctx(encoder, optimizer) as (meta_model, meta_opt):\n",
    "                meta_train_outputs = meta_model(features, adj_mtx)\n",
    "                criterion.reduction = 'none'\n",
    "                meta_train_loss = criterion(meta_train_outputs, labels)\n",
    "                eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=device)\n",
    "                meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "                meta_opt.step(meta_train_loss)\n",
    "                meta_inputs, meta_labels = next()\n",
    "                meta_inputs, meta_labels = meta_inputs.to(device=device, non_blocking=True),\\\n",
    "                        meta_labels.to(device=device, non_blocking=True)\n",
    "                meta_val_outputs = meta_model(meta_inputs)\n",
    "                criterion.reduction = 'mean'\n",
    "                meta_val_loss = criterion(meta_val_outputs, meta_labels)\n",
    "                eps_grads = torch.autograd.grad(meta_val_loss, eps)[0].detach()\n",
    "\n",
    "                out = output[train_idx]\n",
    "                gt = labels[train_idx].reshape(-1)\n",
    "                if args.setting == 'reweight':\n",
    "                        weight = \"STH\"\n",
    "                        loss_train = F.cross_entropy(out, gt, weight = weight)\n",
    "                else:\n",
    "                        loss_train = F.cross_entropy(out, gt)\n",
    "                acc_train = accuracy(out, gt)\n",
    "                loss_train.backward()\n",
    "                optimizer_en.step()\n",
    "                optimizer_cls.step()\n",
    "                gt_v = labels[test_idx].reshape(-1)\n",
    "                out_v = output[test_idx]\n",
    "                loss_val = F.cross_entropy(out_v, gt_v)\n",
    "                acc_val = accuracy(out_v, gt_v)\n",
    "                # print_class_acc(out_v, gt_v)\n",
    "                print('Epoch: {:05d}'.format(epoch+ 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t))\n",
    "                \n",
    "                return acc_train.item(), acc_val.item(), loss_train.item(), loss_val.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "def encode_onehot_torch(labels):\n",
    "    num_classes = int(labels.max() + 1)\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arc_selection-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f122987ad9a59b0c863ec73977cb4833edd644652b774e5b01a9e2fe636c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
